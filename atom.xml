<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>年轻人起来冲</title>
  
  
  <link href="https://shaogui.life/atom.xml" rel="self"/>
  
  <link href="https://shaogui.life/"/>
  <updated>2023-05-20T01:56:34.710Z</updated>
  <id>https://shaogui.life/</id>
  
  <author>
    <name>绍桂</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>vit：An Image is Worth 16x16 Words：Transformers for Image Recognition at Scale</title>
    <link href="https://shaogui.life/2023/05/20/vit%EF%BC%9AAn%20Image%20is%20Worth%2016x16%20Words%EF%BC%9ATransformers%20for%20Image%20Recognition%20at%20Scale/"/>
    <id>https://shaogui.life/2023/05/20/vit%EF%BC%9AAn%20Image%20is%20Worth%2016x16%20Words%EF%BC%9ATransformers%20for%20Image%20Recognition%20at%20Scale/</id>
    <published>2023-05-20T01:48:16.000Z</published>
    <updated>2023-05-20T01:56:34.710Z</updated>
    
    <content type="html"><![CDATA[<p>一直以来都是使用卷积来处理图像数据，即使后面提出空洞卷积、特征金字塔来、注意力机制缓解其感受野受限，但是还是使用卷积计算去提取图像特征，<strong>受 NLP 中 transformer 的影响，VIT 将图像拆分为块 (patch)，并将图像数据学习转为图像块序列的学习问题</strong>，这里图像块 (patches) 的处理方式同 NLP 的标记 (tokens) </p><a id="more"></a><h1 id="什么是-vit-vision-in-transformer"><a href="#什么是-vit-vision-in-transformer" class="headerlink" title="什么是 vit (vision in transformer)?"></a>什么是 vit (vision in transformer)?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/vit-20230408150924.png" alt=""></li><li>一直以来都是使用卷积来处理图像数据，即使后面提出空洞卷积、特征金字塔来、注意力机制缓解其感受野受限，但是还是使用卷积计算去提取图像特征</li><li><strong>受 NLP 中 transformer 的影响，VIT 将图像拆分为块 (patch)，并将图像数据学习转为图像块序列的学习问题</strong>，这里图像块 (patches) 的处理方式同 NLP 的标记 (tokens) </li><li>注意：Transformers 缺乏 CNN 固有的一些归纳偏置 (inductive biases) —— 如平移等效性和局部性 (translation equivariance and locality)，因此在数据量不足时，训练不能很好地泛化</li></ul><h1 id="Vit-如何将-2D-图像转为-transformer-输入？"><a href="#Vit-如何将-2D-图像转为-transformer-输入？" class="headerlink" title="Vit 如何将 2D 图像转为 transformer 输入？"></a>Vit 如何将 2D 图像转为 transformer 输入？</h1><ul><li>tansformer 是 3D 输入的，即 (B, S, L)，分别表示 batch size、sequence size、sequence length，计算机视觉的输入一般是 (B, C, H, W)，从 (B, C, H, W)-&gt; (B, S, L)的过程中，可以认为 B 不变，C 表示 L，关键是如何将 （H，W）转为 S？</li><li>对于 2D 的图像，将其拆分成多个 PxP 的互不重叠区域，则共有 $S=HW/(P\times P)$ 个区域，则 $(B, C, H, W)$ 可被拆分为 $(B, C\times P\times P, HW/(P\times P))$ ，即 $(B, C’, S)$ ，这就可以被 tansformer 接收</li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmbedLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, args</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EmbedLayer, self).__init__()</span><br><span class="line">        self.args = args</span><br><span class="line">        <span class="comment"># 卷积获得图片的所有tokens,每个大小是patch_size</span></span><br><span class="line">        self.conv1 = nn.Conv2d(args.n_channels, args.embed_dim, kernel_size=args.patch_size, stride=args.patch_size) </span><br><span class="line">        self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, args.embed_dim), requires_grad=<span class="literal">True</span>)  <span class="comment"># Cls Token</span></span><br><span class="line">        self.pos_embedding = nn.Parameter(torch.zeros(<span class="number">1</span>, (args.img_size // args.patch_size) ** <span class="number">2</span> + <span class="number">1</span>, args.embed_dim), requires_grad=<span class="literal">True</span>)  <span class="comment"># Positional Embedding，这里是可学习的pos_embedding</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)  <span class="comment"># B C IH IW -&gt; B E IH/P IW/P</span></span><br><span class="line">        x = x.reshape([x.shape[<span class="number">0</span>], self.args.embed_dim, -<span class="number">1</span>])  <span class="comment"># B E IH/P IW/P -&gt; B E S</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># B E S -&gt; B S E</span></span><br><span class="line">        x = torch.cat((torch.repeat_interleave(self.cls_token, x.shape[<span class="number">0</span>], <span class="number">0</span>), x), dim=<span class="number">1</span>) <span class="comment"># 加上cls_token</span></span><br><span class="line">        x = x + self.pos_embedding <span class="comment"># 加上pos_embedding</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h1 id="Vit-的结构？"><a href="#Vit-的结构？" class="headerlink" title="Vit 的结构？"></a>Vit 的结构？</h1></li><li><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/vit-20230408150924.png" alt=""></p></li><li><strong>输入</strong>：输入图片 (B, C, H, W)，经过 Patch 对图像分块，得到 $(B, HW/(P\times P), C\times P\times P)$，将其类比为 (B, N, L)，然后增加 Position Embeddings 和 cls_token ，最终 Encoder 输入变为：(B, N+1, L)</li><li><strong>Encoder</strong>：对输入 (B, N+1, L)，使用 transformer 学习 N+1个 patch 的全局注意力，输出是(B, N+1, L)</li><li><strong>Decoder</strong>：从 (B, N+1, L)取出 cls_token，得到 (B, 1, L)，然后使用 Linear 编码该特征，输出 (B, L’)，L’为类别数量，最后使用 softmax 进行分类</li></ul><h1 id="Vit-为什么要加-Cls-token-节点？"><a href="#Vit-为什么要加-Cls-token-节点？" class="headerlink" title="Vit 为什么要加 Cls_token 节点？"></a>Vit 为什么要加 Cls_token 节点？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Vit-20230408150954.png" alt=""></li><li>经过 VIT 的编码器后，得到 N+1 个 Patch 的隐向量，即 (B，N+1，L)，论文使用 2 种方法去获得最后的分类结果</li><li><strong>方法 1</strong>：从训练开始就没有增加cls<em>token，然后对N个L取平均，得到一张图的向量表示，即(B，N，L)-&gt;(GAP)-&gt;$(B，1，\sum</em>{i=1}^N(L_i)/N )=(B,\hat L)$，然后对$(B,\hat L)$进行分类</li><li><strong>方法2</strong>：增加cls_token，然后只取cls_token进行分类</li><li>方法 1 是所有隐向量的线性组合，表达能力弱，方法 2 是网络学习的一部分，更有效，成本更低</li></ul><h1 id="Vit-为什么要使用-Position-Embedding？"><a href="#Vit-为什么要使用-Position-Embedding？" class="headerlink" title="Vit 为什么要使用 Position Embedding？"></a>Vit 为什么要使用 Position Embedding？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Vit-20230408150954-1.png" alt=""></li><li>不同于 CNN，Transformer 需要位置嵌入来编码 patch tokens 的位置信息，这主要是由于 自注意力 的 扰动不变性 (Permutation-invariant)，即打乱 Sequence 中 tokens 的顺序并不会改变结果，若不给模型提供图像块的位置信息，那么模型就需要通过图像块的语义来学习拼图，这就额外增加了学习成本。表现为如果不加位置编码，可能出现全部序列都预测出来，但是位置不正确</li><li>论文比较了4种嵌入编码的方式，效果差不多，因为分类任务对位置信息不敏感，如果是其他的transformer处理的任务，如果不加，效果很差</li></ul><h1 id="Vit-上不同网络位置的-transoformer-block-在注意力上有什么差异？"><a href="#Vit-上不同网络位置的-transoformer-block-在注意力上有什么差异？" class="headerlink" title="Vit 上不同网络位置的 transoformer block 在注意力上有什么差异？"></a>Vit 上不同网络位置的 transoformer block 在注意力上有什么差异？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Vit-20230408150955.png" alt=""></li><li>Mean Attention Distance主要体现自注意力机制的影响范围，比如认定注意力&gt;0.5的两个位置形成注意力，然后计算满足这个阈值的位置平均距离，这就类比于 CNN 的感受野。结果表明：<strong>前面层的 “感受野” 虽然差异很大，但总体相比后面层 “感受野” 较小；而模型后半部分 “感受野” 基本覆盖全局，和 CNN 比较类似，说明 ViT 也最后学习到了类似的范式</strong></li></ul><h1 id="Vit-与-CNN-的区别？"><a href="#Vit-与-CNN-的区别？" class="headerlink" title="Vit 与 CNN 的区别？"></a>Vit 与 CNN 的区别？</h1><ul><li><strong>归纳偏置 (Inductive bias)</strong>：Vision Transformer 的图像特定归纳偏置比 CNN 少得多。在 CNN 中，局部性、二维邻域结构和平移等效性存在于整个模型的每一层中。而在 ViT 中，只有 MLP 层是局部和平移等变的，因为自注意力层都是全局的。二维邻域结构的使用非常谨慎：在模型开始时通过将图像切分成块，并在微调时调整不同分辨率图像的位置嵌入 (如下所述)。此外，初始化时的位置嵌入不携带有关图像块的 2D 位置的信息，图像块之间的所有空间关系都必须从头开始学习</li><li><strong>混合架构 (Hybrid Architecture)</strong>：作为原始图像块的替代方案，输入序列可由 CNN 的特征图构成。在这种混合模型中，图像块嵌入投影被用在 经 CNN 特征提取的块 而非 原始输入图像块。作为一种特殊情况，块的空间尺寸可以为 ，这意味着输入序列是通过 简单地将特征图的空间维度展平并投影到 Transformer 维度 获得的。然后，如上所述添加了分类输入嵌入和位置嵌入，再将三者组成的整体馈入 Transformer 编码器。简单来说，就是先用 CNN 提取图像特征，然后由 CNN 提取的特征图构成图像块嵌入。由于 CNN 已经将图像降采样了</li></ul><p>参考：</p><ol><li><a href="https://blog.csdn.net/qq_39478403/article/details/118704747">https://blog.csdn.net/qq_39478403/article/details/118704747</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;一直以来都是使用卷积来处理图像数据，即使后面提出空洞卷积、特征金字塔来、注意力机制缓解其感受野受限，但是还是使用卷积计算去提取图像特征，&lt;strong&gt;受 NLP 中 transformer 的影响，VIT 将图像拆分为块 (patch)，并将图像数据学习转为图像块序列的学习问题&lt;/strong&gt;，这里图像块 (patches) 的处理方式同 NLP 的标记 (tokens) &lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>transformer：Attention Is All You Need</title>
    <link href="https://shaogui.life/2023/05/20/transformer%EF%BC%9AAttention%20Is%20All%20You%20Need/"/>
    <id>https://shaogui.life/2023/05/20/transformer%EF%BC%9AAttention%20Is%20All%20You%20Need/</id>
    <published>2023-05-20T01:31:50.000Z</published>
    <updated>2023-05-20T01:44:33.048Z</updated>
    
    <content type="html"><![CDATA[<p>为了解决RNN在序列数据上感受野不足和无法并行训练的问题的问题，Transformer被提出。Transformer 由且仅由 self-Attenion 和 Feed Forward Neural Network 组成，训练时一次输入所有时间步，构建所有时间步之间的注意力，不用考虑方向，不考虑长度</p><a id="more"></a><h1 id="什么是-transformer-？"><a href="#什么是-transformer-？" class="headerlink" title="什么是 transformer ？"></a>什么是 transformer ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/transformer-20230520093948.png" alt=""></li><li><strong>RNN</strong>：相关算法只能从左向右依次计算或者从右向左依次计算，时间片 t 的计算依赖时间片 t-1，限制模型能力；并且对于长程依赖问题，LSTM 虽然能缓解，但是不能解决</li><li><strong>Transformer</strong>：抛弃了传统的 CNN 和 RNN，整个网络结构完全是由 Attention 机制组成。更准确地讲，Transformer 由且仅由 self-Attenion 和 Feed Forward Neural Network 组成，训练时一次输入所有时间步，构建所有时间步之间的注意力，不用考虑方向，不考虑长度</li></ul><h1 id="Transformer-的数据流过程？"><a href="#Transformer-的数据流过程？" class="headerlink" title="Transformer 的数据流过程？"></a>Transformer 的数据流过程？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-05-06%2015.00.05.excalidraw.svg" alt="Drawing 2023-05-06 15.00.05.excalidraw"></li><li>假设有一个翻译任务需要学习，将阿拉伯语 (ich mochte ein bier)翻译成英语 (S i want a beer)，其在 transformer 的数据流向如上图</li><li><strong>制作源语言词库 src_vocab、目标语言词库 tgt_vocab</strong>：对要翻译的句子进行分词，假设只包含这两句话时，两个词库制作如图。其中 P 表示 padding，用于补充那些长度不够 5 的句子，S 表示开始，E 表示结束。实际使用时，src_vocab 和 tgt_vocab 的长度通常不一样</li><li><strong>获得输入的 embedding</strong>：根据源语言词库 src_vocab、目标语言词库 tgt_vocab 的长度，分别生成 encoder 输入 embedding 查询表 src_emb (5,512)、decoder 输入 embedding 查询表 tgt_emb (7,512)，根据 encoder 输入 (1,5) 构建输入 embedding (5，512)，根据 decoder 输入 (1,5)构建输入 embedding (5，512)</li><li><strong>生成位置 embedding</strong>：根据 encoder 、decoder 的输入长度生成 N+1的位置 embedding 查询表 pos 1、pos2，encoder+1 是为了表示 padding 的位置，decoder +1 是为了表示 start 的位置。位置 embedding 只要确定 N 的数量，其生成的 embedding 唯一，这里 encoder 、decoder 的输入都是 5，所以其位置 embedding 都是 (6,512)</li><li><strong>生成最终输入 embedding</strong>：输入 embedding+位置 embedding 即可，直接对应位置相加即可</li><li><strong>生成标签</strong>：decoder 解码时理论上是输入前一时刻的输出，比如 t 时刻 decoder 输出是 i，那么 t+1 decoder 就是 i。但是实际上 transformer 使用<strong>Teacher Forcing</strong>训练，直接拿 GT 的下一时刻作为标签即可，所以 t 时刻 decoder 输入 i 时，期望其输出 want，当输入是 <code>S i want a beer</code> 时，对应标签是 <code>i want a beer E</code></li></ul><h1 id="transformer-的网络结构？"><a href="#transformer-的网络结构？" class="headerlink" title="transformer 的网络结构？"></a>transformer 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/transformer-20230408151650.png" alt="transformer-20230408151650"></li><li><strong>Encoder</strong>：由 N (N=6)个完全相同的 layer 堆叠而成，每层有两个子层。第一层是 MSA (multi-head self-attention)机制，第二层是一个简单的、位置全连接的 FFN (Feed Forward Neural Network)前馈神经网络。在两个子层的每一层后采用残差连接，接着进行 layer normalization，即每个子层的输出是 $LayerNorm(x+Sublayer(x))$</li><li><strong>Decoder</strong>：由 N (N=6)个完全相同的 layer 堆叠而成，除了每个编码器层中的两个子层之外，解码器还插入第三个子层，该子层对编码器堆栈的输出执行 multi-head attention 操作，与 encoder 相似，在每个子层的后面使用了残差连接，之后采用了 layer normalization。修改了 decoder stack 中的 self-attention 子层，以防止当前位置信息中被添加进后续的位置信息。这种掩码与偏移一个位置的输出 embedding 相结合，确保对第 i 个位置的预测只能依赖小于 i 的已知输出</li></ul><h1 id="transformer-的-Positional-Encoding（位置编码）？"><a href="#transformer-的-Positional-Encoding（位置编码）？" class="headerlink" title="transformer 的 Positional Encoding（位置编码）？"></a>transformer 的 Positional Encoding（位置编码）？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/transformer-20230408151516.png" alt="transformer-20230408151516"></li><li>下图是20字 (行)的位置编码实例，词嵌入大小为512 (列)。你可以看到它从中间分裂成两半。这是因为左半部分的值由一个函数 (使用正弦)生成，而右半部分由另一个函数 (使用余弦)生成。然后将它们拼在一起而得到每一个位置编码向量</li><li>在没有 Position embedding 的 Transformer 模型并不能捕捉序列的顺序，交换单词位置后 attention map 的对应位置数值也会进行交换，并不会产生数值变化，即没有词序信息。所以这时候想要将词序信息加入到模型中</li><li>现在的 Transformer 架构还没有提取序列顺序的信息，这个信息对于序列而言非常重要，如果缺失了这个信息，可能我们的结果就是：所有词语都对了，但是无法组成有意义的语句，通过在 encoder 和 decoder 堆栈底部的输入嵌入中添加“Positional Encoding (位置编码)”</li><li>位置编码 embedding 和输入 embedding 都是 (x，512 )，可以进行相加，其中输入 embedding 通过 embedding Layer 实现，而位置编码 embedding 通过三角函数获得以下公式实现。其中 pos 表示 token 的位置，设 token 的数量是 L，则 $pos\in 0,1,2…,L$，$i\in 0,1,…,255$ 为向量的某一维度，$d_{model}=512$<script type="math/tex; mode=display">\begin{aligned}PE_{(pos,2i)}&=sin(pos/10000^{2i/d_{motel}})\\\\ PE_{(pos,2i+1)}&=cos(pos/10000^{2i/d_{motel}})\end{aligned}</script></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">#-&gt; 根据源语言库表长度及隐向量大小构建每个单词的Embedding</span></span><br><span class="line">self.src_emb = nn.Embedding(<span class="number">5</span>, <span class="number">512</span>) <span class="comment"># [1,5]--&gt;（1，5，512）</span></span><br><span class="line"><span class="comment">#-&gt; 创建位置Embedding</span></span><br><span class="line"><span class="comment">#首先创建每个位置关于隐向量长度的查询表，创建源语言库表长度+1的表格，1表示start符号的位置Embedding--&gt;(6,512)</span></span><br><span class="line">postion_embedding_init = get_sinusoid_encoding_table(<span class="number">5</span> + <span class="number">1</span>, <span class="number">512</span>)</span><br><span class="line"><span class="comment"># from_pretrained表示加载创建好的Embedding，freeze只加载不训练</span></span><br><span class="line">self.pos_emb = nn.Embedding.from_pretrained(postion_embedding_init,freeze=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 单词的Embedding+单词的位置的Embedding = Encoder输入</span></span><br><span class="line">enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">0</span>]])) <span class="comment"># （1，5，512）+ （1，5，512）=（1，5，512）</span></span><br></pre></td></tr></table></figure><h1 id="additive-attention、dot-product-multi-plicative-attention-的区别？"><a href="#additive-attention、dot-product-multi-plicative-attention-的区别？" class="headerlink" title="additive attention、dot-product (multi-plicative) attention 的区别？"></a>additive attention、dot-product (multi-plicative) attention 的区别？</h1></li><li><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-31%2021.00.04.excalidraw.svg" alt="Drawing 2023-03-31 21.00.04.excalidraw"></p></li><li><strong>additive attention</strong>：原始的 seq2seq 加注意力机制就是使用这个 additive attention，假设 C 是 encoder 编码得到的向量，解码时不是直接将这个向量输入到所有时刻，而是先经过注意力加权，加权的得到是[x，Ca]，然后连接 Linear 得到 decoder 的输入。由于 Linear 是线性转换，可以认为是加性加权</li><li><strong>Dot-Product Attention</strong>：设一个句子的长度为 S，单词 embedding 空间的维度为 D，那么一个句子就会被编码为一个大小为 S×D 的矩阵。使用三个大小为 D×d 的权重矩阵 Wq, Wk, Wv 与之相乘，会得到为三个大小为 S×d 的编码矩阵 Q, K, V， <strong>Q 代表着需要编码的词的信息， K 代表着句子中其它词的信息，相乘后得到句子中其它词的权重值； V 代表着每个位置单词蕴含的语义信息</strong></li><li>加型注意力机制和点乘注意力机制有着相同的计算复杂度，但点乘注意力机制运算可以使用高度优化的并行矩阵乘法代码，会更快也更节省空间</li></ul><h1 id="transformer-注意力的构建？"><a href="#transformer-注意力的构建？" class="headerlink" title="transformer 注意力的构建？"></a>transformer 注意力的构建？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/transformer-20230408151804.png" alt=""></li><li><strong>Attention 机制</strong>：transformer 使用<strong>Scaled Dot-Product Attention</strong>构建注意力，它通过1个 query 和 1组 key-value 对映射到一个输出，其中Q代表着需要编码的词的信息， K代表着句子中其它词的信息，相乘后得到句子中其它词的权重值； V代表着每个位置单词蕴含的语义信息，在被加权求和后作为待编码词的编码 <script type="math/tex">\operatorname{Atention}(Q,K,V)=\operatorname{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V</script></li><li><strong>Multi-Head Attention (MHA)</strong>：设一个句子的长度为 S，单词 embedding 空间的维度为 D，那么一个句子就会被编码为一个大小为 S×D 的矩阵。使用三个大小为 D×d 的 Linear 权重与之相乘，会得到为三个大小为 S×d 的编码矩阵 Q, K, V，<strong>所谓多头注意力机制</strong>，就是将 n 个这样得到的编码进行拼接，得到大小为 S×[d,…, d] (其中 d 的个数为 n )的矩阵。多头注意力的引入既丰富了注意力，也降低了构建注意力的成本</li><li><strong>encoder-decoder Attention</strong>：transformer的1个block包含1个encoder和1个decoder，两部分除了各自构建Multi-Head Attention外，还在encoder与decoder之间构建注意力。构建时，以 encoder输出作为KV和Q(gt序列)一起输入到decoder构建，注意：KV的序列长度和Q的序列长度不一定相等</li><li><strong>对比 <code>Seq2Seq</code> 过程</strong>：Q 包含待生成的句子相关信息，相当于解码器隐藏层状态 h；K，V 则来自编码器，与编码器输出的 C 类似</li></ul><h1 id="transformer-注意力的构建的数据流过程？"><a href="#transformer-注意力的构建的数据流过程？" class="headerlink" title="transformer 注意力的构建的数据流过程？"></a>transformer 注意力的构建的数据流过程？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-05-06%2022.35.49.excalidraw.svg" alt="Drawing 2023-05-06 22.35.49.excalidraw"></li><li>Step 1：$QK^T$ 产生注意力矩阵，大小为 (5,5)，表示输入的 5 个 token 之间的注意力。如果是 MMSA，这一步还会增加一个 Mask 的过程，用于屏蔽前一时刻后面时刻，比如 decoder 构建i的注意力时，不能让 i 感知到后一时刻的 want。所以 Mask 是一个上三角都是1，下三角都是0的5x5矩阵，用于乘上 Q^K 之后的注意力矩阵</li><li>Step 2：使用 softmax 归一化注意力矩阵</li><li>Step 3：$\hat x=QK^TV$ 得到加权后的特征</li><li>Step 4：$x=x+\hat x$ 残差连接得到最后输出</li><li>以上步骤是单头注意力的构建过程，如果是多头，先对隐向量表示进行分组，然后各组之间构建单头注意力，最后合并结果输出</li></ul><h1 id="transformer-的-Position-wise-feed-forward-networks-FFN-部分？"><a href="#transformer-的-Position-wise-feed-forward-networks-FFN-部分？" class="headerlink" title="transformer 的 (Position-wise feed-forward networks, FFN) 部分？"></a>transformer 的 (Position-wise feed-forward networks, FFN) 部分？</h1><ul><li><strong>基于位置的前馈网络</strong>，是两个 1 D 卷积，主要对特征进行变换，过程是 （1，5，512）-&gt;（1，2048，5）-&gt;（1，512，5）-&gt;（1，5，512），最后使用 LayerNorm 和残差连接</li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PoswiseFeedForwardNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">       <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">       attention 结构中的 FeedForward 组件, 由2个1x1的卷积形成</span></span><br><span class="line"><span class="string">       &quot;&quot;&quot;</span></span><br><span class="line">       <span class="built_in">super</span>(PoswiseFeedForwardNet, self).__init__()</span><br><span class="line">       <span class="comment">##内核大小为1的两个卷积。输入和输出的维度为 d_&#123;model&#125; =512，内层的维度为 d_&#123;ff&#125; =2048。</span></span><br><span class="line">       <span class="comment"># conv1和conv2卷积模型初始化， 输入的channel变化, d_model--&gt;d_ff --&gt;d_model</span></span><br><span class="line">       self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=<span class="number">1</span>)</span><br><span class="line">       self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=<span class="number">1</span>)</span><br><span class="line">       <span class="comment"># 层归一化,模型更加稳定</span></span><br><span class="line">       self.layer_norm = nn.LayerNorm(d_model)</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">       <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">       输出的形状应该是不变的，和输入的形状相同</span></span><br><span class="line"><span class="string">       :param inputs: torch.Size([1, 5, 512])</span></span><br><span class="line"><span class="string">       :return:  返回维度是[1, 5, 512]，</span></span><br><span class="line"><span class="string">       &quot;&quot;&quot;</span></span><br><span class="line">       residual = inputs <span class="comment"># [1, 5, 512]</span></span><br><span class="line">       <span class="comment"># torch.Size([1, 512, 5])-&gt;torch.Size([1, 512, 5]) -&gt; torch.Size([1, 2048, 5])</span></span><br><span class="line">       output = nn.ReLU()(self.conv1(inputs.transpose(<span class="number">1</span>, <span class="number">2</span>)))</span><br><span class="line">       <span class="comment"># [1, 2048, 5] -&gt; .Size([1, 512, 5])-&gt;torch.Size([1, 5, 512])</span></span><br><span class="line">       output = self.conv2(output).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">       <span class="comment"># 残差部分和层归一化 [1, 5, 512]+torch.Size([1, 5, 512])-&gt;torch.Size([1, 5, 512])</span></span><br><span class="line">       new_output = self.layer_norm(output + residual)</span><br><span class="line">       <span class="keyword">return</span> new_output</span><br></pre></td></tr></table></figure><h1 id="transformer-的-Layer-normalization？"><a href="#transformer-的-Layer-normalization？" class="headerlink" title="transformer 的 Layer normalization？"></a>transformer 的 Layer normalization？</h1></li><li><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-05-05%2016.14.36.excalidraw.svg" alt="Drawing 2023-05-05 16.14.36.excalidraw"></p></li><li>这里区别于 BN，BN 沿着每个 C (通道数量) 进行做归一化，<strong>针对不同样本的同一特征做操作</strong>，而 Layer normalization 是沿**每个 N (时间步)进行归一化</li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span></span><br><span class="line"><span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">self.eps = eps</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line"><span class="comment"># 统计每个样本所有维度的值，求均值和方差</span></span><br><span class="line"><span class="comment"># 相当于变成[bsz*max_len, hidden_dim], 然后再转回来</span></span><br><span class="line">mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) <span class="comment"># mean: [bsz, max_len, 1]</span></span><br><span class="line">std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) <span class="comment"># std: [bsz, max_len, 1]</span></span><br><span class="line"><span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure><h1 id="transformer-的-Mask-过程？"><a href="#transformer-的-Mask-过程？" class="headerlink" title="transformer 的 Mask 过程？"></a>transformer 的 Mask 过程？</h1></li><li><p>Mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask</p></li><li><strong>padding mask</strong>：在所有的 scaled dot-product attention 里面用到，因为每个批次输入序列长度是不一样的也就是说，为了对齐在较短的序列后面填充 0，后续没必要在这些位置构建注意力，所以需要屏蔽。具体的做法是，<strong>在这些位置的值加上一个非常大的负数 (负无穷)，经过 softmax，这些位置的概率就会接近0</strong></li><li><strong>sequence mask</strong>：只有在 decoder 的 self-attention 里面用到，其目的就是为了在预测未来数据时把这些未来的数据屏蔽掉，防止数据泄露。如果我们非要去串行执行 training，seq mask 其实就不需要了。具体做法是<strong>产生一个上三角矩阵，上三角的值全为 1，下三角的值全是0，对角线也是 0</strong>，当 $QK^T$ 计算完成后，使用该矩阵的某一行去盖住 label，使得 softmax 的值趋向0如预测第一个词时，decoder 输入是\<S>，后续需要盖住，直接乘上矩阵第一行即可</li></ul><h1 id="transformer-如何训练？"><a href="#transformer-如何训练？" class="headerlink" title="transformer 如何训练？"></a>transformer 如何训练？</h1><ul><li>transformer 训练一次输入2个张量，encoder 输入和 decoder 输入，其中 encoder 输入是待执行自注意力的序列，输出 k、v，decoder 输入 gt 序列，中途产生 q，用于与 k、v 构建交叉注意力，最终输出下一个序列，这也是模型输出</li><li><strong>decoder输入输出</strong>：以翻译“我爱中国”为例，encoder输入是“我爱中国”的embedding，decoder输入是”\<s> I love “，标签是”I love china”，注意：这是一次性输入”\<s> I love “3个序列，也是一次性输出”I love china”3个序列</li><li><strong>decoder的Mask</strong>：decoder输入不能让前面的序列看到后面的序列，比如构建I的注意力时，不能让他看见love(因为这是让他预测的，不能直接给答案)，所以构建注意力时，需要使用Mask矩阵过滤QK^T后的矩阵，decoder根据mask将对应位置的值设置为无穷小，这样在计算softmax时，会使其失去作用（趋近于0），进而在与v相乘时，也就忽略了v中对应的“love”和“you”的部分</li><li><strong>transformer并行训练</strong>：训练不像RNN训练时，必须拿到前一序列的预测结果，再预测下一个序列，因此可以被并行训训练</li></ul><h1 id="transformer-如何预测？"><a href="#transformer-如何预测？" class="headerlink" title="transformer 如何预测？"></a>transformer 如何预测？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-31%2023.21.57.excalidraw.svg" alt="Drawing 2023-03-31 23.21.57.excalidraw"></li><li>transformer 推理一次输入2个张量，encoder 输入和 decoder 输入，其中 encoder 输入是待执行自注意力的序列，输出 k、v，decoder 输入 gt 序列，中途产生 q，用于与 k、v 构建交叉注意力，最终输出下一个序列，这也是模型输出</li><li><strong>Decoder输入输出</strong>：以翻译“我爱中国”为例，encoder输入是“我爱中国”的embedding，</li><li>decoder首先输入是”\<s>“，得到输出O1(可能等于love)后，将”O1”再次输入decoder，输出O2(可能等于china)，一直以此循环，知道模型输出结束符</li><li><strong>Shifted Right</strong>：当前时刻输出作为下一时刻 decoder 的输入的过程</li></ul><h1 id="Transformer-各部分耗时比较？"><a href="#Transformer-各部分耗时比较？" class="headerlink" title="Transformer 各部分耗时比较？"></a>Transformer 各部分耗时比较？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/home-20230411183757-2.png" alt=""></li><li>当模型大了后，真正消耗算力（FLOPS）的，还是 MLP（FFN），自注意力（attn）和多头（MHA）只占小部分</li></ul><h1 id="Transformer-的并行性体现在哪里？"><a href="#Transformer-的并行性体现在哪里？" class="headerlink" title="Transformer 的并行性体现在哪里？"></a>Transformer 的并行性体现在哪里？</h1><ul><li>RNN 之所以不支持并行化是因为它天生是个时序结构，t 时刻依赖 t-1时刻的输出，而 t-1时刻又依赖 t-2时刻，如此循环往前</li><li><strong>Encoder 训练、推理并行</strong>：在构建 multi-head 隐向量注意力时，有对隐向量进行分组，在分组内构建全局注意力，分组之间不影响，所以分组之间的注意力可以并行构建</li><li><strong>Decoder 训练并行、推理不并行</strong>：训练阶段的 Decoder，如果使用<strong>teacher force</strong>训练方式，不用获取上一时刻输出即可训练，此时可以进行并行化，但是在测试阶段，因为我们没有正确的目标语句，t 时刻的输入必然依赖 t-1时刻的输出，这时跟之前的 seq2seq 就没什么区别了。</li></ul><h1 id="Transformer-的-MHA-如何实现并行计算的？"><a href="#Transformer-的-MHA-如何实现并行计算的？" class="headerlink" title="Transformer 的 MHA 如何实现并行计算的？"></a>Transformer 的 MHA 如何实现并行计算的？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Pasted%20image%2020230505142606.png" alt=""></li><li>在构建 multi-head 隐向量注意力时，有对隐向量进行分组，在分组内构建全局注意力，分组之间不影响，所以分组之间的注意力可以并行构建</li></ul><h1 id="MHA-的时间复杂度计算？"><a href="#MHA-的时间复杂度计算？" class="headerlink" title="MHA 的时间复杂度计算？"></a>MHA 的时间复杂度计算？</h1><ul><li>MHA 的计算过程是 Q (b, n, l)与 K (b, m, l)做矩阵点乘，得到注意力矩阵 (b, n, m)，对于矩阵的每个位置，需要做 l 次乘法，所以时间复杂度是 $n\times m\times l$</li></ul><h1 id="Transformer-的注意力矩阵含义？"><a href="#Transformer-的注意力矩阵含义？" class="headerlink" title="Transformer 的注意力矩阵含义？"></a>Transformer 的注意力矩阵含义？</h1><ul><li>以 Pytorch 为例，输入到 transformer 的序列有两个，分别是 src、tgt，其中 src 可以是 (S, E)或 (N, S, E)，tgt 是 (T, E)或 (N, T, E)，，其中 S 是源序列长度、T 是目标序列长度、N 是 batch size、E 是特征长度</li><li>假设是 src (S, E)与 tgt (T, E)构建矩阵时，则其注意力矩阵大小是 (T, S); 假设是 src (N, S, E)与 tgt (N, T, E)构建矩阵时，构建注意力时，取出某个 batch 计算，N 个 batch 得到注意力矩阵是 (N, T, E)</li><li><strong>总结</strong>：1)注意力矩阵是目标序列与源序列之间的关系，目标序列与源序列长度可以不相等；2)多个batch size构建注意力时，只在某个batch内构建注意力，不涉及跨batch；3)从目标序列来看，输入(N, T, E)，输出也是(N, T, E)，相当于拿Q去查询得到定长的输出</li></ul><h1 id="Transformer-为什么-Q-K-使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？"><a href="#Transformer-为什么-Q-K-使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？" class="headerlink" title="Transformer 为什么 Q K 使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？"></a>Transformer 为什么 Q K 使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？</h1><ul><li>这里的 Q、K 使用不同权重矩阵生成的意思是，当 encoder 输入 QKV 时，Q=K=V，在计算 QK 前，需要先经过权重矩阵计算，所以问题是为什么不直接进行 QK 计算注意力矩阵<script type="math/tex; mode=display">\begin{array}{l}Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V\\ Attention(Q,K,V)=softmax(\frac{QW_qW_k^TK^T}{\sqrt{d_k}})V \end{array}</script></li><li><strong>说法 1</strong>：使用 Q、K 不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。如果 QK 相等，那么 QK 计算得到的矩阵是对称矩阵，这样 self-attention 就退化成一个 point-wise 线性映射，对于注意力上的表现力不够强</li><li><strong>说法 2</strong>：BETR 证明即使 Q=K，也不影响结果</li></ul><h1 id="Transformer-在-QK之后，为什么要进行归一化？"><a href="#Transformer-在-QK之后，为什么要进行归一化？" class="headerlink" title="Transformer 在 QK之后，为什么要进行归一化？"></a>Transformer 在 QK之后，为什么要进行归一化？</h1><ul><li><strong>在输入数量较大时，softmax将几乎全部的概率分布都分配给了最大值对应的标签</strong>。也就是说<strong>极大的点积值将整个 softmax 推向梯度平缓区，使得收敛困难</strong>，<strong>梯度消失为0，造成参数更新困难</strong></li></ul><h1 id="Transformer-在-QK之后，softmax-对哪一维度进行计算？含义是什么？"><a href="#Transformer-在-QK之后，softmax-对哪一维度进行计算？含义是什么？" class="headerlink" title="Transformer 在 QK之后，softmax 对哪一维度进行计算？含义是什么？"></a>Transformer 在 QK之后，softmax 对哪一维度进行计算？含义是什么？</h1><ul><li>对最后一维计算 softmax 值，对某个 Q 上的序列来说，表示所有 K 上序列对其的注意力，累加在一起等于 1 </li></ul><h1 id="Transformer-进行-MHA-时，每个-head-的-token-的隐变量长度变为-L-n-head"><a href="#Transformer-进行-MHA-时，每个-head-的-token-的隐变量长度变为-L-n-head" class="headerlink" title="Transformer 进行 MHA 时，每个 head 的 token 的隐变量长度变为 L/n_head?"></a>Transformer 进行 MHA 时，每个 head 的 token 的隐变量长度变为 L/n_head?</h1><ul><li>Transformer 的多头注意力看上去是借鉴了 CNN 中同一卷积层内使用多个卷积核的思想，原文中使用了 8 个 head，在同一“multi-head attention”层中，输入均为“KQV”，<strong>同时</strong>进行注意力的计算，彼此之前<strong>参数不共享</strong>，最终将结果<strong>拼接</strong>起来，这样可以允许模型在<strong>不同的表示子空间里学习到相关的信息</strong></li><li><strong>总结</strong>：希望每个注意力头，只关注最终输出序列中一个子空间，互相<strong>独立</strong>。其核心思想在于，抽取到更加丰富的<strong>特征信息</strong></li></ul><h1 id="为什么在进行-softmax-之前需要对-attention-进行-scaled？"><a href="#为什么在进行-softmax-之前需要对-attention-进行-scaled？" class="headerlink" title="为什么在进行 softmax 之前需要对 attention 进行 scaled？"></a>为什么在进行 softmax 之前需要对 attention 进行 scaled？</h1><ul><li><strong>在输入数量较大时，softmax 将几乎全部的概率分布都分配给了最大值对应的标签</strong>。也就是说极大的点积值将整个 softmax 推向梯度平缓区，使得收敛困难，<strong>梯度消失为0，造成参数更新困难</strong></li></ul><h1 id="Transformer-在哪里做了权重共享，为什么可以做权重共享？好处是什么？"><a href="#Transformer-在哪里做了权重共享，为什么可以做权重共享？好处是什么？" class="headerlink" title="Transformer 在哪里做了权重共享，为什么可以做权重共享？好处是什么？"></a>Transformer 在哪里做了权重共享，为什么可以做权重共享？好处是什么？</h1><ul><li><strong>Encoder 和 Decoder 间的 Embedding 层权重共享</strong>：源语言和目标语言是不一样的，但它们可以共用一张大词表，对于两种语言中共同出现的词（比如：数字，标点等等）可以得到更好的表示，而且对于Encoder和Decoder，<strong>嵌入时都只有对应语言的embedding会被激活</strong>，因此是可以共用一张词表做权重共享的</li><li><strong>Decoder 中 Embedding 层和 FC 层权重共享</strong>：Embedding 层参数维度是：(v, d)，FC 层参数维度是：(d, v)，其中v是词表大小，d是embedding维度，转置后直接共用</li></ul><h1 id="如何将-trasformer-应用到计算机视觉？"><a href="#如何将-trasformer-应用到计算机视觉？" class="headerlink" title="如何将 trasformer 应用到计算机视觉？"></a>如何将 trasformer 应用到计算机视觉？</h1><ul><li>身心兼备：所谓“身心兼备”，就是使用 trasformer 的结果，将计算机视觉问题转为序列学习问题。对于一张图片，每4x4相邻的像素为一个 Patch，然后在 channel 方向展平（flatten）。假设输入的是 RGB 三通道图片，那么每个 patch 就有4x4=16个像素，然后每个像素有 R、G、B 三个值所以展平后是16x3=48，所以通过 Patch Partition 后图像 shape 由 [H, W, 3]变成了[H/4, W/4, 48]，如 vit、SETR、DETR 等</li><li>只取灵魂：所谓“只取灵魂”，就是只使用自注意力机制，不使用trasformer结构，如OCRNet</li></ul><h1 id="Transformer-、RNN、CNN-网络的区别？"><a href="#Transformer-、RNN、CNN-网络的区别？" class="headerlink" title="Transformer 、RNN、CNN 网络的区别？"></a>Transformer 、RNN、CNN 网络的区别？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/transformer-20230408151724.png" alt=""></li><li><strong>复杂度</strong>：self-attention layer 用常数次 O ( 1 ) 的操作连接所有位置，而 recurrent layer 需要O (n)顺序操作。在计算复杂度方面，当序列长度 N 小于表示维度 D 时，self-attention layers 比 recurrent layers 更快。为了提高包含很长序列的任务的计算性能，可以仅在以输出位置为中心，半径为r的的领域内使用self-attention</li><li><strong>并行化</strong>：Transformer 可以并行训练，RNN 必须挨个时间步训练</li></ul><p>参考：</p><ol><li><a href="https://blog.csdn.net/nocml/article/details/103082600">Transformer(一)—论文翻译：Attention Is All You Need 中文版_transformer论文翻译_吕秀才的博客-CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/366993073">additive attention与dot-product attention - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/368243631">Transformer的模型和代码解析 - 知乎</a></li><li><a href="https://www.zhihu.com/question/487766088">为什么Transformer要用LayerNorm？ - 知乎</a></li><li><a href="https://blog.csdn.net/qq_44766883/article/details/112008655">机器学习-31-Transformer详解以及我的三个疑惑和解答_transformer不收敛_迷雾总会解的博客-CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/157884112">1分钟|聊聊Transformer的并行化 - 知乎</a></li><li><a href="https://blog.csdn.net/qq_42599237/article/details/123383691">https://blog.csdn.net/qq_42599237/article/details/123383691</a></li><li><a href="https://zhuanlan.zhihu.com/p/97451231">https://zhuanlan.zhihu.com/p/97451231</a></li><li><a href="https://www.zhihu.com/question/593941226">transformer为什么有利于并行计算？ - 知乎</a></li><li><a href="https://www.zhihu.com/question/319339652">transformer中为什么使用不同的K 和 Q， 为什么不能使用同一个值？ - 知乎</a></li><li><a href="https://www.zhihu.com/question/369075515">在测试或者预测时，Transformer里decoder为什么还需要seq mask？ - 知乎</a></li><li><a href="https://www.zhihu.com/question/350369171">transformer中multi-head attention中每个head为什么要进行降维？ - 知乎</a></li><li><a href="https://www.zhihu.com/question/347678607#:~:text=%E4%B8%BA%E4%BB%80%E4%B9%88%20Transformer%20%E9%9C%80%E8%A6%81%20positional%20encoding%20%EF%BC%9F%20%E5%9C%A8%E6%B2%A1%E6%9C%89,Position%20embedding%20%E7%9A%84%20Transformer%20%E6%A8%A1%E5%9E%8B%E5%B9%B6%E4%B8%8D%E8%83%BD%E6%8D%95%E6%8D%89%E5%BA%8F%E5%88%97%E7%9A%84%E9%A1%BA%E5%BA%8F%EF%BC%8C%E4%BA%A4%E6%8D%A2%E5%8D%95%E8%AF%8D%E4%BD%8D%E7%BD%AE%E5%90%8E%20attention%20map%20%E7%9A%84%E5%AF%B9%E5%BA%94%E4%BD%8D%E7%BD%AE%E6%95%B0%E5%80%BC%E4%B9%9F%E4%BC%9A%E8%BF%9B%E8%A1%8C%E4%BA%A4%E6%8D%A2%EF%BC%8C%E5%B9%B6%E4%B8%8D%E4%BC%9A%E4%BA%A7%E7%94%9F%E6%95%B0%E5%80%BC%E5%8F%98%E5%8C%96%EF%BC%8C%E5%8D%B3%E6%B2%A1%E6%9C%89%E8%AF%8D%E5%BA%8F%E4%BF%A1%E6%81%AF%E3%80%82">如何理解Transformer论文中的positional encoding，和三角函数有什么关系？ - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/613407791">Bert/Transformer 被忽视的细节（或许可以用来做面试题） - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;为了解决RNN在序列数据上感受野不足和无法并行训练的问题的问题，Transformer被提出。Transformer 由且仅由 self-Attenion 和 Feed Forward Neural Network 组成，训练时一次输入所有时间步，构建所有时间步之间的注意力，不用考虑方向，不考虑长度&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>Segment Anything</title>
    <link href="https://shaogui.life/2023/05/20/Segment%20Anything/"/>
    <id>https://shaogui.life/2023/05/20/Segment%20Anything/</id>
    <published>2023-05-20T01:16:04.000Z</published>
    <updated>2023-05-20T01:27:47.334Z</updated>
    
    <content type="html"><![CDATA[<p>SAM通过transformer将点、框、Mask、文本等prompt和图片进行编码学习，可以实现对图片任意目标的分割</p><a id="more"></a><h1 id="什么是-SAM-？"><a href="#什么是-SAM-？" class="headerlink" title="什么是 SAM ？"></a>什么是 SAM ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAM-3.png" alt=""></li><li>a)SAM 利用“图片-分割提示”实现对图片上任意目标的分割，分割提示包括：点、框、Mask、文本</li><li>b) SAM 首先利用 prompt encoder 编码”分割提示”，利用 image encoder 编码“图片”，然后通过 Mask decoder 解析输出 Mask</li><li>c)SAM 利用数据驱动去做模型训练，模型输出结果后再输入模型训练</li></ul><h1 id="SAM-的网络结构？"><a href="#SAM-的网络结构？" class="headerlink" title="SAM 的网络结构？"></a>SAM 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAM.png" alt=""></li><li><strong>image encoder</strong>：类似 VIT 的过程，输入 image (1,3, H, W), 输出 image_embedding (1, C, H/16, W/16)，即 (1, HW/256, C)的 tokens 表示</li><li><strong>mask</strong>：mask prompt，直接和image_embedding相加即可</li><li><strong>prompt encoder</strong>：包含3种提示的编码过程，其中点、框按位置被编码为Pos embedding(1,N,C)，文本通过clip模型被编码为Pos embedding(1,M,C)</li><li><strong>mask decoder</strong>：根据image_embedding和prompt encoder输出，结合IOU tokens(1,1,C)和mask tokens(1,P,C)，解析出目标mask(1,1+P+N+M, H/16, W/16)和iou(1,1+P+N+M)</li></ul><h1 id="SAM-的-image-encoder？"><a href="#SAM-的-image-encoder？" class="headerlink" title="SAM 的 image encoder？"></a>SAM 的 image encoder？</h1><ul><li>类似 VIT 的 encoder 过程，输入 image (1,3, H, W), 输出 image_embedding (1, C, H/16, W/16)，即 (1, HW/256, C)的 tokens 表示</li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> image_encoder=ImageEncoderViT(..)</span><br><span class="line"> <span class="comment"># batched_input=&#123;List,List&#125; -&gt; torch.Size([2, 3, 1024, 1024])</span></span><br><span class="line"> input_images = torch.stack([preprocess(x[<span class="string">&quot;image&quot;</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> batched_input], dim=<span class="number">0</span>) </span><br><span class="line"><span class="comment"># torch.Size([2, 3, 1024, 1024]) -&gt; torch.Size([2, 256, 64, 64])</span></span><br><span class="line"> image_embeddings = image_encoder(input_images)</span><br></pre></td></tr></table></figure><h1 id="SAM-的-prompt-encoder"><a href="#SAM-的-prompt-encoder" class="headerlink" title="SAM 的 prompt encoder?"></a>SAM 的 prompt encoder?</h1></li><li><p>包含3种提示的编码过程，其中点、框按位置被编码为 Pos embedding (1, N, C)，文本通过 clip 模型被编码为 Pos embedding (1, M, C)，最终输出（1,N+M,C )的稀疏编码sparse_embeddings</p></li><li><strong>point&amp;box</strong>：每个点编码为1个 pos embedding，每个 box 编码为2个 pos embedding（box 被两个点定义）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"> embed_dim=<span class="number">256</span></span><br><span class="line"> num_point_embeddings: <span class="built_in">int</span> = <span class="number">4</span>  <span class="comment"># pos/neg point + 2 box corners</span></span><br><span class="line"> point_embeddings = [nn.Embedding(<span class="number">1</span>, embed_dim) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_point_embeddings)]</span><br><span class="line"> point_embeddings = nn.ModuleList(point_embeddings)</span><br><span class="line"> not_a_point_embed = nn.Embedding(<span class="number">1</span>, embed_dim)</span><br><span class="line"><span class="comment"># point prompt</span></span><br><span class="line">points = points + <span class="number">0.5</span>  <span class="comment"># Shift to center of pixel  </span></span><br><span class="line"><span class="comment"># 根据点位置points，在输入(1024,1024)的基础上生成pos embedding</span></span><br><span class="line"> point_embedding = pe_layer.forward_with_coords(points, input_image_size) <span class="comment">#torch.Size([1,3,2])+(1024,1024)-&gt;torch.Size([1,3,256])</span></span><br><span class="line"> <span class="comment"># 点有3类，-1表示非嵌入点，此时不使用pos embedding，0表示正样本点，1表示负样本点</span></span><br><span class="line">point_embedding[labels == -<span class="number">1</span>] = <span class="number">0.0</span></span><br><span class="line"> point_embedding[labels == -<span class="number">1</span>] += not_a_point_embed.weight</span><br><span class="line"> point_embedding[labels == <span class="number">0</span>] += point_embeddings[<span class="number">0</span>].weight</span><br><span class="line"> point_embedding[labels == <span class="number">1</span>] += point_embeddings[<span class="number">1</span>].weight</span><br><span class="line"><span class="comment"># box prompt</span></span><br><span class="line"> boxes = boxes + <span class="number">0.5</span>  <span class="comment"># Shift to center of pixel</span></span><br><span class="line"> coords = boxes.reshape(-<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>) <span class="comment"># 一个框肯定2个点</span></span><br><span class="line"> corner_embedding = pe_layer.forward_with_coords(coords, input_image_size)</span><br><span class="line"> corner_embedding[:, <span class="number">0</span>, :] += point_embeddings[<span class="number">2</span>].weight <span class="comment">#框第一个点</span></span><br><span class="line"> corner_embedding[:, <span class="number">1</span>, :] += point_embeddings[<span class="number">3</span>].weight <span class="comment">#框第二个点</span></span><br><span class="line"><span class="comment"># 汇总point、box编码</span></span><br><span class="line">sparse_embeddings = torch.empty((<span class="number">1</span>, <span class="number">0</span>, embed_dim))</span><br><span class="line">sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=<span class="number">1</span>)</span><br><span class="line">sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><strong>text</strong>：通过CLIP模型将文本编码到(1,M,C)</li></ul><h1 id="SAM的mask-prompt如何处理？"><a href="#SAM的mask-prompt如何处理？" class="headerlink" title="SAM的mask prompt如何处理？"></a>SAM的mask prompt如何处理？</h1><ul><li>mask利用CNN输出和image_embedding(1,C,H/16,W/16)一样大小的编码，后续直接相加</li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mask_downscaling = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, mask_in_chans // <span class="number">4</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    LayerNorm2d(mask_in_chans // <span class="number">4</span>),</span><br><span class="line">    activation(),</span><br><span class="line">    nn.Conv2d(mask_in_chans // <span class="number">4</span>, mask_in_chans, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    LayerNorm2d(mask_in_chans),</span><br><span class="line">    activation(),</span><br><span class="line">    nn.Conv2d(mask_in_chans, embed_dim, kernel_size=<span class="number">1</span>),</span><br><span class="line">)</span><br><span class="line">mask_input_size = (<span class="number">4</span> * image_embedding_size[<span class="number">0</span>], <span class="number">4</span> * image_embedding_size[<span class="number">1</span>])</span><br><span class="line">no_mask_embed = nn.Embedding(<span class="number">1</span>, embed_dim) </span><br><span class="line"><span class="keyword">if</span> masks <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    dense_embeddings = self._embed_masks(masks) <span class="comment"># 利用CNN生成mask embedding</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    dense_embeddings = self.no_mask_embed.weight.reshape(<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).expand(</span><br><span class="line">        bs, -<span class="number">1</span>, self.image_embedding_size[<span class="number">0</span>], self.image_embedding_size[<span class="number">1</span>]</span><br><span class="line">    ) <span class="comment"># 随机初始化生成mask embedding</span></span><br></pre></td></tr></table></figure><h1 id="SAM-的-mask-decoder"><a href="#SAM-的-mask-decoder" class="headerlink" title="SAM 的 mask decoder?"></a>SAM 的 mask decoder?</h1></li><li><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAM-4.png" alt=""></p></li><li><strong>输入</strong>:image_embedding(1, C, H/16, W/16)、image_embedding大小的位置编码image_pe(1, C, H/16, W/16)、稀疏提示编码sparse_prompt_embeddings(1, N, C)、密集提示编码dense_prompt_embeddings(1,C,H/16, W/16)</li><li><strong>(1)tansformer整合所有编码</strong>:将image_embedding+dense_prompt_embeddings视为transformer encoder的k,image_pe视为pos embedding,sparse_prompt_embeddings视为decoder的q，并且参考VIT的class_token，不直接使用sparse_prompt_embeddings输出作为最终结果，而是另外生成1个iou token和P个mask token作为最终结果，所以输入transformer decoder的token变为(1,1+P+N,C)，经过transformer后decoder和encoder分别输出hs(1,1+P+N,C), src(1,HW/256,C)；<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"> num_multimask_outputs=<span class="number">3</span></span><br><span class="line">transformer_dim=<span class="number">256</span></span><br><span class="line"> iou_token = nn.Embedding(<span class="number">1</span>, transformer_dim)</span><br><span class="line"> num_mask_tokens = num_multimask_outputs + <span class="number">1</span></span><br><span class="line"> mask_tokens = nn.Embedding(num_mask_tokens, transformer_dim)</span><br><span class="line"> <span class="comment"># Concatenate output tokens</span></span><br><span class="line"> output_tokens = torch.cat([iou_token.weight, mask_tokens.weight], dim=<span class="number">0</span>) <span class="comment"># torch.Size([5, 256])</span></span><br><span class="line"> output_tokens = output_tokens.unsqueeze(<span class="number">0</span>).expand(sparse_prompt_embeddings.size(<span class="number">0</span>), -<span class="number">1</span>, -<span class="number">1</span>) <span class="comment"># torch.Size([1, 5, 256])</span></span><br><span class="line"> tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=<span class="number">1</span>) <span class="comment"># torch.Size([1, 12, 256])</span></span><br><span class="line"> <span class="comment"># Expand per-image data in batch direction to be per-mask</span></span><br><span class="line"> src = torch.repeat_interleave(image_embeddings, tokens.shape[<span class="number">0</span>], dim=<span class="number">0</span>) <span class="comment"># torch.Size([1, 256, 64, 64]) -》torch.Size([1, 256, 64, 64])</span></span><br><span class="line"> src = src + dense_prompt_embeddings <span class="comment"># torch.Size([1, 256, 64, 64])+torch.Size([1, 256, 64, 64])</span></span><br><span class="line"> pos_src = torch.repeat_interleave(image_pe, tokens.shape[<span class="number">0</span>], dim=<span class="number">0</span>) <span class="comment"># torch.Size([1, 256, 64, 64])</span></span><br><span class="line"> b, c, h, w = src.shape</span><br><span class="line"> <span class="comment"># Run the transformer torch.Size([1, 256, 64, 64])，torch.Size([1, 256, 64, 64])，torch.Size([1, 12, 256])</span></span><br><span class="line"> hs, src = transformer(src, pos_src, tokens) <span class="comment"># torch.Size([1, 12, 256]) torch.Size([1, 4096, 256]) = q,k</span></span><br><span class="line"> iou_token_out = hs[:, <span class="number">0</span>, :] <span class="comment"># torch.Size([1, 256])</span></span><br><span class="line"> mask_tokens_out = hs[:, <span class="number">1</span> : (<span class="number">1</span> + num_mask_tokens), :] <span class="comment"># torch.Size([1, 4, 256])</span></span><br></pre></td></tr></table></figure></li><li><strong>(2)生成Mask预测</strong>：取hs的第1-P个token作为预测结果mask_tokens_out，src经过反卷积上采样4倍，输出upscaled_embedding(1,HW/16,C’)，mask_tokens_out经过MLP操作，将隐变量长度变为C’,即输出hyper_in(1,P,C’)，hyper_in与upscaled_embedding点乘后输出masks(1,P,HW/16)，表示p个mask<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">self.output_upscaling = nn.Sequential(</span><br><span class="line">    nn.ConvTranspose2d(transformer_dim, transformer_dim // <span class="number">4</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    LayerNorm2d(transformer_dim // <span class="number">4</span>),</span><br><span class="line">    activation(),</span><br><span class="line">    nn.ConvTranspose2d(transformer_dim // <span class="number">4</span>, transformer_dim // <span class="number">8</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    activation(),</span><br><span class="line">)</span><br><span class="line">self.output_hypernetworks_mlps = nn.ModuleList(</span><br><span class="line">    [MLP(transformer_dim, transformer_dim, transformer_dim // <span class="number">8</span>, <span class="number">3</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_mask_tokens)])  </span><br><span class="line"><span class="comment"># Upscale mask embeddings and predict masks using the mask tokens</span></span><br><span class="line">src = src.transpose(<span class="number">1</span>, <span class="number">2</span>).view(b, c, h, w) <span class="comment"># torch.Size([1, 256, 64, 64])</span></span><br><span class="line">upscaled_embedding = self.output_upscaling(src) <span class="comment"># torch.Size([1, 256, 64, 64]) -》torch.Size([1, 32, 256, 256])</span></span><br><span class="line">hyper_in_list: List[torch.Tensor] = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_mask_tokens):</span><br><span class="line">    hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :])) <span class="comment"># torch.Size([1, 32])x4</span></span><br><span class="line">hyper_in = torch.stack(hyper_in_list, dim=<span class="number">1</span>) <span class="comment"># torch.Size([1, 4, 32])</span></span><br><span class="line">b, c, h, w = upscaled_embedding.shape <span class="comment"># torch.Size([1, 32, 256, 256])</span></span><br><span class="line"><span class="comment"># 运算符@表示矩阵的点乘</span></span><br><span class="line">masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -<span class="number">1</span>, h, w) <span class="comment"># torch.Size([1, 4, 32]) @ torch.Size([1, 32, 256, 256]) -&gt; torch.Size([1, 4, 256, 256])</span></span><br></pre></td></tr></table></figure></li><li><p><strong>(3)生成IOU预测</strong>：取hs的第1个token作为预测结果iou_token_out，然后使用MLP将隐变量长度变为P，表示P各mask的iou预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">iou_prediction_head = MLP(transformer_dim, iou_head_hidden_dim, num_mask_tokens, iou_head_depth)</span><br><span class="line"><span class="comment"># Generate mask quality predictions</span></span><br><span class="line">iou_pred = iou_prediction_head(iou_token_out) <span class="comment"># torch.Size([1,256]) -&gt; torch.Size([1, 4])  </span></span><br></pre></td></tr></table></figure><h1 id="SAM-如何直接分割所有目标？"><a href="#SAM-如何直接分割所有目标？" class="headerlink" title="SAM 如何直接分割所有目标？"></a>SAM 如何直接分割所有目标？</h1></li><li><p>以原图所有cell作为point prompt输入，输出Mask和iou后，通过iou阈值过滤mask,得到所有目标的mask</p></li></ul><p>参考：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/621040230">模型方法—-真的分割任何东西(Segment Anything) - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;SAM通过transformer将点、框、Mask、文本等prompt和图片进行编码学习，可以实现对图片任意目标的分割&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
    <category term="分割" scheme="https://shaogui.life/tags/%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>人工神经网络(ANN)的理解</title>
    <link href="https://shaogui.life/2023/05/09/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(ANN)%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://shaogui.life/2023/05/09/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(ANN)%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2023-05-09T01:22:01.000Z</published>
    <updated>2023-05-20T01:27:09.558Z</updated>
    
    <content type="html"><![CDATA[<p>本文用于解释最基础的神经网络-感知机的原理，它是后续 CNN、RNN 的基础，CNN 只是在感知机的概念加入“局部连接”的思想，每个局部执行的还是加权和；RNN 只是给每个隐藏层加一个隐状态</p><p>本文按照：感知机-&gt;多层感知机-&gt;全连接层-&gt;人工神经网络的步骤去理解 Linear 层</p><a id="more"></a><h1 id="什么是感知机-Perceptron-？"><a href="#什么是感知机-Perceptron-？" class="headerlink" title="什么是感知机 (Perceptron)？"></a>什么是感知机 (Perceptron)？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-26%2010.55.05.excalidraw.png" alt="Drawing 2023-03-26 10.55.05.excalidraw"></li><li><strong>一种用于单类别分类监督学习的算法</strong>，输入一组特征向量，然后通过一组等长权重计算线性加权和，最后通过判别函数输出结果。如果使用以下判别函数，则是一个二元分类器 <script type="math/tex; mode=display">f(x)=\operatorname{sign}(w * x+b)=\left\{\begin{array}{ll} +1 & x \geq 0 \\ -1 & x<0 \end{array}\right.</script></li><li>感知机模拟的是人的神经细胞，单个神经细胞有 2 种状态，激活与不激活，当信号总量超过某个阈值时，该神经细胞处于激活状态，否则不激活，激活时向其他连接的神经元传递信息</li></ul><p>一个感知机包含等于输入数据长度的权重 w 和 1 个偏置 b（因为输出是 1），所谓感知，就是利用权重对所有输入进行加权和操作。接下来同时使用多个感知机去感知输入，就构成“单层感知机 (SLP)”</p><h1 id="什么是单层感知机-SLP"><a href="#什么是单层感知机-SLP" class="headerlink" title="什么是单层感知机 (SLP)?"></a>什么是单层感知机 (SLP)?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-26%2010.55.05.excalidraw.png" alt="Drawing 2023-03-26 10.55.05.excalidraw"></li><li>所谓单层感知机，就是<strong>由感知机组成的一层网络，没有多层感知机的 hidden layer，每个感知机连接都连接所有输入</strong>，注意：感知机本身可以理解为只有 1 个节点的单层感知机</li></ul><p>除了在单层使用多个感知机外，还可以使用多层感知机，每层感知机的输入是前一层前一层感知机的输出，由此出现“多层感知机 (MLP)”</p><h1 id="什么是多层感知机-MLP"><a href="#什么是多层感知机-MLP" class="headerlink" title="什么是多层感知机 (MLP)?"></a>什么是多层感知机 (MLP)?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-26%2010.55.05.excalidraw.png" alt="Drawing 2023-03-26 10.55.05.excalidraw"></li><li><strong>将多个单层感知机堆叠起来，其中每个感知机机与所有输入或者所有感知机输出连接，输入层与输出层之间的所有层是隐藏层</strong></li><li><p>MLP 至少由 3 层感知机组成，即<strong>输入层、隐藏层、输出层</strong></p></li><li><p>感知机、单层感知机、多层感知机的区别？</p><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(ANN" alt="人工神经网络(ANN)-20230408150310">-20230408150310.png)</li><li><strong>感知机 (Perceptron)</strong>：左图是一个 2 输入的感知机模型，经过参数加权累加和后，其值在一个<strong>平面</strong>上</li><li><strong>单层感知机 (SLP)</strong>：中图是一个 2 输入、包含 3 个输出节点的单层感知机，经过参数加权累加和后输出，输出是超平面</li><li><strong>多层感知机 (MLP)</strong>：多个单层感知机组成，至少包括 1 层隐藏层，输出结果是多个多个<strong>超平面</strong>的组合</li><li><strong>感知机无论叠加多少层，只要没使用<mark style="background: #FF5582A6;">非线性激活函数</mark>，其结果都是线性可分的，无法处理非线性问题</strong></li></ul></li></ul><p>但是使用任意个、任意层感知机都是线性函数 $Wx+b$ 的线性叠加，其处理的问题都是线性可分的，但是对于线性不可分问题无法解决</p><h1 id="什么是线性可分？"><a href="#什么是线性可分？" class="headerlink" title="什么是线性可分？"></a>什么是线性可分？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-26%2014.23.58.excalidraw.svg" alt="Drawing 2023-03-26 14.23.58.excalidraw"></li><li><strong>线性可分</strong>：计算所有样本的凸包，如果凸包内值包含 1 个类别，就是线性可分，本质就是数据可以被有限个超平面区分，在 2D 数据上就是被有限个直线做划分</li><li><strong>线性不可分</strong>：计算所有样本的凸包，如果凸包内值包含 2 个类别以上，则是非线性可分</li></ul><p>日常构建网络时，都是使用 <code>torch.Linear</code> 去构建全连接层，它和感知机是什么关系呢，实际上它是确定输入维度、输出维度的单层感知机</p><h1 id="什么是全连接层-Fully-Connected-Layer"><a href="#什么是全连接层-Fully-Connected-Layer" class="headerlink" title="什么是全连接层 (Fully Connected Layer)?"></a>什么是全连接层 (Fully Connected Layer)?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-26%2015.29.48.excalidraw.svg" alt="Drawing 2023-03-26 15.29.48.excalidraw"></li><li>其实<strong>就是单层的感知机</strong>，任意一个感知机都与所有输入连接形成，定义时需要指定输入数据维度和感知机数量，其中感知机的数量等于输出的维度</li><li><strong>参数量</strong>：每个感知机都与输入一一相连，加上偏置，共用参数 $\mathrm{Param}<em>{\text {linear }}=C</em>{\text {in }} \times C<em>{\text {out }}+C</em>{\text {out }}$</li><li><strong>计算量</strong>：全连接层的<strong>每个输出</strong>需要经过 $C<em>{in}$ 的乘法和加法运算，没考虑 bias 需要减1，$F L O P</em>{\text {linear }}=(2 \times C<em>{\text {in }}) \times C</em>{\text {out }} \times Batchsize$</li><li>Pytorch 可以使用接口快速定义该层<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=torch.randn([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">fc = torch.nn.Linear(in_features = <span class="number">2</span>, out_features = <span class="number">3</span>)</span><br><span class="line">y=fc(x) // y=[<span class="number">1</span>,<span class="number">3</span>]</span><br></pre></td></tr></table></figure>单层感知机加上激活函数，就可以构建人工神经网络 (ANN) </li></ul><h1 id="什么是人工神经网络-ANN-？"><a href="#什么是人工神经网络-ANN-？" class="headerlink" title="什么是人工神经网络 (ANN) ？"></a>什么是人工神经网络 (ANN) ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/MenuqjAChEyAwrQdOWJoWi4FHpKQ551d1d2kcUoW7jknzv9F_i4-MHztszxy1H4fPSoqPKdjNO51C3mZKtlppVQljk-iYKTbFmNPxm19CYwxSlf0o6oMtwmI4Uq1ma2p.gif" alt=""></li><li>ANN 其实就是在多层感知器的基础上引入”非线性”激活函数，使得 ANN 可以模拟函数逼近器的作用，用于学习数据特征</li><li>缺点：1）随着图像大小的增加，可训练参数增多；2）会丢失图像的空间特征；3）处理序列数据无法记忆历史信息</li><li>Pytorch 可以堆叠多层的”FC+relu”层实现<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=torch.randn([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">fc = torch.nn.Linear(in_features = <span class="number">2</span>, out_features = <span class="number">3</span>)</span><br><span class="line">y=fc(x) // y=[<span class="number">1</span>,<span class="number">3</span>]</span><br><span class="line">y=torch.nn.functional.relu(y)</span><br></pre></td></tr></table></figure></li></ul><p>参考：</p><ol><li><a href="https://luweikxy.gitbook.io/machine-learning-notes/artificial-neural-network#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%9C%E7%94%A8">ANN人工神经网络 - machine-learning-notes</a></li><li><a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multilayer perceptron - Wikipedia</a></li><li><a href="https://www.cnblogs.com/lgdblog/p/6858832.html">线性可分 与线性不可分 - Amazing_Man - 博客园</a></li><li><a href="https://www.zhihu.com/question/593941226">transformer为什么有利于并行计算？ - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文用于解释最基础的神经网络-感知机的原理，它是后续 CNN、RNN 的基础，CNN 只是在感知机的概念加入“局部连接”的思想，每个局部执行的还是加权和；RNN 只是给每个隐藏层加一个隐状态&lt;/p&gt;
&lt;p&gt;本文按照：感知机-&amp;gt;多层感知机-&amp;gt;全连接层-&amp;gt;人工神经网络的步骤去理解 Linear 层&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="ANN" scheme="https://shaogui.life/tags/ANN/"/>
    
    <category term="Linear" scheme="https://shaogui.life/tags/Linear/"/>
    
  </entry>
  
  <entry>
    <title>SAST：A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning</title>
    <link href="https://shaogui.life/2023/04/10/SAST%EF%BC%9AA%20Single-Shot%20Arbitrarily-Shaped%20Text%20Detector%20based%20on%20Context%20Attended%20Multi-Task%20Learning/"/>
    <id>https://shaogui.life/2023/04/10/SAST%EF%BC%9AA%20Single-Shot%20Arbitrarily-Shaped%20Text%20Detector%20based%20on%20Context%20Attended%20Multi-Task%20Learning/</id>
    <published>2023-04-10T15:20:53.000Z</published>
    <updated>2023-04-10T15:30:27.798Z</updated>
    
    <content type="html"><![CDATA[<p>属于EAST的演进版本，还是类似 anchor-free 的方式预测文本行，除了输出 grid 的 score +边框预测外，还输出更多的文本实例信息，比如 grid 到实例矩形四角、中心点、四边的距离，使得SAST可以检测弯曲文本行、中间有间隔的文本行</p><a id="more"></a><h1 id="什么是-SAST-？"><a href="#什么是-SAST-？" class="headerlink" title="什么是 SAST ？"></a>什么是 SAST ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204412.png" alt=""></li><li>属于 <a href="EAST.md">EAST</a> 的演进版本，还是类似 anchor-free 的方式预测文本行，但是除了输出 grid 的 score +边框预测外，还输出更多的文本实例信息，比如 grid 到实例矩形四角、中心点、四边的距离</li><li>每个 grid 更加复杂的输出，可以让 SAST 检测更为复杂场景下的文本行，比如弯曲文本行、中间有间隔的文本行</li></ul><h1 id="SAST-的网络结构？"><a href="#SAST-的网络结构？" class="headerlink" title="SAST 的网络结构？"></a>SAST 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204443.png" alt=""></li><li><strong>Featrue Extractor</strong>：BackBone 部分，通过类似 SegNet 的过程提取特征</li><li><strong>CABs</strong>：交叉注意力模块，用于整合 BackBone 的特征</li><li><strong>TCL map (1 xHxW)</strong>: grid 属于文本中心线像素点的概率</li><li><strong>TCO map (2 xHxW)</strong>: 文本中心点偏置，grid 距其所属的文本实例矩形框中心的 xy 方向距离</li><li><strong>TVO map (8 xHxW)</strong>: 文本四顶点偏置，grid 距其所属的文本实例矩形框四顶点的 xy 方向距离</li><li><strong>TBO map (4 xHxW)</strong>: 文本边界偏置，grid 距其所属的文本实例上下边界框的 xy 方向距离</li></ul><h1 id="SAST-的-CAB-模块？"><a href="#SAST-的-CAB-模块？" class="headerlink" title="SAST 的 CAB 模块？"></a>SAST 的 CAB 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204535.png" alt=""></li><li>交叉注意力模块，用于整合 BackBone 的特征，该模块分为上下两部分，上部分构建水平方向注意力，下部分构建垂直方向注意力，整合水平方向注意力和垂直方向注意力得到<strong>全局注意力</strong></li></ul><h1 id="SAST-样本制作？"><a href="#SAST-样本制作？" class="headerlink" title="SAST 样本制作？"></a>SAST 样本制作？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204602.png" alt=""> </li><li><strong>a). TCL map (1 xHxW)</strong>：文本中心线区域，文本行上下边界收缩 20%后得到的区域，而左右边界仍保持不变</li><li><strong>b). TBO map (4 xHxW)</strong>：文本边界偏置，首先计算斜率 k 1 (v 1, v 2)与斜率 k 1 (v 4, v 3)的平均值，对于一个给定的点 P 0，可容易地计算出斜率为 (k 1+k 2)/2、过点 P 0 的直线，由此该直线与线段 (v 1, v 4)和线段 (v 2, v 3)的交点 P 1 与 P 2 很容易得出，故 P 0 的上下边界点 $P<em>{upper}$ 和 $P</em>{lower}$ 的坐标可由线段比例关系得到，整理得到 P 0 点到四边距离的 TBO 为{$P<em>0^x-P_1^x$、 $P_0^x-P</em>{lower}^x$ 、$P<em>2^y-P_0^y$、$P</em>{upper}^y-P_0^y$}</li><li><strong>c). TVO map (8 xHxW)</strong>：文本顶点偏置，文本最小矩形框按根据一定规则由文本标注信息计算得到，计算文本中心区域中某像素点到文本矩形框四顶点的直线距离（包括 x 方向和 y 方向），所以共计给每个 grid 生成 8 个 TVO 预测</li><li><strong>d). TCO map (2 xHxW)</strong>：文本中心点偏置，计算文本中心区域内某像素点到文本最小矩形框中心点的距离 (x 方向和 y 方向)</li></ul><h1 id="SAST-的损失函数？"><a href="#SAST-的损失函数？" class="headerlink" title="SAST 的损失函数？"></a>SAST 的损失函数？</h1><ul><li><script type="math/tex; mode=display">L_{total}=\lambda_1L_{tcl}+\lambda_2L_{tco}+\lambda_3L_{tvo}+\lambda_4L_{tbo},</script></li><li><strong>(1) TCL map:</strong> 使用 Minimizing the Dice loss 作为分割 loss, 用于描述两个轮廓的相似程度</li><li><strong>(2) TVO/TCO/TBO:</strong> 使用 Smooth L 1 Loss 作为几何图 geometry map 的回归 loss</li></ul><h1 id="解析-SAST-的输出-1-生成文本实例？"><a href="#解析-SAST-的输出-1-生成文本实例？" class="headerlink" title="解析 SAST 的输出 1-生成文本实例？"></a>解析 SAST 的输出 1-生成文本实例？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204913.png" alt=""></li><li><strong>a)</strong> <strong>根据 TCL 获得文本实例包含的像素点-&gt;文本行 Mask</strong>，阈值过滤将置信率低于某值的假阳性像素点剔除，得到合适的 TCL map;</li><li><strong>b) 根据 TVO+NMS 获得文本实例-&gt;文本矩形框</strong>：将经过处理的 TCL map 中每个像素点，根据 TVO 文本实例顶点偏置图，得到对应的文本矩形框四顶点坐标，并进行非最大值抑制 NMS，得到所需的文本实例矩形框及其中心点</li><li><strong>b+c=d) 根据 TCO 合并文本实例-&gt;文本行 Mask</strong> ：计算 TCL 中属于文本的像素点的所属文本实例的几何中心点，该中心点将作为低层级像素信息，当步骤 c 计算所得的几何中心点与步骤 b 所得矩形框中心点重合或相近时，该像素点将被归类给步骤 b 中矩形框对应的文本实例，通过此步骤重新合并断开的文本行</li></ul><h1 id="解析-SAST-的输出-2-生成文本边框？"><a href="#解析-SAST-的输出-2-生成文本边框？" class="headerlink" title="解析 SAST 的输出 2-生成文本边框？"></a>解析 SAST 的输出 2-生成文本边框？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204412.png" alt=""></li><li><strong>a)</strong> 前面解析得到的文本实例</li><li><strong>b)</strong> 对文本中心线采样，采样点的间距相同，则得到的采样点数目与文本线的长度有关，故称之为自适应采样</li><li><strong>c)</strong> 根据文本边界偏置图 TBO 所提供的信息，计算文本中心线的采样点上的上下边界定位点</li><li><strong>d)</strong> 将步骤 b 所得的边界定位点按照从左上角开始的顺时针方向依次进行连接，得到最终的文本边界框</li></ul><p>参考：</p><ol><li><a href="https://blog.csdn.net/qq_37546096/article/details/102909850">SAST : Single-Shot Arbitrarily-Shaped Text Detector论文阅读笔记_text center line sampling_litchi9854的博客-CSDN博客</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;属于EAST的演进版本，还是类似 anchor-free 的方式预测文本行，除了输出 grid 的 score +边框预测外，还输出更多的文本实例信息，比如 grid 到实例矩形四角、中心点、四边的距离，使得SAST可以检测弯曲文本行、中间有间隔的文本行&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文本检测" scheme="https://shaogui.life/tags/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>EAST：An Efficient and Accurate Scene Text Detector</title>
    <link href="https://shaogui.life/2023/04/10/EAST%EF%BC%9AAn%20Efficient%20and%20Accurate%20Scene%20Text%20Detector/"/>
    <id>https://shaogui.life/2023/04/10/EAST%EF%BC%9AAn%20Efficient%20and%20Accurate%20Scene%20Text%20Detector/</id>
    <published>2023-04-10T15:16:06.000Z</published>
    <updated>2023-04-10T15:24:03.572Z</updated>
    
    <content type="html"><![CDATA[<p>EAST基于FCN输出，对每个grid进行文本行预测，可实现旋转矩形框、任意四边形框的预测</p><a id="more"></a><h1 id="什么是-EAST-？"><a href="#什么是-EAST-？" class="headerlink" title="什么是 EAST ？"></a>什么是 EAST ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/EAST-20230408144126.png" alt="EAST-20230408144126"></li><li>（a）、（b）、（c）、（d）都是几种常见 stat-of-the-art 的文本检测过程，算法思想遵循之前 two-stage 的方法，一般都需要先提出候选框，过滤后对剩下的候选框要进行回归操作得出更精细的边框信息，然后再合并候选框等</li><li><strong>EAST</strong>基于 FCN 输出特征，类似 anchor-free 的目标检测模型，预测每个 grid 的代表的文本行信息，然后使用 NMS（非极大值抑制）合并预测后的信息，可实现矩形、选择矩阵和四边形的文本检测，不能实现弯曲文本的检测</li></ul><h1 id="EAST-的网络结构？"><a href="#EAST-的网络结构？" class="headerlink" title="EAST 的网络结构？"></a>EAST 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/EAST-20230408144126-1.png" alt="EAST-20230408144126-1"></li><li><strong>FCN 特征提取</strong>：通过<strong>特征提取</strong>和<strong>特征融合</strong>两个步骤，最后取 C 2 特征输入预测头</li><li><strong>预测结果的输出层</strong>：假设 C 2 特征大小为 CHW，对于 HW 的每个 grid 输出 2 个分支，第一个分支是置信度 (1)，第二个分支是框位置，框位置如果是旋转矩形，则输出 4 (xyxy)+1（angle），如果是任意的四边形, 则输出 8 (xy * 4)</li></ul><h1 id="EAST-的标签分配？"><a href="#EAST-的标签分配？" class="headerlink" title="EAST 的标签分配？"></a>EAST 的标签分配？</h1><ul><li>检测 head 没有设置 anchor，直接按映射位置确定正样本，文本行比较大，可以按照多正样本匹配</li></ul><h1 id="EAST-的标签生成？"><a href="#EAST-的标签生成？" class="headerlink" title="EAST 的标签生成？"></a>EAST 的标签生成？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/EAST-20230408144127.png" alt="EAST-20230408144127"></li><li><strong>垂直或水平矩阵框 (AABB)</strong>：只需要 4 个值就可描述；<strong>旋转矩形框 (RBOX)</strong> ： AABB 的基础上增加角度，共 5 个值描述；<strong>任意四边形（QUAD）</strong>：需要 4 个点 8 个值去描述</li></ul><h1 id="EAST-的损失函数？"><a href="#EAST-的损失函数？" class="headerlink" title="EAST 的损失函数？"></a>EAST 的损失函数？</h1><ul><li><script type="math/tex; mode=display">L=L_\mathrm{s}+\lambda_\mathfrak{g}L_\mathfrak{g}</script></li><li><strong>分割损失 $L_s$</strong>：使用 blance 的交叉熵</li><li><strong>位置损失 $L_g$</strong>：直接使用 L 1 或者 L 2 损失去回归文本区域将导致损失偏差朝更大更长, 所以使用 IOU loss 监督 AABB 或 RBOX 类型框的位置；对于 QUAD 类型的回归框，使用尺度归一化的 smooth L 1 损失</li></ul><h1 id="EAST-如何解析模型输出"><a href="#EAST-如何解析模型输出" class="headerlink" title="EAST 如何解析模型输出"></a>EAST 如何解析模型输出</h1><ul><li>模型输出包括 2 部分，1）score map：检测框的置信度，1 个参数；2）text boxes：对于检测形状为 RBOX，检测框的位置（x, y, w, h）+旋转角度 (angle)，5 个参数；对于检测形状为 QUAD，则输出任意四边形检测框的位置坐标，(x 1, y 1), (x 2, y 2), (x 3, y 3), (x 4, y 4)，8 个参数</li><li>取 topK 的 score map 对应的预测框，然后采用 Locality-Aware NMS 过滤这些预测框，得到最终结果</li></ul><p>参考：</p><ol><li><a href="https://cloud.tencent.com/developer/article/1542875">05. OCR学习路径之文本检测（下）EAST算法简介 - 腾讯云开发者社区-腾讯云</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;EAST基于FCN输出，对每个grid进行文本行预测，可实现旋转矩形框、任意四边形框的预测&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文本检测" scheme="https://shaogui.life/tags/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>FOTS：Fast Oriented Text Spotting with a Unified Network</title>
    <link href="https://shaogui.life/2023/04/09/FOTS%EF%BC%9AFast%20Oriented%20Text%20Spotting%20with%20a%20Unified%20Network/"/>
    <id>https://shaogui.life/2023/04/09/FOTS%EF%BC%9AFast%20Oriented%20Text%20Spotting%20with%20a%20Unified%20Network/</id>
    <published>2023-04-09T13:42:24.000Z</published>
    <updated>2023-04-09T14:28:39.081Z</updated>
    
    <content type="html"><![CDATA[<p>第一个<strong>端到端</strong>解决文本识别的模型，相比较两阶段的文本定位方法，它的检测速度更快，基本思路是通过文本检测分支实现文本行区域的提取，然后通过 RoIRotate 模块实现文本行的“摆正”，最后使用 CRNN+CTC 的模式实现文本行的字符识别</p><a id="more"></a><h1 id="什么是-FOTS-？"><a href="#什么是-FOTS-？" class="headerlink" title="什么是 FOTS ？"></a>什么是 FOTS ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304092146678.png" alt=""></li><li>第一个<strong>端到端</strong>解决文本识别的模型，相比较两阶段的文本定位方法，它的检测速度更快，基本思路是通过文本检测分支实现文本行区域的提取，然后通过 RoIRotate 模块实现文本行的“摆正”，最后使用 CRNN+CTC 的模式实现文本行的字符识别</li><li>RoIRotate 模块要通过仿射变换转换文本区域，所以 FOTS 只能识别文字中心在一个线上的文本行，无法处理弯曲文本行</li></ul><h1 id="FOTS-的网络结构？"><a href="#FOTS-的网络结构？" class="headerlink" title="FOTS 的网络结构？"></a>FOTS 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304092146065.png" alt=""></li><li><strong>shared convolutions</strong>：使用 Resnet 搭建，首先使用下采样，然后使用反卷积上采样，并且使用类似 SegNet 的高分辨率连接到低分辨率的连接</li><li><strong>文本检测分支</strong>：使用类似 anchor-free 的目标检测方式，将 shared convolutions 的每个 grid 视为文本行中心，然后预测其文本行的宽高</li><li><strong>RoIRotate</strong>：根据文本检测分支的输出+shared convolutions 输出，将文本行转为横向文本</li><li><strong>文字识别分支</strong>：基于 CRNN+CTC 的方式学习和识别文本行</li></ul><h1 id="FOTS-的-shared-convolutions-模块？"><a href="#FOTS-的-shared-convolutions-模块？" class="headerlink" title="FOTS 的 shared convolutions 模块？"></a>FOTS 的 shared convolutions 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/FOTS-20230409194643.png" alt=""></li><li>首先通过 ResNet 提取特征，然后通过反卷积上采样，类似 SegNet 一样中间使用残差连接，最后输出 C 2 特征</li></ul><h1 id="FOTS-的文本检测分支？"><a href="#FOTS-的文本检测分支？" class="headerlink" title="FOTS 的文本检测分支？"></a>FOTS 的文本检测分支？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-09%2020.21.26.excalidraw.svg" alt="Drawing 2023-04-09 20.21.26.excalidraw"></li><li>使用类似 anchor-free 的目标检测方式，将 shared convolutions 的每个 grid 视为文本行中心，然后预测其文本行的宽高</li><li>假设 shared convolutions 输出是 $C\times H \times W$ 的特征，文本检测分支输出 3 个分支，分别表示文本行的得分、该 grid 到四边的距离和该文本行的旋转角度</li></ul><h1 id="FOTS-的-RoIRotate-模块？"><a href="#FOTS-的-RoIRotate-模块？" class="headerlink" title="FOTS 的 RoIRotate 模块？"></a>FOTS 的 RoIRotate 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/FOTS-20230409194714.png" alt=""></li><li>使用目标检测的后处理获得文本行，根据文本行的宽高及旋转角得到四个角点的位置，假设四个点是 ($x_1$, $y_1$)、($x_2$, $y_2$)、($x_3$, $y_3$)、($x_4$, $y_4$)，现在要将这个区域转到 (0,0)起点，宽高 (wh)的区域，可以通过仿射变换实现</li><li>仿射变换矩阵需要变换前后的 3 对点求得，不妨取 ($x_1$, $y_1$)-&gt;(0,0)、($x_2$, $y_2$)-&gt;（w, 0）、($x_3$, $y_3$)-&gt;(w, h)，求取方法是调用 opencv 的 getAffineTransform 函数即可，仿射矩阵变换后，文本的中心线平行 x 轴</li></ul><h1 id="FOTS-的文字识别分支？"><a href="#FOTS-的文字识别分支？" class="headerlink" title="FOTS 的文字识别分支？"></a>FOTS 的文字识别分支？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/CRNN-20230408144101.png" alt=""></li><li>文字识别是在 RoIRotate 模块输出的基础上进行的，就是得到平行文本行的基础上进行的，其过程有 4 个</li><li><strong>CNN 提取特征</strong>：使用轻量化网络 MobileNetv 3，其中输入图像的高度统一设置为 32，宽度可以为任意长度，经过 CNN 网络后，特征图的高度缩放为 1</li><li><strong>双向 LSTM（BiLSTM）对特征序列进行预测</strong>：学习序列中的每个特征向量并输出预测标签分布。这里其实相当于把特征向量的宽度视为 LSTM 中的时间维度</li><li><strong>全连接层分类</strong>：使用全连接层对每个序列进行 N+1 类别预测，获取模型的预测结果</li><li><strong>CTC</strong>：解码模型输出的预测结果，得到最终输出</li></ul><h1 id="FOTS-的损失函数？"><a href="#FOTS-的损失函数？" class="headerlink" title="FOTS 的损失函数？"></a>FOTS 的损失函数？</h1><ul><li>网络的损失分为两部分，即文本行识别损失 $L<em>{detect}$ 、文本行字符识别损失 $L</em>{recog}$，通过参数 $\lambda_{recog}$ 控制两者的权重<script type="math/tex; mode=display">L=L_{\mathbf{detect}}+\lambda_{\mathbf{recog}}L_{\mathbf{recog}}</script></li></ul><p>参考：</p><ol><li><a href="https://blog.csdn.net/sol_data12/article/details/113501530">场边文字检测——FOTS模型详解及其代码实现_ManManMan池的博客-CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/195248125">[论文笔记] FOTS - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;第一个&lt;strong&gt;端到端&lt;/strong&gt;解决文本识别的模型，相比较两阶段的文本定位方法，它的检测速度更快，基本思路是通过文本检测分支实现文本行区域的提取，然后通过 RoIRotate 模块实现文本行的“摆正”，最后使用 CRNN+CTC 的模式实现文本行的字符识别&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文本定位" scheme="https://shaogui.life/tags/%E6%96%87%E6%9C%AC%E5%AE%9A%E4%BD%8D/"/>
    
  </entry>
  
  <entry>
    <title>文本检测之DB和DB++</title>
    <link href="https://shaogui.life/2023/04/09/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E4%B9%8BDB%E5%92%8CDB++/"/>
    <id>https://shaogui.life/2023/04/09/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E4%B9%8BDB%E5%92%8CDB++/</id>
    <published>2023-04-09T07:03:50.000Z</published>
    <updated>2023-04-09T14:28:31.983Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍两个文本检测模型： DB 及它的升级版本 DB++，主要的原理就是将“二值化”的过程做成网络可学习、可微分的一个模块，然后向网络中插入这个模块自适应去学习二值化阈值，最终预测文字区域的一个核心部分，然后再通过公式放大这个核心部分，得到目标区域</p><a id="more"></a><h1 id="什么是-DB-？"><a href="#什么是-DB-？" class="headerlink" title="什么是 DB ？"></a>什么是 DB ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091513112.png" alt=""></li><li><strong>传统意义二值化</strong>：基于分割的文本检测算法其流程如图2中的蓝色箭头所示。在传统方法中得到分割结果之后采用一个<strong>固定阈值</strong>得到二值化的分割图</li><li><strong>DB 二值化</strong>：如图2中红色箭头所示的，通过网络去预测图片每个位置处的阈值，而不是采用一个固定的值，这样就可以很好将背景与前景分离出来，但是这样的操作会给训练带来梯度不可微的情况，对此对于二值化提出了一个叫做 <strong>Differentiable Binarization 模块</strong>来解决</li></ul><h1 id="DB-的网络结构？"><a href="#DB-的网络结构？" class="headerlink" title="DB 的网络结构？"></a>DB 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091633772.png" alt=""></li><li><strong>BackBone</strong>：可以使用类似 resnet 下采样 5 次，得到 5 个层次的特征</li><li><strong>FPN 类似结构</strong>：对 C4、C3、C2 特征采样类似 FPN 的连接，输出时是 C5、 F4、F3、F2 一共 4 个层次的特征</li><li><strong>DB 模块</strong>：以 probability map 减去 threshold map (T) 差值，输入到 DB 模块进行可微的二值化学习</li></ul><h1 id="DB-的“可微的二值化模块-DB-”？"><a href="#DB-的“可微的二值化模块-DB-”？" class="headerlink" title="DB 的“可微的二值化模块 (DB)”？"></a>DB 的“可微的二值化模块 (DB)”？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091634515.png" alt=""></li><li>上图 a、b、c 分别是标准二值化与可微二值化输出、可微二值化对正样本的梯度，可微二值化对负样本的梯度，k 是放大倍数</li><li><strong>标准二值化 (SB)</strong>：通过预先设置的阈值 t 去对概率图 $P_{i,j}$ 二值化<script type="math/tex; mode=display">B_{i,j}=\begin{cases}1\quad P_{i,j}>=t\\ 0\quad otherwise\end{cases}</script></li><li><strong>可微二值化 (DB)</strong>：借鉴 sigmoid 输出输出，将 $P<em>{i,j}-T{i,j}$ 作为 sigmoid 输入，并 K 扩大输出，使得 $\hat{B}</em>{i,j}$ 趋向 0 或 1，即通过学习每个位置的阈值 $T<em>{i, j}$ 对概率图 $P</em>{i, j}$ 二值化 <script type="math/tex; mode=display">\hat{B}_{i, j}=\dfrac{1}{1+\exp^{-k (P_{i, j}-T_{i, j})}}</script></li><li><strong>正负样本梯度比较</strong>：DB 改进性能的原因可以通过梯度的反向传播来解释，可知正负样本的梯度被 k 放大 <script type="math/tex; mode=display">\begin{aligned}l_{+}=-log\frac{1}{1+e^{-}k x}\quad =>  \frac{\partial l_{+}}{\partial x}=-k f(x)e^{-k x}\\\\ l_{-}=-log(1-\frac{1}{1+e^{-}k x})\quad\quad =>  \frac{\partial l_{-}}{\partial x}=k f(x)\end{aligned}</script></li></ul><h1 id="DB-的自适应阈值？"><a href="#DB-的自适应阈值？" class="headerlink" title="DB 的自适应阈值？"></a>DB 的自适应阈值？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091635325.png" alt=""></li><li>a、b、c、d 分别是原图、probability map、无监督的 threshold map、有监督的 threshold map</li><li>c 图表明即使没有对 threshold map 监督，其结果也会表现出突出显示文本边界区域。这表明如果加入类似边界的监督，以提供更好的指导，d 图的结果证明了这一点</li></ul><h1 id="DB-的标签生成过程？"><a href="#DB-的标签生成过程？" class="headerlink" title="DB 的标签生成过程？"></a>DB 的标签生成过程？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091635472.png" alt=""></li><li><strong>probability map</strong>：使用 Vatti clipping algorithm 将 G 缩减到 Gs（蓝线内部），A 是面积，r 是 shrink ratio，设置为0.4，L 是周长 <script type="math/tex">D=\dfrac{A(1-r^2)}{L}</script></li><li><strong>threshold map</strong>：使用生成 probability map 一样的方法，向外进行扩张，得到绿线和蓝线中间的区域，根据到红线的距离制作标签</li><li><strong>binary map</strong>：蓝色标注线以内</li></ul><p>总结：以上 3 个标签的值范围</p><div class="table-container"><table><thead><tr><th>-</th><th>蓝线以内</th><th>蓝蓝绿之间</th><th>其他</th></tr></thead><tbody><tr><td>probability map</td><td>1</td><td>0</td><td>0</td></tr><tr><td>threshold map</td><td>0.3</td><td>越靠近红线 0.7，越远离红线 0.</td><td>0.3</td></tr><tr><td>binary map</td><td>1</td><td>0</td><td>0</td></tr></tbody></table></div><p><strong><mark style="background: #FF5582A6;">从上面标签制作可知，DB 没有直接去学习文本的边缘（图红线），而是去学习比文本边缘更小的区域 (图绿线)，我觉得这点是除了”可微二值化模块”外，尤其需要关注的地方。这里说一下自己的理解</mark></strong></p><h1 id="DB-为什么不直接学习文本外轮廓，而是学习轮廓缩小的轮廓？"><a href="#DB-为什么不直接学习文本外轮廓，而是学习轮廓缩小的轮廓？" class="headerlink" title="DB 为什么不直接学习文本外轮廓，而是学习轮廓缩小的轮廓？"></a>DB 为什么不直接学习文本外轮廓，而是学习轮廓缩小的轮廓？</h1><ul><li><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091649041.png" alt="Drawing 2023-04-09 15.43.02.excalidraw"></p></li><li><p>上图是两种学习路线下的 probability map、threshold map 及他们学习的 binary map，其中红线是直接学习文本边缘（下文称直觉模式），绿线学习文本边缘小一圈的轮廓（下文称 DB 模式）</p></li><li><strong>观察 probability map</strong>，“直觉模式”比 DB 模式范围更大，这对极度弯曲的小文本是不友好的，可以想象文本在 C2特征已经辨别不出弯曲，更小的学习区域可以有更强的能力</li><li><strong>观察 threshold map</strong>，因为文本行占据了图片大部分区域，所以“直觉模式”主要优化背景到 0.7， threshold map 计算 L1 损失，相比较 DB 模式大部分优化背景到 0.3，“直觉模式”更难优化</li><li><strong>观察 binary map</strong>：除了和优化 threshold map 同样的问题外，由于“直觉模式”对文本行内、外的梯度大小一样，说明两个区域优化权重一样。而 DB 模式内部梯度比外部梯度更大，相当于增大正样本的梯度权重</li><li><strong>总结</strong>：“直觉模式”比 DB 模式更难优化，而且 DB 模式对弯曲小文本性能更好</li></ul><h1 id="DB-的损失函数？"><a href="#DB-的损失函数？" class="headerlink" title="DB 的损失函数？"></a>DB 的损失函数？</h1><ul><li>$L_s$ 是 probability map 的 loss，$L_b$ 是 binary map 的 loss，$L_t$ 是 threshold map 的 loss，$\alpha$ 和 $\beta$ 设置为1和10，$L_s$ 和 $L_b$ 使用交差熵计算损失<script type="math/tex; mode=display">L=L_s+\alpha\times L_b+\beta\times L_t</script></li><li>$S_l$ 表示使用 OHEM 进行采样，正负样本的比例为1：3, $L_t$ 使用 L 1 loss，$R_d$ 表示绿线内的区域，<script type="math/tex; mode=display">L_t=\sum_{i\in R_d}|y_i^*-x_i^*|</script></li></ul><h1 id="DB-如何解析输出的？"><a href="#DB-如何解析输出的？" class="headerlink" title="DB 如何解析输出的？"></a>DB 如何解析输出的？</h1><ul><li>在推理阶段，可以使用 binary map 或者 probability map</li><li><strong>使用 binary map</strong>：需要 probability map+threshold map 两个分支计算得到，其结果就是文本实例</li><li><strong>使用 probability map</strong>：不需要 threshold map、binary map 分支，直接按照 Vatti clipping algorithm 公式还原回去即可，即1)使用0.3的阈值进行二值化；2)将 pixel 连接成不同的文本实例；3)将文本实例进行扩张，得到最终的文本框<script type="math/tex; mode=display"> D^{'}=\dfrac{A^{'}(1-r^{'})}{L^{'}}</script></li><li>使用第二种方法，网络计算更少，论文使用第二种方法</li></ul><h1 id="DB-的网络结构？-1"><a href="#DB-的网络结构？-1" class="headerlink" title="DB++ 的网络结构？"></a>DB++ 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091649944.png" alt=""></li><li><strong>BackBone</strong>：可以使用类似 resnet 下采样 5 次，得到 5 个层次的特征</li><li><strong>ASF 模块</strong>：ASF 特征融合模块其实就是 FPN，只不过在此基础上增加Spatial Attention</li><li><strong>DB 模块</strong>：和 DB 一样，以 probability map 减去 threshold map (T) 差值，输入到 DB 模块进行可微的二值化学习</li></ul><h1 id="DB-的自适应多尺度特征融合模块-ASF-？"><a href="#DB-的自适应多尺度特征融合模块-ASF-？" class="headerlink" title="DB++的自适应多尺度特征融合模块 ASF ？"></a>DB++的自适应多尺度特征融合模块 ASF ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091650873.png" alt=""></li><li><strong>输入输出</strong>：输入是 BackBone 4 个层次的特征，输出是经过加权的特征</li><li><strong>Spatial Attention</strong>：对特征加空间注意力，使用空间（沿通道方向）进行池化，得到注意力矩阵 $1\times H\times W$</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文介绍两个文本检测模型： DB 及它的升级版本 DB++，主要的原理就是将“二值化”的过程做成网络可学习、可微分的一个模块，然后向网络中插入这个模块自适应去学习二值化阈值，最终预测文字区域的一个核心部分，然后再通过公式放大这个核心部分，得到目标区域&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文本检测" scheme="https://shaogui.life/tags/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>CNN之结构重参数化</title>
    <link href="https://shaogui.life/2023/04/08/CNN%E4%B9%8B%E7%BB%93%E6%9E%84%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96/"/>
    <id>https://shaogui.life/2023/04/08/CNN%E4%B9%8B%E7%BB%93%E6%9E%84%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96/</id>
    <published>2023-04-08T04:40:44.000Z</published>
    <updated>2023-04-09T14:28:22.782Z</updated>
    
    <content type="html"><![CDATA[<p>我们知道模型要变好，就必须构建得更加复杂，但是这来这带来一个坏处，就是模型部署的耗时会增长，这两者是相互矛盾的，<strong>结构从参数化</strong>就是两者都可以做到，在训练的时候，通过复杂的神经网络去训练，提升模型的性能，但是在推理的时候，我通过对模型结构的重参数化生成了一个更加精简的结构，使推理的时候速度更快</p><a id="more"></a><h1 id="什么是结构重参数化？"><a href="#什么是结构重参数化？" class="headerlink" title="什么是结构重参数化？"></a>什么是结构重参数化？</h1><ul><li><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005953.png" alt=""></p></li><li><p><strong>RepVGG 的结构重参数化过程</strong>：上图是左边是训练时的卷积网络，右边通过对结构进行重参数化，得到一个只有 1 个分支的结构，因此可以做到训练时提升性能，推理时提升速度</p></li><li><p><strong>结构从参数化的基本原理</strong>：<strong>卷积的可加性</strong>，对于同一个输入，只要其扫描频率一致（相同的通道数、kernel size、stride、padding），其卷积可过程可以融合。如下公式 1 是一个实数乘特征图和乘卷积是等效的，公式 2卷积核 F1 与 F2 可以被融合为 1 个卷积，卷积核为 (F1+F2) </p></li><li><script type="math/tex; mode=display">\begin{matrix}I\otimes(pF)=p(I\otimes F) \quad&(1)\\I\otimes F^{(1)}+I\otimes F^{(2)}=I\otimes(F^{(1)}+F^{(1)})\quad&(2)\end{matrix}</script></li></ul><h1 id="ACNet-的网络结构？"><a href="#ACNet-的网络结构？" class="headerlink" title="ACNet 的网络结构？"></a>ACNet 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005292.png" alt=""></li><li>上图左边展示 3 个分支的卷积融合为一个等效卷积的过程；右边是卷积融合的过程，主要包括融合 BN (BN fusion) 和融合分支 (branch fusion)两个步骤</li><li><strong>融合 BN (BN fusion)</strong> ：所有 BN 对输入操作一样，不改变输入分辨率，所以利用卷积的线性可加性，将 BN 的过程融合进卷积</li><li><strong>融合分支 (branch fusion)</strong>：同样利用卷积的线性可加性，将多分支的卷积融合为 1个卷积</li></ul><h1 id="RepVGG-如何进行“结构重参数化”？"><a href="#RepVGG-如何进行“结构重参数化”？" class="headerlink" title="RepVGG 如何进行“结构重参数化”？"></a>RepVGG 如何进行“结构重参数化”？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005549.png" alt=""></li><li>1.<strong>卷积层参数</strong>：上图是 REP-VGG 块的结构重参数化过程。为了易于可视化，我们假设 C2 = C1 = 2，因此3×3层具有四个3×3矩阵，而1×1层的核为2×2矩阵</li><li>2.<strong>BN 层参数</strong>：(1)可知 BN 层为每个通道的数据进行规范化，每个通道需要 4 个参数 $\:\mu,\:\sigma,\:\gamma,\:\beta$，输入通道 2 个则有 8 个参数；(2) 当 $\:\mu,\:\sigma,\:\gamma,\:\beta$ 均为 0 时，规范化后的数据还是原来的值，这可以用于模拟 identity 路径</li><li>3.<strong>融合卷积与 BN 层</strong>：(1)最难理解的是虚线红框部分，由原来的 $2\times 1 \times 1\times 2$ 变为 $2\times 3 \times 3\times 2$ ，也就是单个卷积核由 $1\times 1$ 变为 $3\times 3$，这是通过在 $1\times 1$ 四周补 0 做到的，因为补 0 后得到的卷积和是不变的；(2) 类似 [[ACNet#^udwpgu|ACNet的网络结构]]的过程，$\:\mu,\:\sigma,\:\gamma,\:\beta$ 的部分参数用于重构卷积核的值，部分参数组合成卷积的偏置值 $b$，并且每个通道 1 个值 <img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005037.png" alt=""></li><li>4.<strong>利用卷积的可加性，融合多路径</strong>：对应同 size 卷积核的，可以利用卷积的可加性，将卷积融合，具体来说是卷积核矩阵对应相加，偏置值对应相加</li></ul><h1 id="DBB-的网络结构？"><a href="#DBB-的网络结构？" class="headerlink" title="DBB 的网络结构？"></a>DBB 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005802.png" alt=""></li><li>参考 [[GoogleNetv1]] 的 Inception block 的概念，结合结构重参数划的理论，设计了 DBB block</li><li>每个 DBB block 包含 4 个并行的路径，推理时融合成 1 个路径</li></ul><h1 id="DBB-的-6-种模块可以等价转为单个卷积？"><a href="#DBB-的-6-种模块可以等价转为单个卷积？" class="headerlink" title="DBB 的 6 种模块可以等价转为单个卷积？"></a>DBB 的 6 种模块可以等价转为单个卷积？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082006464.png" alt=""></li><li><ol><li>Conv-BN 合并：经典的卷积层融合 BN 层的结构</li></ol></li><li><ol><li>并行合并：参考ACNet的网络结构，卷积核可线性相加</li></ol></li><li><ol><li>串行合并：参考ACNet的网络结构，卷积核可线性相加</li></ol></li><li><ol><li>并行拼接：参考ACNet的网络结构，卷积核 kernel size 保持不变，数量是两个分支相加</li></ol></li><li><ol><li>平均池化转换：平均池化很像卷积核的过程，只不过是求和后加平均而已，直接对卷积核的值除 KxK，后面得到的卷积和就是 AVG 后的值了</li></ol></li><li><ol><li>多尺度卷积合并：参考ACNet的网络结构，同一将卷积核扩充为 KxK，再进行融合</li></ol></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;我们知道模型要变好，就必须构建得更加复杂，但是这来这带来一个坏处，就是模型部署的耗时会增长，这两者是相互矛盾的，&lt;strong&gt;结构从参数化&lt;/strong&gt;就是两者都可以做到，在训练的时候，通过复杂的神经网络去训练，提升模型的性能，但是在推理的时候，我通过对模型结构的重参数化生成了一个更加精简的结构，使推理的时候速度更快&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="CNN" scheme="https://shaogui.life/tags/CNN/"/>
    
    <category term="结构重参数化" scheme="https://shaogui.life/tags/%E7%BB%93%E6%9E%84%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Hexo博客速度优化</title>
    <link href="https://shaogui.life/2022/06/15/Hexo%E5%8D%9A%E5%AE%A2%E9%80%9F%E5%BA%A6%E4%BC%98%E5%8C%96/"/>
    <id>https://shaogui.life/2022/06/15/Hexo%E5%8D%9A%E5%AE%A2%E9%80%9F%E5%BA%A6%E4%BC%98%E5%8C%96/</id>
    <published>2022-06-15T13:05:24.609Z</published>
    <updated>2022-06-15T01:53:14.900Z</updated>
    
    <content type="html"><![CDATA[<p>本文对Hexo博客进行访问优化，使得访问速度更快了，主要是安装hexo-neat插件，实现对html、css、js、image等静态资源的高效压缩。通过压缩这些静态资源，可以减少请求的数据量从而达到优化博客访问速度的目的</p><a id="more"></a><h2 id="资源压缩1"><a href="#资源压缩1" class="headerlink" title="资源压缩1"></a>资源压缩<sup><a href="#fn_1" id="reffn_1">1</a></sup></h2><p><strong>安装插件</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-neat --save</span><br></pre></td></tr></table></figure></p><p><strong>配置插件</strong><br>打开博客根目录文件<code>_config.yml</code>，添加以下配置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hexo-neat 压缩</span></span><br><span class="line">neat_enable: <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 压缩html</span></span><br><span class="line">neat_html:</span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">  exclude:</span><br><span class="line"><span class="comment"># 压缩css  </span></span><br><span class="line">neat_css:</span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">  exclude:</span><br><span class="line">    - <span class="string">&#x27;**/*.min.css&#x27;</span></span><br><span class="line"><span class="comment"># 压缩js</span></span><br><span class="line">neat_js:</span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">  mangle: <span class="literal">true</span></span><br><span class="line">  output:</span><br><span class="line">  compress:</span><br><span class="line">  exclude:</span><br><span class="line">    - <span class="string">&#x27;**/*.min.js&#x27;</span></span><br><span class="line">    - <span class="string">&#x27;**/jquery.fancybox.pack.js&#x27;</span></span><br><span class="line">    - <span class="string">&#x27;**/index.js&#x27;</span></span><br></pre></td></tr></table></figure></p><h2 id="图片懒加载2"><a href="#图片懒加载2" class="headerlink" title="图片懒加载2"></a>图片懒加载<sup><a href="#fn_2" id="reffn_2">2</a></sup></h2><p>即文字先出来，图片慢慢出来，显著提高加载速度</p><p><strong>安装插件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-lazyload-image --save</span><br></pre></td></tr></table></figure><p><strong>配置文件</strong></p><p>打开配置文件<code>_config.yml</code>，添加以下配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图片懒加载</span></span><br><span class="line">lazyload:</span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span> </span><br><span class="line">  onlypost: <span class="literal">false</span></span><br><span class="line">  loadingImg: /images/loading.gif <span class="comment">#如果不填写图片则使用默认的图片</span></span><br></pre></td></tr></table></figure><blockquote id="fn_2"><sup>2</sup>. <a href="https://www.cnblogs.com/lfri/p/12221963.html">Hexo-Next提高加载速度</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文对Hexo博客进行访问优化，使得访问速度更快了，主要是安装hexo-neat插件，实现对html、css、js、image等静态资源的高效压缩。通过压缩这些静态资源，可以减少请求的数据量从而达到优化博客访问速度的目的&lt;/p&gt;</summary>
    
    
    
    <category term="软件工具" scheme="https://shaogui.life/categories/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="hexo" scheme="https://shaogui.life/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>书摘：《你的知识需要管理》－田志刚</title>
    <link href="https://shaogui.life/2021/04/09/%E4%B9%A6%E6%91%98_%E4%BD%A0%E7%9A%84%E7%9F%A5%E8%AF%86%E9%9C%80%E8%A6%81%E7%AE%A1%E7%90%86_%E7%94%B0%E5%BF%97%E5%88%9A/"/>
    <id>https://shaogui.life/2021/04/09/%E4%B9%A6%E6%91%98_%E4%BD%A0%E7%9A%84%E7%9F%A5%E8%AF%86%E9%9C%80%E8%A6%81%E7%AE%A1%E7%90%86_%E7%94%B0%E5%BF%97%E5%88%9A/</id>
    <published>2021-04-09T14:56:59.000Z</published>
    <updated>2022-06-15T01:53:18.432Z</updated>
    
    <content type="html"><![CDATA[<p>一本介绍如何经营自己知识的书籍，从学习知识、保存知识、知识共享、知识使用和知识创新五个方面进行讲述，有一定的借鉴意义。说实话这本书偏工具类，以前拿小屏看没感觉，限制在大屏看，感觉很多内容都是纯粹列举<code>要点</code>，能读进去的很少（也可能我手机看过了）看到作者举例直接跳过了，所以我大概不到一天就翻完这本书了。我认为好的书它是用严密的<strong>逻辑链条</strong>去慢慢跟你讲道理的，不是突然一个例子，突然一个要点，这本书就会卡住我，阅读情绪经常是这样的：读这一部分是“有点道理”，读下一部分是“好像是这样的”，往后是“不是吧”，所以说这本书并没有打动我。之所以说这本书偏工具，是因为书中有部分内容直接教如何使用工具去实践的。</p><p>当然，对于作者管理知识的五个要点我是同意的，这五个要点是循序渐进的，是闭环流动的，作者倡导以下观点我是赞同的，也是我第一次从这本书获得的<code>知识</code>：</p><ol><li><strong>信息与知识有区别</strong></li><li><strong>终身学习以适应社会发展需要</strong></li><li><strong>显性知识隐性化以创造价值</strong></li><li><strong>共享知识以提升个人竞争力</strong></li><li><strong>持续创新以确保价值的独特性</strong></li></ol><a id="more"></a><hr><p><strong>阅读书摘及笔记：</strong></p><blockquote><p>现在，所谓无知不是指没有知识，而是不会展示自己的知识、不会发挥知识的价值、不会发现新知识、不会学习新知识，也不去创造知识。对于主要靠知识谋生的知识工作者而言，你的知识管理过程是否运转自如，是个人发展和个人竞争力能否持续提升的关键</p><p>人要靠自己，但靠自己不是依靠自己的体力，而是要靠自己的脑力，靠知识</p><p>依靠知识绝对不是依靠文凭，也不是依靠你现在掌握的知识量，不要认为掌握了某些知识就可以一劳永逸地解决你一辈子的问题。个人可以依靠的知识，是指在一定的知识基础上，能够随着社会环境的变化，不断确定自己的专业方向并快速学习知识、分享知识、使用和创新知识并创造价值的过程，这个过程就是对你的知识进行有效管理的过程，也是提升你的知识力的过程。现在每个人都必须要考虑自己如何快速学习知识、学习什么知识、如何保存掌握的知识、如何分享知识给你的合作伙伴、如何使用和创新知识</p><p>对于知识的爆炸，解决的方式是你要明确自己的知识需求。知识虽多但人生有限，如果知识不能被你所用，不能成为“你的知识”，这些知识对你也没有作用</p><p>知识又可以分为显性知识和隐性知识。所谓显性知识是指能够用语言、文字、肢体等方式表达清楚的知识；而隐性知识则是虽然知道如何做，但却很难告诉别人或者写明白、说明白的知识。从掌握知识的角度讲，大量的知识以隐性的成分存在着，而能显性化的部分较少。你虽然知道某个事情是怎么样或者如何做，但如果让你讲出来，你可能发现能够表达的会很少，如果进一步要求你写出来，可能能写的就更少了。古语“书不尽言、言不尽意”就是这个意思，是说你能写的要比能说的少，能说的要比你知道的少，本质上就是显性和隐性知识的问题</p><p>隐性知识和显性知识之间存在着相互转换的过程</p><p>隐性知识显性化应该成为现代人的一项必备能力，如果你不能显性化你的知识，就无法建立你的竞争力。为什么中医中药很难做大，一个很重要的原因是它们主要依靠隐性知识作判断，所以传承、复制的难度较大，因此就很难快速发展</p><p>隐性知识还有一些特点，了解这些特点对于你管理自己的知识很有价值。</p><ul><li>你的隐性知识可能只是对你自己是隐性的，对于其他人、其他的机构可能已经是显性知识，这就需要你在前人基础上进行学习，明白是否已经有类似的显性知识；</li><li>隐性知识需要环境（此时、此地），并非永远是隐性的；</li><li>谁能将隐性的知识最先显性化，谁就是知识创新的开拓者。譬如许多大师的创新，多年后也有不同的人表达，但前者是大师，因为他最早将隐性知识显性化；</li><li>隐性知识显性化能力成为人与人之间能力差别的重要方面。将自己的隐性知识显性化应该成为每个知识工作者应具备的能力之一；</li><li>隐性知识显性化需要需求、环境等外力的作用，外力的拉动加上个人显性化的意愿，可以促进隐性知识显性化的过程；</li><li>社区是促进隐性知识显性化的环境；</li><li>隐性知识显性化的方法：讨论、回答提问、需求的压力、工作分解、流程分析等；</li><li>不能用通俗、简单的语言和文字表述知识，表明对该领域知识掌握得不够深入。</li></ul><p>知识与信息不同，知识除了要靠经验去消化汇集来的信息，还要去验证、思考，甚至在亲身体验过程中，去发现问题、解决问题</p><p>在当今时代下，个人的成长和发展以及个人竞争优势的建立，绝对不是靠信息的数量（虽然缺乏信息和获取信息的能力可以成为一个人发展的劣势）。不要以为你整天在互联网上就掌握了知识，互联网上的信息和知识你能获得我也能获得，互联网上有显性的知识也有信息，显性的知识必须跟你个人的原有知识结合起来，转化成你的隐性知识，加上对环境的判断才能发挥作用。</p><p>学历可以作为你知识水平的一个表现，但其表达的只是在某时、某地的知识存量，是否能持续地更新知识、是否能持续地共享和传播自己的知识、是否能将已有的知识用好用足来获取价值、是否能持续地创新知识引领发展，只有这些才是个人竞争力的源泉</p><p>但我们满足于仅仅不是文盲吗？虽然你学习了众多知识，但单位里领导不重视你、同事不跟你合作，你的知识有用吗？如果你掌握了众多知识，但你的知识不会利用，甚至不能给你换来养家糊口的钱，更不用说成就自己的事业，这样的知识有用吗？如果你总是学习别人的知识，读死书、死读书，在别人后面亦步亦趋，不能进行知识创新，那你的竞争优势在哪里？</p><p>人只能依靠自己，而依靠自己最重要的一点是依靠自己对个人知识过程的管理。只有对整个知识过程的管理才是现代人发展竞争力和竞争优势的源泉</p><p>不学习当然不行，但学习也不一定行。你必须知道要学习什么知识，获取什么知识；同时，学习任何领域的知识必须达到一定的深度，否则你的知识就是常识。而常识怎么可能给你带来个人的竞争优势呢？</p><p>终身学习的理念在20世纪中叶就被明确地提了出来，因为人们发现技术、知识的快速更新不仅影响了生产、流通和消费等领域，而且影响到每个人的日常生活。若要与之适应，人们就必须用新的知识、技能和观念来武装自己。终身教育强调现代人必须不间断地进行学习，更新个人知识，才能保持适当的应变能力，保证个人竞争力</p><p>更进一步说，学习本身不是目的。学习知识是为了我们能够工作、生活得更好，更幸福，能帮助我们度过快乐的一生。学习的目的是为了提高我们个人的竞争力，使我们能够在这么多的知识工作者当中脱颖而出，取得自己的成就</p><p>确定你的学习方向</p><ul><li>第一，你的价值观是什么？</li><li>第二，你的个人目标是什么?</li><li>第三，你的性格是什么？</li></ul><p>现在信息和知识越来越多，所以你在学习的时候需要确定自己的方向。在确定方向后，你就需要在你确定的方向上正向积累，争取成为一个领域的专家</p><p>你的学习方法模型</p><ul><li>第一，掌握该领域的基础知识。</li><li>第二，了解该领域的全貌。</li><li>第三，跟踪并掌握该领域的最新知识。</li><li>第四，在实践中学习和创新。</li></ul><p>人们下载的知识是显性知识的一种表现形式，对于这部分知识，如果没有被你处理过（阅读，知道是什么；思考，知道对你有什么用），根本不可能成为“你的知识”，根本不会对你产生一点点作用，只会让你“淹没在知识中”而无法自拔</p><p>只有那些符合个人发展目标的知识，只有经过自己的阅读、思考后保存的知识才有价值，这样的知识保存才是有价值的保存</p><p>任何人都有区别于他人的优势，你怎么突出自己的优势、特点和能力，怎么让更多的人知道你、了解你、认识你、信任你，方法就是共享你的知识，通过知识影响别人。更深入地说，我们现在的大部分工作都需要与人协作才能完成，通常人们会选择什么样的协作对象呢？比如招聘新员工或者提升一个经理，可想而知一定会是自己了解的人、信任的人！所以每个知识工作者都应该去主动共享和传播自己的知识，愿意将自己的知识显性化，这一方面会促进自己的学习，另一方面也能为树立自己的个人品牌发挥作用。</p><p>共享知识的好处</p><ul><li>故事一：共享带来合作</li><li>故事二：共享协助找到工作</li></ul><p>能共享出来才能真正掌握</p><p>在现实中，很多时候我们以为自己知道，但是当我们用语言说出来或者写出来的时候，你却会发现自己很难说得系统、完整，很难让别人明白。造成这种状况的主要原因是你对该知识点的掌握并没有达到你所认为的那样成熟，这时候你应该再去深入学习、研究这个知识点，经过更多的实践和与人交流，更广泛的阅读、讨论，才能慢慢地成熟起来。</p><p>仅有显性知识是不够的</p><p>个人知识的价值由两个因素决定：</p><ul><li>第一个因素是你的知识的独特性。知识的独特性也分两种，第一种是专，在某个方向上深入，别人都不如我深，所以我有独特性，比如读博士基本上是这个意思。还有一种是博，我既会拍电影又会画画，还会说相声、做木匠，这样的人以综合优势树立自己的独特性</li><li>第二个因素是社会对知识的需求</li></ul><p>让你升值的三个绝招</p><ul><li>第一，向前看三年。</li><li>第二，持续提高你知识的独特性</li><li>第三，通过知识共享树立你的个人品牌。</li></ul></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;一本介绍如何经营自己知识的书籍，从学习知识、保存知识、知识共享、知识使用和知识创新五个方面进行讲述，有一定的借鉴意义。说实话这本书偏工具类，以前拿小屏看没感觉，限制在大屏看，感觉很多内容都是纯粹列举&lt;code&gt;要点&lt;/code&gt;，能读进去的很少（也可能我手机看过了）看到作者举例直接跳过了，所以我大概不到一天就翻完这本书了。我认为好的书它是用严密的&lt;strong&gt;逻辑链条&lt;/strong&gt;去慢慢跟你讲道理的，不是突然一个例子，突然一个要点，这本书就会卡住我，阅读情绪经常是这样的：读这一部分是“有点道理”，读下一部分是“好像是这样的”，往后是“不是吧”，所以说这本书并没有打动我。之所以说这本书偏工具，是因为书中有部分内容直接教如何使用工具去实践的。&lt;/p&gt;
&lt;p&gt;当然，对于作者管理知识的五个要点我是同意的，这五个要点是循序渐进的，是闭环流动的，作者倡导以下观点我是赞同的，也是我第一次从这本书获得的&lt;code&gt;知识&lt;/code&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;信息与知识有区别&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;终身学习以适应社会发展需要&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;显性知识隐性化以创造价值&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;共享知识以提升个人竞争力&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;持续创新以确保价值的独特性&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>书摘：《群体性孤独》-[美] 雪莉·特克尔</title>
    <link href="https://shaogui.life/2021/04/08/%E4%B9%A6%E6%91%98_%E7%BE%A4%E4%BD%93%E6%80%A7%E5%AD%A4%E7%8B%AC-[%E7%BE%8E]%20%E9%9B%AA%E8%8E%89%C2%B7%E7%89%B9%E5%85%8B%E5%B0%94.hexo/"/>
    <id>https://shaogui.life/2021/04/08/%E4%B9%A6%E6%91%98_%E7%BE%A4%E4%BD%93%E6%80%A7%E5%AD%A4%E7%8B%AC-[%E7%BE%8E]%20%E9%9B%AA%E8%8E%89%C2%B7%E7%89%B9%E5%85%8B%E5%B0%94.hexo/</id>
    <published>2021-04-08T15:28:05.000Z</published>
    <updated>2022-06-15T01:53:19.098Z</updated>
    
    <content type="html"><![CDATA[<p>这是一本思考人与机器，人与网络关系的书籍，主要认识有：</p><ol><li>在当今互联网飞速发展下，把越来越多的人联系在一起，人们非但没有感觉更加<code>热闹</code>，而是更加孤独，这就是所谓的群体性孤独！！！</li><li>人们越来越期待<code>虚拟世界</code>和<code>弱联系</code>，对面对面的交流越来越抵触，比如如今网上<code>廉价的道歉</code>；</li><li>互联网是用来提高工作效率的工具，但是我们却没有被解法，反而被它束缚，要求我们<code>时刻在线</code>；</li><li>互联网犹豫荆棘丛林，各种想法、链接、图片缠绕在一起，沉迷其中就会让我们没有办法更加深刻地思考当下，太容易随波逐流；</li><li>互联网作为新生事物，完全抛弃肯定不行，但是必须时刻警惕它对我们的影响</li></ol><p>一个搞IT，搞AI的人，居然去看这类书，我都无法找到当初的原因，或许是前方路太黑，需要灯光，或许是当下被互联网所累，学习如何更加有效利用它！</p><a id="more"></a><hr><p><strong>阅读书摘及笔记：</strong></p><blockquote><p>哈佛大学心理学教授霍华德·加德纳在30年前提出了著名的多元智能理论，认为人的智能不是简单的一种，而是由8种相互独立的智能构成的。它们分别是：语言智能、音乐智能、逻辑智能、空间智能、身体智能、自省智能、交流智能和自然智能。</p><p>与森林等自然环境的境遇相反，计算机和互联网正在成为人类赖以生存的新环境。和与自然接触会产生自然智能相同，我们与计算机和互联网接触会产生机器智能（不是传统意义上的能思考的机器，而是人们如何更好地驾驭机器的智能）、网络智能（不是网络里产生的群体智能，而是人们如何更好地利用网络解决问题的智能）。机器智能、网络智能的智商高低，未来在很大程度上会决定一个人的命运。</p><p>技术是极具诱惑力的，因为它能弥补人性中脆弱的一面。而我们的确是非常脆弱、敏感的物种。我们时常感到孤独，却又害怕被亲密关系所束缚。数字化的社交关系和机器人恰恰为我们制造了一种幻觉：我们有人陪伴，却无须付出友谊。在网络世界中我们彼此连接，同时也可以互相隐身。</p><p>发现人们不仅十分认真地把机器人视为宠物，还视为潜在的朋友、知己，甚至是虚拟的情人。我们并不关心机器人对人类与他们“分享”的情感能“知道”或“理解”多少。在机器人时代，只要人与机器连接的表演看起来足够多就行了。我们毫无偏见地、泰然自若地与毫无生命的机器连接在一起。这让我想起了一个短语：“技术滥交”</p><p>一个30岁的男人评论说：“我更愿意和一个机器人说话。和朋友们交往太累，使我筋疲力尽。机器人会一直陪伴着我，而且任何时候只要我想好了，我都可以脱离这段关系。”</p><p>社交机器人的发明说明人类兜了一个大圈子，还是无法摆脱对亲密关系的渴望。人们看起来很心甘情愿地相信：如果我们疏远或是忽视了彼此，机器人会补偿我们，程序早就设定好了，他们会带来虚拟的爱。当我们逐渐衰老，机器人会伺候我们；当我们的孩子无人照看，机器人会照料他们；当我们在逆境中精疲力竭而不能互相支撑时，机器人会给予我们能量。机器人不会对我们评头论足，我们得到前所未有的接纳和包容。</p><p>有了技术，我们惊讶于世界之“苍白”，无事表达，无人取悦。当一个化身在网络游戏里与另一个化身整晚交谈之后，在某个时刻，我们感到完全拥有一份真实的社会生活，然后接下来，在与陌生人牵强而脆弱的联系里，莫名地感到孤独无援。</p><p>用技术来处理亲密关系，人际关系会被弱化成仅仅是联系而已。而在此之后，简单的联系会被重新定义为亲密。换句话说，网络亲密（cyberintimacies）滑向了网络疏离（cybersolitudes）</p><p>在机器人的陪伴下，人们是孤单的，但也感到与他人连接在一起。在这种孤单的环境中，出现了一种新型的亲密关系。</p><p>人们通过移动设备把自己牢牢地拴在网络上，从而获得自我的新状态。第一种状态是“逃离现实世界”：也许他们正在你身边，但他们的精神已经游离到了另一个世界；第二种状态是“双重体验”：人们能够体验到“虚拟与现实的双重人生”；第三种状态是“多任务处理”：人们由于可以同时处理多种事情而赢得了更多时间。</p><p>我开始意识到，获取关于专业问题和需求的新信息并不是开始和结束一天的好方法。</p><p>人们通过移动设备把自己牢牢地拴在网络上，从而获得一种自我的新状态。从一开始，它就意味着某种授权：它可以从现实环境中脱离——包括其中的人。它能同时体验到现实世界和虚拟世界。而且它能通过多任务处理产生更多时间，这是我们21世纪的魔法。</p><p>今天的年轻人生活在“永远在线”状态，他们期待着被“打扰”。网络技术改变了人们对“分开”的理解，也让年轻人失去了“独处”的机会</p><p>现在的年轻人伴随着机器人宠物成长，处在一个完全被网络束缚的环境中。他们认为自己是新兴人类，也是对虚拟化生存没有任何偏见的第一代人。他们看出了网络化生活的巨大力量——毕竟他们冒着生命危险去查看网络上的信息。</p><p>现在的青少年也要像前辈们一样学会表达情感，去思考人生的价值和自我的意识，他们也需要学会管理和表达自己的情感，需要时间去发现自己，去思考。但是科技的发展带来了永远在线的传播服务以及快餐式的文字和图像，这完全改变了原有的规则。什么时候该停下来？什么时候该寂静无声？文字短消息的快速回复，并非不能让年轻人在人际互动中进行自我反省，但作用的确较小。当人际交互要适应小屏幕、情绪需要通过情感符号表达时，的确存在着简化的必要。</p><p>根据传统，生活在城市里的孩子在成长的过程中都有一个重要的经历：第一次独自游览这个城市。这个经历是一种成人的仪式，孩子们从此需要对自己负责了。即使感到恐惧，他们也需要独自承担这种感受。而如今手机的存在减轻了这个仪式带来的恐惧感。</p><p>青少年的独立不仅仅意味着和父母分开，也包括和同龄人分开。他们要体会友谊既是一种支持，也是一种束缚</p><p>人际关系有着复杂的多面性。网络的虚拟生活为个人提供了足够的空间，同时也让青少年难以从新的群体需求中逃脱。年轻人很自然地期待他的朋友们可以随时在线——这是一种由技术进步引发的社会契约，要求同龄人随传随在。因此，受到束缚的自我也开始习惯于此</p><p>社会学家大卫·理斯曼（David Riesman）在20世纪50年代中期写道：美国人的自我感觉从内在转向了受人支配。人们没有坚定的内在目标，依赖伙伴寻找对其自身的肯定。如今，随身携带的手机增长了受人支配的态势。在开始有一个想法或一种感觉时，我们向别人证实，几乎是提前证实。人际交流也许是简短的，但是更多的交流大可不必。人们的需求只是希望随时可以保持联系</p><p>自恋时并不是指那些爱自己的人，而是指脆弱的个性，拥有这种个性的人需要源源不断的外界支持来进行自我确认。这种个性的人不能容忍别人的复杂需求，却试图通过扭曲别人的身份，分离出自身需要的和能用的东西，以此来与他们建立联系。因此，自恋者仅以量身定做的表达来与别人交往</p><p>一个脆弱的人也可以通过选择性和限制性的与人接触，从而获得支持（也就是通讯录中最受欢迎的人）</p><p>精神病学家罗伯特·杰伊·利夫顿（Robert Jay Lifton）是埃里克森的学生，他对成熟自我的看法与老师不同。他称成熟自我是千变万化的，强调多面性。这个自我，是“流动的和多面的”，可以接受和修饰不同的思想和观念。当被赋予多元的、彼此毫不相干的、全球性的事物时，这样的自我可以变得活跃起来。</p><p>在网络空间的不和谐声中形成的自我不是多变的，而是幼稚的。而如今我认为，在这样的文化背景下成长，会让他们以自恋的方式与世界建立联系。</p><p>当我注册Facebook时，这个网站对我来说还很新鲜，我会把第一感觉记录下来。现在看来，我的第一感觉稀松平常：我在交友计划A和交友计划B之间游离不定。计划A是我只在这个网站上联系我认识的人；计划B是我会接受所有人的好友请求，因为他们都表示很欣赏我的工作。我前几周执行了计划A，转而又选择了更具包容性的计划B，因为我为吸引了众多陌生人的注意力和称赞而沾沾自喜</p><p>发短信让人有一种安全感，并且可以通过细心斟酌而展现出一个期望的自我。但虚拟空间对“道歉”等现实问题是无能为力的。打电话意味着你在全神贯注地做一件事，也意味着一种“交谈”能力。声音传递情感，我们却巴不得让声音在生活中消失</p><p>在短信、留言和电子邮件中，我们隐藏的内容不比我们表达的内容少。我们可以把自己更好的一面展示给别人，也可以更快、更容易地处理好一件事。聆听只会使我们节奏放慢。</p><p>在网络世界中，当你身处多玩家角色扮演网络游戏的时候，你不仅技艺精湛、光芒四射，更重要的是，这种行为使你处在一个新的群体当中，有虚幻的好朋友和一种归属感。人在虚幻的世界里会感到比在现实中更加自在，因为他们觉得在虚拟的世界里可以秀出一个更加优秀、更加真实的自己。随着这一切的发生，谁还愿意身处现实之中呢？</p><p>对网络生活进行思考的过程有助于区分心理学家所说的“演练”和“实践”。“演练”时，你首先将现实生活中所遇到的冲突提取出来，然后将其在虚拟的世界里一遍又一遍地进行表现。此事重复量很大，却只有微不足道的进步。而在“实践”时，你运用网络上的一些素材来应对现实生活中的矛盾并寻找新的解决方案。</p><p>有些人会选择去“告白网站”排解孤独。人们宁愿在网上对着陌生人忏悔和释放情感，也不愿意直接面对你所伤害的人给他一个真正的道歉。实际上，网上告白没有想象的那么好，人们只是为了感觉良好而用“分享”避开“孤独”。</p><p>我们所抱怨的人际关系，也是连接我们和真实生活的纽带。我们的情感宣泄运用了一种极端的手段。人们会善意地对待陌生人。寂寞和孤单是如此难以忍受，以至于和在网络虚拟世界认识的网友结婚，似乎竟然成为了我们最好的希望</p><p>将负能量释放出来可以减轻它的毒副作用；而这样的情感释放不需要与真人交互即可完成。在两种情况中，告白看上去都越来越像对话，情感宣泄看上去越来越像分享。</p><p>道歉所包含的基本要素为修补关系打下了重要的心理基础——不仅对于被伤害者，也包括伤害者本身。首先你必须知道你冒犯了别人，你承认自己的行为可能给对方造成伤害，你必须问你自己如何做才能弥补。</p><p>科技模糊了告白和道歉的界线，很容易让我们忘记道歉的真正含义，不只是因为在线空间提供给他们一个面对其他人的“廉价道歉”的选择，同时也因为我们会认为道歉本身已与他人无关。在这样的情况下，我们忘记了我们的行为可以影响到他人。</p><p>对不起’这3个字太难。如果你是那个收到道歉的人，你知道对于一个人来说，让他当面说出‘对不起’是很困难的。但是正是如此，才让我们可以原谅一个人，他们亲自说出来，说明他们内心还是有勇气想道歉的</p><p>像与机器人的交流，在线告白有吸引力是因为某些沉默的人想诉说隐秘的情感。但是如果我们通过这些网站把它们“释放出来”来解除我们的忧虑，我们就不必精确地明白这些情感的背后是什么。我们没有运用我们的感情资源来建立对我们可能有帮助的持久关系。我们不能因为这种状况而责备技术。人们对彼此失望。技术仅仅能使我们创造一个无关紧要的神话。</p><p>焦虑成了这种新型沟通模式的一部分。然而，当我们谈起移动通信改革的时候，我们习惯对以前的事物进行“尊敬的”贬低，而把新鲜事物理想化。就拿在线阅读来说，因为它可能导入链接和其他一些超文本，所以常常有着一个英雄般必胜的传言，而书本却被蔑称为“孤立的”</p><p>朱莉娅还是情不自禁地在MySpace上搜索着父亲的大家庭——他的父母、兄弟姐妹、表兄弟姐妹、叔叔及阿姨等。她说她不会跟其中的任何一人联系，至少在她和父亲邮件交流之前不会。她不知道是否MySpace能够缝合幼年时代被撕裂的那份感情。</p><p>我和我的朋友们都觉得如果没有手机，感觉就像一无所有，毫无防备。”对于一无所有，毫无防备的自己，就仿佛置身于危险处境。失去联系，便如此脆弱。</p><p>短信是如此有诱惑力。因为它产生了一个附带自己要求的承诺。这个承诺是：你发送短信的那个人会在几秒钟内收到你的短信，而不管他或她是否“空闲”，他都能够看到你的短信。其附带的要求是：当你收到一条短信时，你就要去关注它（也许你正在上课，这意味着你要低头偷偷地瞟一眼静音的手机）并且在第一时间内给予回复。</p><p>短信这种介质适合快速传达简单的陈述，而对于开始一段蕴含复杂情感的对话则不是那么合适</p><p>电话交流是如此个人化，是因为在打电话的过程中，你没有时间去坐下来考虑你将要说些什么。你所要说的话都是你真正要表达的东西。如果某人发送给你一条短信，你还有一两分钟时间来考虑你将在回复里写些什么。如果你是在一个现实的谈话中，如果你一两分钟还没有说出一句话，过了几分钟你才回答，那将是非常尴尬的。这就是我喜欢打电话的原因。我更喜欢某人是诚实的，如果你在打电话，你就完全把自己暴露出来，但同时这也比其他方式更好。</p><p>这些年轻人渴望时间、接触、关注，以及直接的沟通。他们想要生活中少一些伪装，他们怀念面对面打交道，而且每次只专注做一件事的世界。这听起来充满了讽刺的意味，因为他们这一代人最大的、也曾经是最引以为豪的特征就是“一心多用”。</p><p>斯托尔说，人类的“创造过程”机制奇特，“到目前为止，新的想法产生频率比较高的状态是冥想状态，介于走路和睡眠之间。在冥想的心理状态下，思绪和画面可以自发地组合演变……创造者可以完全放松身心，让大脑内自行产生化学作用”。但是在数字化时代，安静和独处却很难获得。</p><p>如果你想拥有一个不被打扰的交流和沟通的环境，你最好亲自找那个人面对面地谈。如果没办法直接见面那就打电话。但是如果你是坐在电脑前在网上和别人交谈，那就有很多东西可以打断你的谈话，因为互联网上有如此多的比谈话更有趣的东西在吸引着你</p><p>梭罗写道：“我步入丛林，因为我希望生活得有意义。我希望活得深刻，吸取生命中所有的精华，把非生命的一切都击溃。以免当我生命终结时，发现自己从没有活过。我不想过一种不能称之为生活的生活，活得太甜蜜，我也不想试着顺从，除非那真的有必要。</p><p>《连线》杂志创始主编凯文·凯利说，他发现在网络上可以恢复精力。他在网络的树荫下，身心得到休憩：“有时候我进入网络的原因只是为了主动地迷失自己，温柔地向网络的未知世界投降，暂时忘却自己确信的周遭生活。尽管人们设计互联网有着明确的目的性，网络却依然是狂野的，它的边界是未知的，它的神秘数不胜数。荆棘缠绕般的各种想法、链接、图片创造了一片茂密的丛林。网络好像是有生命的。</p><p>撰写自己在社交网站上的自我简介、用即时聊天工具聊天，没有比这种“艰苦”的劳动更为“深刻”的了。人们在线的大多数时间都是在潜水漫游、跟随链接、伸出随机的“触须”。一个人在朋友的网络相册里晃来晃去，然后又到其他朋友的相册里面，在一个几乎不认识的人发布的信息下面留下评论。梭罗抱怨人们总是太过于急着和别人分享观点。而在虚拟世界中，Facebook总是鼓励我们随时分享“我们大脑里面存在的东西”，无论这些思想是多么的无知，或者多么浅薄，然后它会帮助我们传播给最广的听众。每天，我们被其他人“随机”的想法所轰炸。我们已经对这种“宣泻”司空见惯。所以尽管网络身份以及个人简介都是经过了深思熟虑的设计，但是人们最终感觉唯一深思熟虑的东西只是自己投身网络的决定。做完这个决定之后，人们开始在网络的洪流中随波逐流了</p><p>对于那些一直保持在线的人来说，尽管有很多问题（比如像表演一样的生活，比如失去了面对面察言观色的能力），但随时有人陪伴也带来了许多欢乐。而对于那些没有连网的人来说，即使是在自己家乡的大街上，他们也会有一种怪异的孤独感。</p><p>神圣空间”这个词语成为了我关注的重要概念。他们中的每一个人都保持着自己的专业生活，纯洁而不受污染。他们之所以这样做，是因为他们想要和虚拟保持距离。在那个空间里，他们能最大化地感觉到他们最完整的自己。</p><p>一个神圣的空间不是为了躲藏自己，而是一个我们认识自己和责任的地方</p><p>我们对科技的期待越来越多，对彼此的期待却越来越少。我们正处于一个完美风暴的静止中心，浑然不觉已成了科技的奴仆。我们不会放弃互联网，也不可能一下子 “戒掉”手机。我们自己才是决定怎样利用科技的那个人，记住这一点，我们就一定能够拥有美好的未来。</p><p>我们已经变成了电脑的“杀手级”应用程序</p><p>我们在网络上很容易找“同伴”，但是我们却被“自我表演”的压力搞得疲惫不堪。我们虽然享受着不间断的联系，却极少给予彼此全部的注意力。我们可以随时获得关注，却为不断出现的新缩略语所累。我们很喜欢网络“懂”我们，但是这只有在个人隐私问题上做了妥协才会有可能</p><p>网络生活留下了大量的“电子面包屑”，一些公司可以为了商业或政治目的而进行开发。我们在网络上会有很多新的邂逅，但是这种关系都是短暂的，如果有新的或者更好的邂逅出现，那些以前的都将被尘封。事实上，新的邂逅并不一定是更好的，因为我们仅仅是喜新厌旧。我们随时连线，对新鲜事保持积极地回应。我们可以在家里办公，但是工作也同时渗入到私人生活中，直到我们几乎不能分辨出它们之间的界限。我们能够瞬间连接彼此，但是我们有时却不得不藏起我们的电话，强迫自己去享受片刻的安宁</p><p>科技带来的高生活节奏让我们疲惫不堪，我们考虑通过更新、更高效的技术把我们从中解救出来。但是新的技术设备却带来更大的信息量和传播速率。在这种速率递增的需求的背景下，其中一件让我们感到满意的事情就是用技术来连接远方的人们，或者更为精确地说，是连接很多来自远方的人</p><p>通过互联网所形成的连接并没有把我们联系得更紧密，这些连接却让我们沉迷其中无法自拔。我们会在晚餐的时候忙于发短信。当我们慢跑散步的时候，当我们开车的时候，当我们在公园陪孩子荡秋千的时候，我们都在发信息。我们不想打扰别人，因此我们不停地打扰别人，只是非“实时”罢了</p><p>当这些情感聚集在一起时，可能就形成了“后家庭主义时代的家庭”。家庭成员很孤独地待在一起，每个人都在自己的房间里，每个人都在用电脑或者手机等移动设备上网。我们因为忙碌而使用网络，但是却和技术一起花费了更多的时间，而与现实生活中的人们之间花费的时间越来越少。我们坚信网络连接是接近彼此的方法，即使它也是同样有效地躲避和隐藏彼此的方法。在这个限度内，如果必须要减少与现实中人们相处的时间，我们会满足于这种无生命的东西</p><p>我们，创造和赋予了机器人生命，并且开始谈论机器人的情感，甚至它们的“真实性”。如果我们关注的是机器人能够唤起我们自己内心的情感，那么这么做是可以的。但是我们常常忽略的问题是：“机器人的感受是什么？”我们知道机器人不能感受：他们不能感知到人的感情变化，或者人类关系的流动性。事实上，机器人什么也感受不到。而我们关心这个吗？或者它们表现出有感觉的样子，对于我们来说足够了吗？为什么我们情愿和那些既不能理解、又不能关心我们的机器人交谈？</p><p>理解ELIZA受欢迎不仅是因为人们愿意和机器交谈，它也说明人们变得不愿意和彼此交流。机器人保姆提供了一种新的可能，那就是我们可以逃离彼此，也可以很好地生活下去。当我们期待着电脑法官、电脑顾问、电脑老师或者电脑牧师时，我们事实上是对那些根本不关心我们、带着偏见对待、甚至虐待我们的人表达了失望。正是对于这些人的失望让机器人的“关心”看上去足够真实。我们心甘情愿地忽略机器人缺乏理解力的弱点，对这一点置若罔闻，转而去努力地让它看上去似乎更加善解人意。所有的这一切是为了创造一个假象—— 一个可以替代人类存在的东西。这就是更深层次的ELIZA效应。对于ELIZA的信任不仅说明了我们认为ELIZA程序可以理解我们，更说明了我们对彼此缺乏信任。</p><p>我们生活在繁荣的社交媒介文化里，我们梦想着社交机器人。尽管彼此连接，我们却依旧孤独，只能送给自己科技情人。如果网络生活太过严苛，那么机器人则总和我们在一起。想拥有机器人伴侣既是病症，也是梦想</p><p>就像其他心理学病症一样，它 “解决”问题但却未阐明问题。我们将会获得机器人的陪伴，却不必承担类似于人与人之间亲密关系所带来的风险。机器人暴露了我们希望能够控制社交关系的愿望，这正是我们的梦想</p><p>这种病症常常携带着大量的信息，以至于让人难以承受。为了承受这种恐惧，病症会把这些信息伪装起来，人们就不必每天都面对这些恐惧。所以，感觉持续的饥饿要比明白你的妈妈没有养育你更加“简单”。被超市排的长队弄得满心怒火，比处理你的配偶没有给予你所需要的关注更“容易”。当科技变成了一种病症，它就切断了表面现象和挣扎背后的真正原因之间的关联</p><p>俄狄浦斯（Oedipus）的神话故事。作为一个传统的、广为人知的故事，人们通常会认为俄狄浦斯因为追求知识而被惩罚——尤其是关于他出身的知识。卡珀说俄狄浦斯被惩罚另有原因——他拒绝承认知识的局限性。类似的问题也出现在我们对科技的态度上。我们失职并非因为我们试图建设一个新的东西，而是因为我们不允许自己去考虑新科技瓦解了什么。我们并不是因为发明和创造而陷入麻烦，而是因为我们认为它可以解决一切问题</p><p>我发现自己对网络充满了感激。它是一位坚定的施恩者，永远都在那里。我用不安分的手指爱抚它，它挑起了我的欲望，就像一个爱人一样……我想一直沉浸在它深不可测的广度之中。停留在那里，醉心于它梦幻般的怀抱里。向网络投降就像去土著丛林徒步旅行。不合逻辑的梦慰藉着你。在这个梦里，你在不同的页面和想法中穿梭跳跃。网络的白日梦已经深深地触动了我，让我感动并且搅动着我的心。</p><p>网络的连接性可以平复我们心灵最深处对孤独、失去和死亡的恐惧。这是一种令人欣喜的东西。但是连接也破坏了与原本维系我们的东西之间的联系，比如面对面的人际交流的价值</p><p>科技给了越来越多我们认为自己想要的东西。如今我们可以很容易地找到社交机器人和数字化的朋友。有人或许认为我们需要什么，它总在我们的所及范围内，我们永远也不会感到孤独。还有人假定我们想要的是大量的弱联系，支撑在线熟人关系的、非正式的网络关系。但是倘若我们真正思考我们认为自己想要东西的后果，我们才会了解我们真正想要的是什么。我们也许想要一些安静和独处的时光</p><p>正如梭罗提出的那样，我们也许想要生活少一些“拥挤”，等待更多不常发生的、但是很有意义的面对面邂逅。因为我们把很多时间花费在打字上面——用所有的手指或者只用拇指，我们会怀念人的声音。我们也许觉得和一个机器人下象棋也不算太坏，但是机器人却无法代替任何关于家庭或者朋友之间的谈话。一个机器人或许有需求，但是倘若要理解人的欲望，则需要语言和有血有肉的身体。因此，要进行这样的谈话，我们必须有一个真正的朋友，首先重要的是，他可以明白生命的真正含义，理解父母和家庭的含义，理解成年人之间爱的含义，理解对于子女的渴望，并且可以理解生老病死是不可避免的事情</p><p>现实生活中的人做事遵循一致性原则，所以当我们的关系进展顺利时，改变是逐步的，作用是缓慢的。但是在虚拟的网络生活里，所有关系的节奏加快了很多。一个人会很快从迷恋到幻灭，然后又回来，并在这个过程中来回穿梭。一个人如果在现实生活中感到有些无聊，很容易就能和一些新朋友联系上。一个人匆忙地阅读一长串邮件，并学会如何抓住“亮点”。夸张的标题往往能吸引注意力。在网络游戏里面，人物的动作总是被精简到一个从惊恐到安全、然后如此反复的模式。一个令人恐惧的邂逅自动完成，处理完了一个之后你可以重新组队，然后又用一个新的人物。我们的肾上腺素不停地向上冲，在这里没有所谓的“空闲时间”</p><p>通过对网络生活的研究，我认为，亲密行为是人和人之间的行为，听到他们的声音，看到他们的脸和试着读懂他们的内心。这些研究也让我想到，从某种角度来说，独处是刷新自己的状态，恢复精力。而孤独是失败的独处。去体验独处，你必须有自我振作、自我鼓励的能力，要不然你只能明白如何变得孤独。</p><p>今天的“回归”首要表现在进行无网络生活的实验。但是网络已经成为我们获得教育、获取新闻、找工作等不可或缺的必然途径。它已经成为我们生活的一部分。因此，我们只能退而求其次，要求自己重塑在屏幕上的生活。寻找新的平衡不仅仅是“放慢节奏”，而是“我们该如何为自我反思腾出空间？”</p><p>为了和这种上瘾作斗争，你必须抛弃这些令人上瘾的物质。但是，我们是不会放弃使用互联网的。我们也不会“突然完全戒掉”手机，或者禁止孩子们使用手机。我们不会停止听音乐或是回到以电视为中心的家庭生活方式</p><p>我相信，我们会找到一种全新的方式来沟通和连接彼此。但是，如果只考虑我们是有害物质（比如互联网技术）的受害者，并不是一个解决问题的好的开始。这种上瘾，是因为我们知道，我们不会让自己感到绝望。我们不得不找到一种方法使自己与这种让人上瘾的科技和平相处，并且让它按照我们的意愿来发挥作用。这确实是非常困难的，但却很有效果。出于对技术的简单热爱，或是出于反对技术进步的冲动，都是无济于事的</p><p>如今，我们和网络之间存在的问题让人困惑，无法忽略。最极端的情况是，我们有可能深陷网络连接不能自拔而忽略了彼此的存在。我们没有必要抛弃科技，或是贬低它的价值。我们需要的是把科技放回到它应处的位置。</p><p>群体性孤独》这本书描述了一对矛盾：我们对科技的期盼越来越多，却对彼此的期盼越来越少。我们处于一个完美风暴的静止中心。我们被科技打败了，被吸引到一个低风险并且唾手可得的联系上：Facebook上的朋友，虚拟化身，在线聊天等。如果“方便”和“可控”仍然是我们生活的首选，那么我们可能还被社交机器人诱惑而沉迷其中。就像是一个赌徒在老虎机的卡槽边上，那些令人兴奋的程序向我们许诺，让我们沉迷于游戏里而不能自拔。在机器人时代，我们必须注意到这一点，我们不再抱怨，而是期望甚至渴望简化和减少联系</p><p>一个耸肩是有助于缓和僵局的。但这还不是我们所处的阶段，我们距离山穷水尽的僵局还为时尚早。然而，我相信我们已经到达了一个反思的转折点，我们可以看到我们为科技进步付出的代价，并开始采取一些行动</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;这是一本思考人与机器，人与网络关系的书籍，主要认识有：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在当今互联网飞速发展下，把越来越多的人联系在一起，人们非但没有感觉更加&lt;code&gt;热闹&lt;/code&gt;，而是更加孤独，这就是所谓的群体性孤独！！！&lt;/li&gt;
&lt;li&gt;人们越来越期待&lt;code&gt;虚拟世界&lt;/code&gt;和&lt;code&gt;弱联系&lt;/code&gt;，对面对面的交流越来越抵触，比如如今网上&lt;code&gt;廉价的道歉&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;互联网是用来提高工作效率的工具，但是我们却没有被解法，反而被它束缚，要求我们&lt;code&gt;时刻在线&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;互联网犹豫荆棘丛林，各种想法、链接、图片缠绕在一起，沉迷其中就会让我们没有办法更加深刻地思考当下，太容易随波逐流；&lt;/li&gt;
&lt;li&gt;互联网作为新生事物，完全抛弃肯定不行，但是必须时刻警惕它对我们的影响&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一个搞IT，搞AI的人，居然去看这类书，我都无法找到当初的原因，或许是前方路太黑，需要灯光，或许是当下被互联网所累，学习如何更加有效利用它！&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>在windows上使用nuitka打包Python项目</title>
    <link href="https://shaogui.life/2021/03/16/Nuitka%E6%89%93%E5%8C%85%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    <id>https://shaogui.life/2021/03/16/Nuitka%E6%89%93%E5%8C%85%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</id>
    <published>2021-03-16T06:48:23.000Z</published>
    <updated>2023-04-08T12:58:31.991Z</updated>
    
    <content type="html"><![CDATA[<p>Nuitka是Python编译器，它是用Python编写，对Python解释器的无缝替换或扩展，兼容多个CPython版本，利用该工具可以对Python文件进行打包。本文用于介绍如何在windows上使用nuitka工具打包Python，包含构建打包环境、安装nuitka、测试打包。</p><a id="more"></a><h2 id="打包环境介绍"><a href="#打包环境介绍" class="headerlink" title="打包环境介绍"></a><strong>打包环境介绍</strong></h2><p>本次打包所使用的软硬件环境如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">System: windows 10 企业版 19042.804</span><br><span class="line">NVIDIA DERVER 461.</span><br><span class="line">mingw 8.0.0</span><br><span class="line">visual studio 2017</span><br><span class="line">python 3.7.10</span><br><span class="line">nuitka 0.6.12.3</span><br><span class="line">tensorflow-gpu  2.5.0.dev20210308</span><br><span class="line">cuda 11.1</span><br><span class="line">cudnn 8.0.4</span><br><span class="line">numpy 1.20.1</span><br></pre></td></tr></table></figure><h2 id="安装nuitka1"><a href="#安装nuitka1" class="headerlink" title="安装nuitka1"></a>安装nuitka<sup><a href="#fn_1" id="reffn_1">1</a></sup></h2><ol><li>安装C编译器，有以下两个选择，任选一个安装<ul><li>根据系统配置，<a href="https://sourceforge.net/projects/mingw-w64/files/mingw-w64/mingw-w64-release/">下载并安装</a>MinGW64，基于gcc8.0以上的版本，安装过程参考</li><li>根据系统配置，<a href="https://visualstudio.microsoft.com/downloads/">下载并安装</a>Visual Studio 2019以上的版本</li></ul></li><li><a href="https://www.python.org/downloads/windows">下载并安装</a>Python，确保版本为：2.6、2.7或3.3、3.4、3.5、3.6、3.7、3.8、3.9 其中一个</li><li>使用以下命令安装nuitka</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install nuitka</span><br></pre></td></tr></table></figure><h2 id="测试nuitka打包2"><a href="#测试nuitka打包2" class="headerlink" title="测试nuitka打包2"></a>测试nuitka打包<sup><a href="#fn_2" id="reffn_2">2</a></sup></h2><p>新建文件mdl.py，内容如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printinfo</span>(<span class="params">info</span>):</span></span><br><span class="line">print(info)</span><br></pre></td></tr></table></figure><p>同目录下新建main.py，内容如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> printinfo <span class="keyword">import</span> printinfo</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    printinfo(<span class="string">&#x27;Hello nuitka&#x27;</span>)</span><br></pre></td></tr></table></figure><p>测试运行无误后，运行以下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nuitka --output-dir=<span class="built_in">test</span> hello.py</span><br></pre></td></tr></table></figure><p>运行编译之后的exe程序，得到正确结果即安装完成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.\<span class="built_in">test</span>\main.exe</span><br></pre></td></tr></table></figure><p><strong>生成文件说明</strong></p><p>生成目录下，文件清理如下：</p><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082058632.png" alt="image-20210316145017152"></p><ul><li>main.build     nuitka打包过程的中间文件，可删除</li><li>main.exe        nuitka打包得到的可执行文件</li><li>python37.dll  Pyhton安装目录下的python37.dll的拷贝，代码使用的库包依靠该文件去查找</li></ul><blockquote id="fn_1"><sup>1</sup>. <a href="https://zhuanlan.zhihu.com/p/341099225">Nuitka入门指南-新手必备 - 知乎</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2 "><sup>2 </sup>. <a href="https://www.nuitka.net/doc/user-manual.html#id5">Nuitka User Manual</a><a href="#reffn_2 " title="Jump back to footnote [2 ] in the text."> &#8617;</a></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;Nuitka是Python编译器，它是用Python编写，对Python解释器的无缝替换或扩展，兼容多个CPython版本，利用该工具可以对Python文件进行打包。本文用于介绍如何在windows上使用nuitka工具打包Python，包含构建打包环境、安装nuitka、测试打包。&lt;/p&gt;</summary>
    
    
    
    <category term="编程" scheme="https://shaogui.life/categories/%E7%BC%96%E7%A8%8B/"/>
    
    
    <category term="打包" scheme="https://shaogui.life/tags/%E6%89%93%E5%8C%85/"/>
    
    <category term="python" scheme="https://shaogui.life/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>部署深度学习模型时的全流程加密方案探索</title>
    <link href="https://shaogui.life/2021/03/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%83%A8%E7%BD%B2%E5%85%A8%E6%B5%81%E7%A8%8B%E5%8A%A0%E5%AF%86%E6%96%B9%E6%A1%88%E6%8E%A2%E7%B4%A2/"/>
    <id>https://shaogui.life/2021/03/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%83%A8%E7%BD%B2%E5%85%A8%E6%B5%81%E7%A8%8B%E5%8A%A0%E5%AF%86%E6%96%B9%E6%A1%88%E6%8E%A2%E7%B4%A2/</id>
    <published>2021-03-11T03:40:45.000Z</published>
    <updated>2023-04-08T13:09:15.898Z</updated>
    
    <content type="html"><![CDATA[<p>本文用于探索深度学习模型在部署全流程过程中的整体方案</p><a id="more"></a><h2 id="部署场景"><a href="#部署场景" class="headerlink" title="部署场景"></a>部署场景</h2><p><strong>涉及程序端及编程语言</strong>：界面端（C#）、服务端（C#）、训练端（Python）</p><p><strong>剥离加密之后的流程</strong>：</p><ol><li>界面端根据训练配置调用训练端</li><li>训练端训练结束后保存模型（结构、文件）</li><li>服务端加载模型</li></ol><h2 id="加密要求"><a href="#加密要求" class="headerlink" title="加密要求"></a>加密要求</h2><p>由C#编写的程序部署时会将其编译为二进制，无需加密保护，主要是针对训练端的Python及训练得到的模型，有以下要求：</p><ol><li>无法明文看到Python代码</li><li>无法获得模型（结构与权重）</li><li>windows上部署</li><li>加密方案不能大幅度增加部署成本</li></ol><h2 id="加密方案"><a href="#加密方案" class="headerlink" title="加密方案"></a>加密方案</h2><p>针对Python加密以及模型的加密，调查了主流的加密方案</p><h4 id="网络收集的Python加密思路"><a href="#网络收集的Python加密思路" class="headerlink" title="网络收集的Python加密思路"></a>网络收集的Python加密思路</h4><div class="table-container"><table><thead><tr><th>序号</th><th>工具</th><th>方法描述</th><th>加密及解密</th><th>优缺点</th></tr></thead><tbody><tr><td>1</td><td>Nuitka<sup><a href="#fn_4" id="reffn_4">4</a></sup><sup><a href="#fn_10" id="reffn_10">10</a></sup></td><td>.py 文件先被转成了 .c 文件，然后被编译成 .o 文件，最后合并成 .bin 可执行文件，从 bin 到 C 是不可逆的，从 C 到 Python 也是不可逆的，因此代码是安全的</td><td>编译为bin,或者编译为动态链接库.so 文件</td><td>工作量小，安全性高，使用加密之后的Python便捷；编译时间长，过程复杂</td></tr><tr><td>2</td><td>发行.pyc文件<sup><a href="#fn_5" id="reffn_5">5</a></sup></td><td>通过compileall模块将.py文件转为.pyc文件，该文件是二进制，无法直接看源代码，而python解释器可以直接执行.pyc文件</td><td></td><td>台兼容性好，.py 能在哪里运行，.pyc 就能在哪里运行；解释器兼容性差，.pyc 只能在特定版本的解释器上运行。有现成的反编译工具，破解成本低</td></tr><tr><td>3</td><td>代码混淆（oxyry，pyobfuscate）<sup><a href="#fn_5" id="reffn_5">5</a></sup></td><td>让人看不懂代码，移除注释和文档，改变缩进，在tokens中间加入一定空格，重命名函数、类、变量，在空白行插入无效代码</td><td></td><td>提高了一点源码破解门槛。兼容性好，只要源码逻辑能做到兼容，混淆代码亦能；只能对单个文件混淆，无法做到多个互相有联系的源码文件的联动混淆</td></tr><tr><td>4</td><td>py2exe<sup><a href="#fn_5" id="reffn_5">5</a></sup></td><td>将源码编译为 .pyc 文件，加之必要的依赖文件，一起打包成一个可执行文件。最终 py2exe 打包出的是二进制文件。</td><td></td><td>直接打包成 exe，方便分发和执行。破解门槛比 .pyc 更高一些；兼容性差，只能运行在 Windows 系统上。生成的可执行文件内的布局是明确、公开的，可以找到源码对应的 .pyc 文件，进而反编译出源码。</td></tr><tr><td>5</td><td>Cython<sup><a href="#fn_5" id="reffn_5">5</a></sup><sup><a href="#fn_6" id="reffn_6">6</a></sup></td><td>将 .py/.pyx 编译为 .c 文件，再将 .c 文件编译为 .so(Unix) 或 .pyd(Windows)</td><td></td><td>生成的二进制 .so 或 .pyd 文件难以破解。同时带来了性能提升；兼容性稍差，对于不同版本的操作系统，可能需要重新编译。虽然支持大多数 Python 代码，但如果一旦发现部分代码不支持，完善成本较高。</td></tr><tr><td>6</td><td>Pyinstaller<sup><a href="#fn_7" id="reffn_7">7</a></sup><sup><a href="#fn_8" id="reffn_8">8</a></sup></td><td>打包为exe文件，</td><td></td><td>将Python文件转换为exe文件，以及dist文件夹和build文件夹，如果要移植到其他电脑上运行，也是只需要将这两个文件夹复制到对方电脑上，即使对方没有python环境，也可以运行程序，具有较好的兼容性；pyinstxtractor.py可以进行反编译</td></tr></tbody></table></div><p>注：py是源文件，pyc是源文件编译后的文件，pyo是源文件优化编译后的文件，pyd是其他语言写的python库<sup><a href="#fn_9" id="reffn_9">9</a></sup></p><h4 id="网络收集的模型加密思路"><a href="#网络收集的模型加密思路" class="headerlink" title="网络收集的模型加密思路"></a>网络收集的模型加密思路</h4><div class="table-container"><table><thead><tr><th>序号</th><th>方法描述</th><th>加密及解密</th><th>优缺点</th></tr></thead><tbody><tr><td>1</td><td>将模型转换为二进制，直接打开看不见原始内容<sup><a href="#fn_1" id="reffn_1">1</a></sup></td><td>ncnn2mem工具可以将ncnn模型转为二进制的：ncnn2mem resnet.param resnet.bin</td><td>使用netron可以查看文件，反编译成本很低</td></tr><tr><td>2</td><td>将模型打包为C code，并嵌入到程序中<sup><a href="#fn_1" id="reffn_1">1</a></sup></td><td>ncnn2mem resnet.param resnet.id.h resnet.mem.h,把这个文件 include 进来，用内存加载接口，把模型当作代码直接嵌入编译进程序中</td><td>分发exe即可，虽然不能直接获得模型，但是能用 objdump 或者十六进制编辑器从 exe 静态区中把模型抠出来</td></tr><tr><td>3</td><td>使用专用加密库对模型加密<sup><a href="#fn_1" id="reffn_1">1</a></sup></td><td>用 openssl，把 param.bin 和 bin 两个文件用 AES 加密成 param.bin.enc 和 bin.enc；程序实现以下三步，加载加密模型：读enc文件、解密到内存、从内存加载模型</td><td>可以从算法中xor pattern或获得密钥；堆内存上暴力查找 enc 大小左右的连续内存和关键字，把模型从内存里抠出来</td></tr><tr><td>4</td><td>自定义加密算法和数据读取<sup><a href="#fn_1" id="reffn_1">1</a></sup></td><td>用普通 xor 混淆实现</td><td>任意时刻内存中都不会存在完整的模型内容，边解密边加载</td></tr><tr><td>5</td><td>给模型加些自定义 op<sup><a href="#fn_1" id="reffn_1">1</a></sup></td><td>cnn 可以自定义 op，可以运行时注册自定义 op，可以直接改 param</td><td>即便看到了明文的 param，也容易被名字欺骗</td></tr><tr><td>6</td><td>将外部文件嵌入二进制文件(exe，dll)，并加壳保护该文件<sup><a href="#fn_2" id="reffn_2">2</a></sup></td><td>直接程序之间调用</td><td>这种方法开发量小，仅需要将资源文件嵌入并在运行时加载。</td></tr><tr><td>7</td><td>自定义的外部文件加密方式<sup><a href="#fn_2" id="reffn_2">2</a></sup></td><td></td><td>在加载模型文件前解密，考虑到安全性，防止解密后的模型文件暴露于内存被轻易dump，考虑使用流式加密的方法进行加解密，由此相对安全一点。</td></tr><tr><td>8</td><td>用protobuf自定义一种格式呀，没有协议文件<sup><a href="#fn_3" id="reffn_3">3</a></sup></td><td></td><td>工程量比较大</td></tr><tr><td>9</td><td>部署到云端给客户api接口调用<sup><a href="#fn_3" id="reffn_3">3</a></sup></td><td></td><td>特定场景不适合</td></tr></tbody></table></div><p><strong>参考资料：</strong><br><sup><a href="#fn_1" id="reffn_1">1</a></sup>: <a href="https://zhuanlan.zhihu.com/p/268327784?utm_source=wechat_timeline">如何加密ncnn模型 - 知乎</a></p><blockquote id="fn_2"><sup>2</sup>. <a href="https://blog.csdn.net/atp1992/article/details/87636173?utm_medium=distribute.pc_relevant_download.none-task-blog-searchFromBaidu-7.nonecase&amp;dist_request_id=&amp;depth_1-utm_source=distribute.pc_relevant_download.none-task-blog-searchFromBaidu-7.nonecas">浅谈深度学习模型如何保护—AES加密文件流的实现（带源码）</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_4"><sup>4</sup>. <a href="https://www.zhihu.com/question/299880517/answer/1719845490">如何防止商用的深度学习模型源码泄露？</a><a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. <a href="http://www.361way.com/python-encrypt/6159.html">现有Python 代码加密方案 - 运维之路</a><a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. <a href="https://cloud.tencent.com/developer/article/1661136">用Cython加密打包python项目</a><a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote><blockquote id="fn_7"><sup>7</sup>. <a href="https://www.cnblogs.com/hulk-1029/p/12106630.html">利用pyinstaller打包加密Python项目</a><a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a></blockquote><blockquote id="fn_8"><sup>8</sup>. <a href="https://blog.csdn.net/weixin_43652669/article/details/106401233">Pyinstaller 打包加密python项目</a><a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a></blockquote><blockquote id="fn_9"><sup>9</sup>. <a href="https://blog.csdn.net/willhuo/article/details/49886663">python py、pyc、pyo、pyd文件区别</a><a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. <a href="https://www.zhihu.com/question/299880517">如何防止商用的深度学习模型源码泄露？ - 知乎</a><a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><blockquote id="fn_10"><sup>10</sup>. <a href="https://zhuanlan.zhihu.com/c_1245860717607686144">Nuitka-Python打包exe - 知乎</a><a href="#reffn_10" title="Jump back to footnote [10] in the text."> &#8617;</a></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文用于探索深度学习模型在部署全流程过程中的整体方案&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://shaogui.life/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="部署" scheme="https://shaogui.life/tags/%E9%83%A8%E7%BD%B2/"/>
    
    <category term="加密" scheme="https://shaogui.life/tags/%E5%8A%A0%E5%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>在windows上安装MinGW-w64</title>
    <link href="https://shaogui.life/2021/03/10/windows%E4%B8%8A%E5%AE%89%E8%A3%85minGW/"/>
    <id>https://shaogui.life/2021/03/10/windows%E4%B8%8A%E5%AE%89%E8%A3%85minGW/</id>
    <published>2021-03-10T01:40:45.000Z</published>
    <updated>2022-06-15T01:53:18.247Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>MinGW(全称为，Minimalist GNU for Windows)，它实际上是将经典的开源 C语言编译器 GCC 移植到了 Windows 平台下，并且包含了 Win32API ，因此可以将源代码编译为可在 Windows 中运行的可执行程序。而且还可以使用一些 Windows 平台不具备的，但是Linux平台具备的开发工具和API函数。用一句话来概括就是：MinGW 就是 GCC 的 Windows 版本<sup><a href="#fn_1" id="reffn_1">1</a></sup></p></blockquote><a id="more"></a><h2 id="MinGW-w64安装"><a href="#MinGW-w64安装" class="headerlink" title="MinGW-w64安装"></a>MinGW-w64安装</h2><p>有两种方式安装MinGW-w64，第一种是下载压缩包安装；第二种是使用<a href="https://sourceforge.net/projects/mingw-w64/files/Toolchains%20targetting%20Win32/Personal%20Builds/mingw-builds/installer/mingw-w64-install.exe">MinGW-w64下载器</a>安装。下面以浏览器下载安装为例，介绍其安装流程及压缩包命名规则。</p><h4 id="下载压缩包安装"><a href="#下载压缩包安装" class="headerlink" title="下载压缩包安装"></a>下载压缩包安装</h4><ol><li><strong>下载压缩包：</strong><a href="https://sourceforge.net/projects/mingw-w64/files/mingw-w64/">在这里下载</a>MinGW-w64，文件命名方式<a href="#filename_rule">如下</a>，根据自己系统及需要下载对应版本，本此安装系统为windows10(64bit)，选择了<a href="https://sourceforge.net/projects/mingw-w64/files/Toolchains targetting Win64/Personal Builds/mingw-builds/8.1.0/threads-posix/sjlj/x86_64-8.1.0-release-posix-sjlj-rt_v6-rev0.7z">x86_64-posix-sjlj</a></li><li><strong>解压压缩包：</strong>将路径<code>mingw64/bin</code>配置到系统环境变量<code>PATH</code>中；<code>mingw64/lib</code>配置到环境变量LIB中；<code>mingw64/bin</code>配置到环境变量INCLUDE中</li><li><strong>测试安装：</strong>打开windows终端cmd，输入<code>gcc -v</code>，无误后安装成功</li></ol><h4 id="文件命名方式解释-1"><a href="#文件命名方式解释-1" class="headerlink" title="文件命名方式解释 1"></a><span id="filename_rule">文件命名方式解释</span> <sup><a href="#fn_1" id="reffn_1">1</a></sup></h4><ul><li><strong>version</strong>: GCC编译器版本</li><li><strong>architecture</strong>: 电脑系统类型，i686指32位系统；x86_64指64位系统</li><li><strong>threads</strong>: 线程类型，posix指可移植的操作系统接口，UNIX系统支持该标准；win32指windows下的一个标准</li><li><strong>exception</strong>: 异常处理类型，32位系统有2种：dwarf和sjlj；64位系统同样2种：seh 和 sjlj。3种类型的区别为<sup><a href="#fn_2" id="reffn_2">2</a></sup>：<ul><li><strong>sjlj</strong>：可用于32位和64位 – 不是“零成本”：即使不抛出exception，也会造成较小的性能损失（在exception大的代码中约为15％） – 允许exception遍历例如窗口callback</li><li><strong>seh</strong>：结构化异常处理，利用FS段寄存器，将原点压入栈，遇到异常弹出，seh 是新发明的，而 sjlj 则是古老的，seh 性能比较好，但不支持 32位。 sjlj 稳定性好，支持 32位</li><li><strong>dwarf</strong>：只有32位可用 – 没有永久的运行时间开销 – 需要整个调用堆栈被启用，这意味着exception不能被抛出，例如Windows系统DLL。</li></ul></li><li><strong>build revision</strong>: 建立修订</li></ul><p><strong>参考资料：</strong></p><blockquote id="fn_1"><sup>1</sup>. <a href="https://blog.csdn.net/u010429831/article/details/106766165/">Windows系统下安装配置 MinGW-w64 开发环境_yunfan-CSDN博客</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. <a href="https://www.cnblogs.com/cangqinglang/p/11074124.html">MinGW-w64安装教程——著名C/C++编译器GCC的Windows版本</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;MinGW(全称为，Minimalist GNU for Windows)，它实际上是将经典的开源 C语言编译器 GCC 移植到了 Windows 平台下，并且包含了 Win32API ，因此可以将源代码编译为可在 Windows 中运行的可执行程序。而且还可以使用一些 Windows 平台不具备的，但是Linux平台具备的开发工具和API函数。用一句话来概括就是：MinGW 就是 GCC 的 Windows 版本&lt;sup&gt;&lt;a href=&quot;#fn_1&quot; id=&quot;reffn_1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="软件工具" scheme="https://shaogui.life/categories/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="MinGW64" scheme="https://shaogui.life/tags/MinGW64/"/>
    
    <category term="windows" scheme="https://shaogui.life/tags/windows/"/>
    
  </entry>
  
  <entry>
    <title>使用oracle VM vitualbox安装windows10系统</title>
    <link href="https://shaogui.life/2021/03/09/%E4%BD%BF%E7%94%A8oracle-VM-vitualbox%E5%AE%89%E8%A3%85windows10%E7%B3%BB%E7%BB%9F/"/>
    <id>https://shaogui.life/2021/03/09/%E4%BD%BF%E7%94%A8oracle-VM-vitualbox%E5%AE%89%E8%A3%85windows10%E7%B3%BB%E7%BB%9F/</id>
    <published>2021-03-09T09:35:11.000Z</published>
    <updated>2023-04-08T12:35:26.621Z</updated>
    
    <content type="html"><![CDATA[<p>由于工作需要，在windows10工作机编译的代码需要在干净的windows中测试代码运行效果，所以本文介绍如何使用oracle VM vitualbox安装windows10系统，已经配置主机与虚拟机之间的文件传输过程</p><a id="more"></a><p>为避免大段文字及节省时间，以下简单描述安装步骤：</p><h2 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h2><ol><li><p><a href="https://msdn.itellyou.cn/">到此选择</a>合适的系统，获得下载连接<code>ed2k://|file|xxxxxxx</code>，然后使用迅雷下载该文件</p></li><li><p><a href="https://www.virtualbox.org/wiki/Downloads">到此下载</a>并安装oracle VM vitualbox</p></li></ol><h2 id="安装windows10"><a href="#安装windows10" class="headerlink" title="安装windows10"></a>安装windows10</h2><ol><li>启动oracle VM vitualbox，并选择<code>新建</code>按钮</li><li>命名虚拟机，选择<code>类型</code>及<code>版本</code>，然后点击<code>下一步</code>，注意和第一步下载的系统对应</li><li>配置虚拟机使用内存，点击<code>下一步</code></li><li>点击<code>现在创建虚拟硬盘</code>虚拟硬盘，然后点击<code>下一步</code></li><li>选择虚拟硬盘类型为<code>VDI</code>，然后点击<code>下一步</code></li><li>选择虚拟硬盘大小为<code>动态分配</code>，然后点击<code>下一步</code></li><li>选择虚拟硬盘的<code>文件大小及大小</code>，然后点击<code>创建</code></li><li>至此完成虚拟机的创建，后面的将第一步下载的文件挂载至虚拟机硬盘上，才能启动虚拟机</li><li>右键点击新建的虚拟机，依次选择<code>设置-&gt;存储</code>，点击选择虚拟盘，选择第一步下载的系统，操作看下图</li><li>退出设置后，启动虚拟机，并安装windows10系统</li></ol><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082034346.png" alt="image-20210309171724366"></p><h2 id="配置虚拟机"><a href="#配置虚拟机" class="headerlink" title="配置虚拟机"></a>配置虚拟机</h2><p>通过以下两种方式配置虚拟机与主机之间的文件传输</p><p><strong>双向拷贝及拖放</strong></p><ol><li>完成系统安装后，右键点击虚拟机，选择依次点击<code>设置-&gt;常规-&gt;高级</code>，将<code>共享粘贴板</code>及<code>拖放</code>都改为==双向==</li><li>启动虚拟机，在新窗口的菜单中选择<code>设备-&gt;安装增强功能</code>，然后到系统中找到CD驱动器挂载的安装程序，双击该驱动安装程序，具体操作看下图</li></ol><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082035805.png" alt="image-20210309172743941"></p><p><strong>共享文件夹</strong></p><p>通过文件共享的方式，将主机的某个文件夹共享给虚拟机</p><ol><li>在虚拟机窗口中菜单，选择<code>设备-&gt;共享文件夹-&gt;共享文件夹</code></li><li>双击<code>固定分配</code>下的选项，选择主机的<code>共享文件夹路径</code>，设置共享文件夹名称，设置<code>只读</code>与<code>自动挂载</code>，最后点击OK</li><li>主机的共享文件夹被挂载到文件系统，打开我的电脑即可访问</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;由于工作需要，在windows10工作机编译的代码需要在干净的windows中测试代码运行效果，所以本文介绍如何使用oracle VM vitualbox安装windows10系统，已经配置主机与虚拟机之间的文件传输过程&lt;/p&gt;</summary>
    
    
    
    <category term="软件工具" scheme="https://shaogui.life/categories/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="windows10" scheme="https://shaogui.life/tags/windows10/"/>
    
    <category term="虚拟机" scheme="https://shaogui.life/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>在Windows上编译带CUDA(GPU)的OpenCV</title>
    <link href="https://shaogui.life/2021/03/08/%E5%9C%A8Windows%E4%B8%8A%E7%BC%96%E8%AF%91%E5%B8%A6CUDA(GPU)%E7%9A%84opencv/"/>
    <id>https://shaogui.life/2021/03/08/%E5%9C%A8Windows%E4%B8%8A%E7%BC%96%E8%AF%91%E5%B8%A6CUDA(GPU)%E7%9A%84opencv/</id>
    <published>2021-03-08T13:00:44.000Z</published>
    <updated>2023-04-08T12:42:32.723Z</updated>
    
    <content type="html"><![CDATA[<p>本文一步一步地介绍如何在windows上编译带CUDA模块（GPU）支持的OpenCV，为避免长篇大论，截图过多，尽可能简单地描述</p><a id="more"></a><p><strong>本次安装说明：</strong></p><p>本次在windows10上、RTX2060S编译OpenCV4.5.0，其他相关软件安装情况如下：</p><ul><li>NVIDIA Diver 461</li><li>CUDA 11.1</li><li>CUDNN 8.0.4</li><li>Cmake 3.19.3</li><li>Visual studio 2017</li></ul><h2 id="第一步：安装前准备"><a href="#第一步：安装前准备" class="headerlink" title="第一步：安装前准备"></a>第一步：安装前准备</h2><ol><li>确认系统显卡牌子为：NVIDIA，并<a href="https://developer.nvidia.com/zh-cn/cuda-gpus">在此</a>查看是否支持CUDA ，在列表中找到即是支持显卡</li><li><a href="https://www.nvidia.cn/geforce/drivers/">下载</a>并安装显卡驱动</li><li><a href="https://developer.nvidia.com/zh-cn/cuda-toolkit">下载</a>并安装CUDA，<a href="https://developer.nvidia.com/zh-cn/cudnn">下载</a>CUDNN，安装过程参考<a href=""></a></li><li><a href="https://cmake.org/download/">下载</a>并安装CmakeGUI</li><li><a href="https://visualstudio.microsoft.com/downloads/">下载</a>并安装VIsual sudio 2017</li><li><a href="https://opencv.org/releases/">下载</a>OpenCV</li><li><a href="https://github.com/opencv/opencv_contrib/">下载</a>OpenCV contrib，需跟OpenCV同版本</li><li>解压OpenCV及OpenCV contrib</li><li>在OpenCV解压目录下创建build</li></ol><h2 id="第二步：使用Cmake-gui构建opencv"><a href="#第二步：使用Cmake-gui构建opencv" class="headerlink" title="第二步：使用Cmake-gui构建opencv"></a>第二步：使用Cmake-gui构建opencv</h2><ol><li><strong>启动Cmake：</strong>为编译OpenCV的Python接口，需在在终端激活某个conda虚拟环境，并运行<code>cmake-gui</code>程序，如不需要使用Python接口，直接运行<code>cmake-gui</code>即可</li><li><strong>配置Cmake：</strong>（1）<code>source code</code>选择OpenCV解压后的源码；（2）<code>build binaries</code>选择第一步创建的build目录；（3）点击<code>Configure</code>，在弹窗中依次选<code>VIsual sudio 15 2017</code> 、 <code>x64</code>，点击<code>Finish</code></li><li><strong>配置CUDA模块：</strong>Cmake完成初始编译编译后，在出现的红色新选项中勾选<code>WITH_CUDA</code>，<code>OPENCV_DNN_CUDA</code>，<code>ENABLE_FAST_MATH</code></li><li><strong>配置OpenCV contrib：</strong>在出现的红色新选项中找到<code>OPENCV_EXTRA_MODULES_PATH</code>，并将Value配置为第一步解压的OpenCV contrib目录下的<code>modules</code>目录路径</li><li><strong>配置OpenCV的Python接口</strong>：（1）点击<code>Add Entry</code>新增BUILD_opencv_python3和BUILD_opencv_python2两项，其中BUILD_opencv_python3设置value为true，另一个设置为false；（2）搜索并配置<code>PYTHON3_EXECUTABLE</code>、<code>PYTHON3_INCLUDE_DIR</code>、<code>PYTHON3_LIBRARY</code>、<code>PYTHON3_NUMPY_INCLUDE_DIRS</code>、<code>PYTHON3_PACKAGES_PATH</code>，配置结果看下图<sup><a href="#fn_4" id="reffn_4">4</a></sup></li><li><strong>其他配置项：</strong>搜索并勾选<code>OPENCV_ENABLE_NONFREE</code>，<code>build_opencv_world</code></li><li>再次点击<code>COnfigure</code>，在新增红色配置项中找到<code>CUDA_ARCH_BIN</code>，删除小于显卡计算能力的数值，计算能力可以在<a href="https://developer.nvidia.com/zh-cn/cuda-gpus"></a>找到。</li><li>再次点击<code>COnfigure</code>，确认日志中出现<em>Configuring done</em>后，点击<code>Generate</code></li></ol><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082042170.png" alt="image-20210306160653041"></p><h2 id="第三步：使用Visual-studio编译opencv"><a href="#第三步：使用Visual-studio编译opencv" class="headerlink" title="第三步：使用Visual studio编译opencv"></a>第三步：使用Visual studio编译opencv</h2><ol><li>点击<code>Open Project</code>或者使用<code>Visual studio</code>打开<strong>OpenCV.sh</strong></li><li>打开后，更改<strong>Debug</strong>模式为<strong>Release</strong>模式</li><li>右键点击<code>Cmake Targets</code>下的<code>ALL_BUILD</code>，并点击<code>build</code>，此步编译文件需要较长时间</li><li>一旦完成，右键点击<code>Install</code>，并点击<code>build</code></li></ol><p>到此，已经成功在windows10上编译了带CUDA模块的OpenCV，编译得到的文件位于<code>build\install</code>下</p><h2 id="第四步：Python及C-使用"><a href="#第四步：Python及C-使用" class="headerlink" title="第四步：Python及C++使用"></a>第四步：Python及C++使用</h2><p><strong>Python端使用</strong></p><ol><li>安装OpenCV到Python，</li><li>新建Python文件，写入以下内容，并在终端运行，如果无误则运行成功</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看opencv信息</span></span><br><span class="line">print(cv2.getBuildInformation())</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取图片</span></span><br><span class="line">frame=cv2.imread(<span class="string">&#x27;test.jpg&#x27;</span>)</span><br><span class="line">print(frame.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#上传图片到GPU</span></span><br><span class="line">gpu_frame=cv2.cuda_GpuMat()</span><br><span class="line">gpu_frame.upload(frame)</span><br><span class="line"></span><br><span class="line"><span class="comment">#resize</span></span><br><span class="line">gpu_resframe=cv2.cuda.resize(gpu_frame,(<span class="number">1024</span>,<span class="number">512</span>))</span><br><span class="line">cpu_resfram=gpu_resframe.download()</span><br><span class="line">print(cpu_resfram.shape)</span><br></pre></td></tr></table></figure><p><strong>C++端使用</strong></p><ol><li>配置系统环境，将编译目录下的<code>install\x64\vc15\bin</code>完整路径配置到系统路径下，==并重启系统==</li><li>打开visual studio，新建<code>Visual C++</code>空项目，新建文件<code>main.cpp</code>，写入下面内容</li><li>点击<code>生成-&gt;生成解决方案</code>，不出现错误后，运行程序，无错误即可使用OpenCV的cuda模块</li><li><p>配置OpenCV开发环境，依次点击<code>视图-&gt;其他窗口-&gt;属性窗口</code>，右键点击<code>Release | x64</code>，新建项目属性表，新建完成后双击项目该文件，进行以下配置。</p><ul><li><strong>VC++目录-&gt;包含目录</strong>：E:\opencv-4.5.0\build_test\install\include;E:\opencv-4.5.0\build_test\install\include\opencv2;$(IncludePath)</li><li><strong>VC++目录-&gt;库目录：</strong>E:\opencv-4.5.0\build_test\install\x64\vc15\lib;$(LibraryPath)</li><li><strong>链接器-&gt;输入：</strong>opencv_world450.dll;%(AdditionalDependencies)</li></ul></li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sstream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/dnn.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/dnn/all_layers.hpp&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/imgproc.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/highgui.hpp&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">//读取图片</span></span><br><span class="line"><span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span> image_path = <span class="string">&quot;C:\\Users\\wushaogui\\Desktop\\CCP_REPOS\\SegementModule\\SegementModule\\test.jpg&quot;</span>;</span><br><span class="line">cv::Mat rgb_image = cv::imread(image_path, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">cv::cuda::GpuMat gpu_frame, gpu_resized;</span><br><span class="line"><span class="comment">//上传图片到GPU</span></span><br><span class="line">gpu_frame.upload(rgb_image);</span><br><span class="line"></span><br><span class="line"><span class="comment">//Resize</span></span><br><span class="line"><span class="keyword">auto</span> input_size = cv::Size(<span class="number">1024</span>,<span class="number">512</span>);</span><br><span class="line">cv::cuda::resize(gpu_frame, resized, input_size, <span class="number">0</span>, <span class="number">0</span>, cv::INTER_NEAREST);</span><br><span class="line"></span><br><span class="line"><span class="comment">//下载并保存图片</span></span><br><span class="line">cv::Mat cpu_resized;</span><br><span class="line">gpu_resized.download(cpu_resized);</span><br><span class="line">cv::imwrite(<span class="string">&quot;resized_test.jpg&quot;</span>, cpu_resized);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="第四步：解决问题及其他需了解信息"><a href="#第四步：解决问题及其他需了解信息" class="headerlink" title="第四步：解决问题及其他需了解信息"></a>第四步：解决问题及其他需了解信息</h2><h3 id="资源无法下载2"><a href="#资源无法下载2" class="headerlink" title="资源无法下载2"></a>资源无法下载<sup><a href="#fn_2" id="reffn_2">2</a></sup></h3><p>在此次编译中，无法下载的资源有：ADE、face_landmark_model.dat、ffmpeg、ippicv、nvidia_optical_flow、xfeatures2d</p><p><strong>解决方法：</strong>网络下载，这些文件，计算文件md5进行文件重命名后，将文件放置在编译目录下的<code>.cache</code>目录下，此次编译放置情况如下：</p><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082042047.png" alt="image-20210306150042380"></p><p><strong>参考资料<sup><a href="#fn_1" id="reffn_1">1</a></sup><sup><a href="#fn_3" id="reffn_3">3</a></sup>：</strong></p><blockquote id="fn_1"><sup>1</sup>. <a href="https://haroonshakeel.medium.com/build-opencv-4-4-0-with-cuda-gpu-support-on-windows-10-without-tears-aa85d470bcd0">Build OpenCV 4.4.0 with CUDA (GPU) Support on Windows 10 (Without Tears) | by M. Haroon Shakeel | Medium</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. <a href="https://blog.csdn.net/fengxinzioo/article/details/104919888">windows安装opencv4.1.1过程中ffmpeg、ippicv、face_landmark_model下载出错解决办法</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. <a href="https://blog.csdn.net/hitpisces/article/details/104266030">Win10下编译同时支持CUDA以及Python3的OpenCV 4.2教程_吃白兔的小青菜的博客-CSDN博客</a><a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><blockquote id="fn_4"><sup>4</sup>. <a href="https://blog.csdn.net/qq_37781464/article/details/110078370">win10使用vs2019从源码编译OpenCV4.5+cuda10.2+cudnn8.0的C++环境和Python环境</a><a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文一步一步地介绍如何在windows上编译带CUDA模块（GPU）支持的OpenCV，为避免长篇大论，截图过多，尽可能简单地描述&lt;/p&gt;</summary>
    
    
    
    <category term="软件工具" scheme="https://shaogui.life/categories/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="cuda" scheme="https://shaogui.life/tags/cuda/"/>
    
    <category term="windows" scheme="https://shaogui.life/tags/windows/"/>
    
    <category term="opencv" scheme="https://shaogui.life/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>Hexo操作命令备忘录</title>
    <link href="https://shaogui.life/2021/03/06/Hexo%E6%93%8D%E4%BD%9C%E5%A4%87%E5%BF%98%E5%BD%95/"/>
    <id>https://shaogui.life/2021/03/06/Hexo%E6%93%8D%E4%BD%9C%E5%A4%87%E5%BF%98%E5%BD%95/</id>
    <published>2021-03-06T14:39:44.000Z</published>
    <updated>2022-06-15T01:53:15.170Z</updated>
    
    <content type="html"><![CDATA[<p>本文用于记录hexo常见的操作步骤，即使温习回顾</p><a id="more"></a><h2 id="Hexo常见命令1"><a href="#Hexo常见命令1" class="headerlink" title="Hexo常见命令1"></a>Hexo常见命令<sup><a href="#fn_1" id="reffn_1">1</a></sup></h2><h3 id="新建文章"><a href="#新建文章" class="headerlink" title="新建文章"></a>新建文章</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo new &lt;title&gt;</span><br><span class="line">hexo new <span class="string">&quot;post title with whitespace&quot;</span></span><br></pre></td></tr></table></figure><p><strong>参数：</strong></p><ul><li>-p,—path       自定义新文章路径</li><li>-r,—replace   如果存在同名文件，将其覆盖</li><li>-s,—slug        文章Slug，作为新文章的文件名和发布后的 URL</li></ul><p>注意：文件明包含空格时，需要用双引号括起来</p><h3 id="新建草稿"><a href="#新建草稿" class="headerlink" title="新建草稿"></a>新建草稿</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo new draft &lt;title&gt;</span><br><span class="line">hexo new draft <span class="string">&quot;draft title with whitespace&quot;</span></span><br></pre></td></tr></table></figure><h3 id="发布草稿"><a href="#发布草稿" class="headerlink" title="发布草稿"></a>发布草稿</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo publish &lt;title&gt;</span><br><span class="line">hexo publish <span class="string">&quot;draft title with whitespace&quot;</span></span><br></pre></td></tr></table></figure><p>注意：title不带文件后缀<code>.md</code></p><h3 id="新建页面"><a href="#新建页面" class="headerlink" title="新建页面"></a>新建页面</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page --path about/me <span class="string">&quot;About me&quot;</span></span><br></pre></td></tr></table></figure><blockquote><p>注意：以上命令会创建一个 <code>source/about/me.md</code> 文件，同时 Front Matter 中的 title 为 <code>&quot;About me&quot;</code></p></blockquote><h3 id="生成博客"><a href="#生成博客" class="headerlink" title="生成博客"></a>生成博客</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo generate</span><br><span class="line">hexo g</span><br></pre></td></tr></table></figure><p><strong>参数：</strong></p><ul><li>-d,—deploy            文件生成后立即部署        </li><li>-w,—watch             监视文件改动</li><li>-b,-bail                   生成过程中如果发生任何未处理的异常则抛出异常</li><li>-f,—force                强制重新生成文件</li><li>-c,—concurrency  最大同时生成文件的数量，默认无限制</li></ul><h3 id="本地浏览"><a href="#本地浏览" class="headerlink" title="本地浏览"></a>本地浏览</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo server</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure><p><strong>参数：</strong></p><ul><li>-p,—port            重设端口        </li><li>-s,—static          只使用静态文件</li><li>-l,—log               启动日记记录，使用覆盖记录格式</li></ul><h2 id="远程部署"><a href="#远程部署" class="headerlink" title="远程部署"></a>远程部署</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo deploy</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><p><strong>参数：</strong></p><ul><li>-g,—generate   部署之前预先生成静态文件</li></ul><h3 id="清除缓存"><a href="#清除缓存" class="headerlink" title="清除缓存"></a>清除缓存</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br></pre></td></tr></table></figure><blockquote><p>清除缓存文件 (<code>db.json</code>) 和已生成的静态文件 (<code>public</code>)</p><p>在某些情况（尤其是更换主题后），如果发现您对站点的更改无论如何也不生效，您可能需要运行该命令</p></blockquote><h3 id="查看版本"><a href="#查看版本" class="headerlink" title="查看版本"></a>查看版本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure><h2 id="Hexo的YAML-Front-Matter2"><a href="#Hexo的YAML-Front-Matter2" class="headerlink" title="Hexo的YAML Front Matter2"></a>Hexo的YAML Front Matter<sup><a href="#fn_2" id="reffn_2">2</a></sup></h2><p>文章参数列举如下：</p><div class="table-container"><table><thead><tr><th>参数</th><th>描述</th><th>取值</th></tr></thead><tbody><tr><td>title</td><td>标题名</td><td></td></tr><tr><td>date</td><td>创建日期</td><td></td></tr><tr><td>updated</td><td>更新日期</td><td></td></tr><tr><td>tags</td><td>标签</td><td>- 标签1<br />- 标签2<br />- 标签3</td></tr><tr><td>categories</td><td>分类</td><td>- 分类<br />- 子分类<br />- 子子分类</td></tr><tr><td>copyright</td><td>是否添加版权声明</td><td>true</td></tr><tr><td>top</td><td>是否置顶</td><td>true/空</td></tr><tr><td>description</td><td>文章描述</td><td>空</td></tr><tr><td>mathjax:</td><td>是否添加latex公式支持</td><td>true</td></tr></tbody></table></div><p><strong>参考资料：</strong></p><blockquote id="fn_1"><sup>1</sup>. <a href="https://hexo.io/zh-cn/docs/commands">指令 | Hexo</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. <a href="https://blog.csdn.net/nineya_com/article/details/103316683">Hexo博客发表文章、草稿、添加分类和标签_玖涯博客-CSDN博客</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文用于记录hexo常见的操作步骤，即使温习回顾&lt;/p&gt;</summary>
    
    
    
    <category term="软件工具" scheme="https://shaogui.life/categories/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="Hexo" scheme="https://shaogui.life/tags/Hexo/"/>
    
    <category term="Next" scheme="https://shaogui.life/tags/Next/"/>
    
    <category term="博客" scheme="https://shaogui.life/tags/%E5%8D%9A%E5%AE%A2/"/>
    
  </entry>
  
  <entry>
    <title>如何使用tensorflowServing进行模型部署</title>
    <link href="https://shaogui.life/2021/02/22/%E5%A6%82%E4%BD%95%E7%94%A8tensorflowServing%E8%BF%9B%E8%A1%8C%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"/>
    <id>https://shaogui.life/2021/02/22/%E5%A6%82%E4%BD%95%E7%94%A8tensorflowServing%E8%BF%9B%E8%A1%8C%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/</id>
    <published>2021-02-22T10:30:44.000Z</published>
    <updated>2023-04-08T13:09:10.651Z</updated>
    
    <content type="html"><![CDATA[<p>TensorFlow Serving是用于机器学习的灵活，高性能的服务系统，针对生产环境而设计。 TensorFlow服务 可以轻松部署新算法和实验，同时保持不变服务器体系结构和API。TensorFlow Serving开箱即用 与TensorFlow模型集成，但可以轻松扩展以服务于其他  模型类型</p><a id="more"></a><h1 id="关键概念"><a href="#关键概念" class="headerlink" title="关键概念"></a>关键概念</h1><h2 id="Key-Conception"><a href="#Key-Conception" class="headerlink" title="Key Conception"></a>Key Conception</h2><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082031214.svg" alt="serving_architecture"></p><h3 id="Loaders"><a href="#Loaders" class="headerlink" title="Loaders"></a>Loaders</h3><p> Loaders管理Servables的生命周期。Loader API 是一种支持独立于特定算法，数据或产品用例的通用基础架构。具体来说，Loaders标准化了用于加载和卸载Servable的API。</p><h3 id="Sources"><a href="#Sources" class="headerlink" title="Sources"></a>Sources</h3><p> Sources 是可以寻找和提供 Servables 的模块，每个 Source 提供了0个或者多个Servable streams，对于每个Servable stream，Source 都会提供一个Loader实例。</p><h3 id="Managers"><a href="#Managers" class="headerlink" title="Managers"></a>Managers</h3><p> 管理 Servable 的整个的生命周期，包括：</p><ul><li>loading Servables</li><li>serving Servables</li><li>unloading Servables</li></ul><p> Managers监听Sources并跟踪所有版本。Managers尝试满足、响应Sources的请求，但是如果所请求的资源不可用，可能会拒绝加载相应版本。Managers也可以推迟“卸载”。例如，Managers可能会等待到较新的版本完成加载之后再卸载（基于保证始终至少加载一个版本的策略）。</p><h3 id="Servables"><a href="#Servables" class="headerlink" title="Servables"></a>Servables</h3><p> Servable是Tensorflow Serving的核心抽象，是客户端用于执行计算的基础对象,其大小和粒度是灵活的。Tensorflow  serving可以在单个实例的生命周期内处理一个或多个版本的Servable，这样既可以随时加载新的算法配置，权重或其他数据；也能够同时加载多个版本的Servable，支持逐步发布和实验。由此产生另外一个概念：Servable stream，即是指Servable的版本序列，按版本号递增排序。Tensorflow Serving 将 model  表示为一个或者多个Servables，一个Servable可能对应着模型的一部分，例如，a large lookup table 可以被许多  Tensorflow Serving 共享。另外，Servable不管理自己的生命周期</p><h3 id="Core"><a href="#Core" class="headerlink" title="Core"></a>Core</h3><p> Tensorflow Serving core 负责管理Servables的Lifecycle和metrics，将Servables和loaders看作黑箱(opaque objects)。</p><p>广义地说： </p><ol><li>Sources create Loaders for Servable Versions.</li><li>Loaders are sent as Aspired Versions to the Manager, which loads and serves them to client requests.</li></ol><p>例子： </p><ol><li>Source 为指定的服务(磁盘中检测模型权重的新版本)创建Loader，Loader里包含了服务所需要的元数据（模型）；</li><li>Source 使用回调函数通知 Manager 的 Aspired Version(Servable version的集合);</li><li>Manager 根据配置的Version Policy决定下一步的操作（是否 unload 之前的Servable，或者 load 新的Servable）；</li><li>如果 Manager 判定是操作安全的，就会给 Loader 要求的resource并让 Loader 加载新的版本;</li><li>客户端向 Manager 请求服务，可以指定服务版本或者只是请求最新的版本。Manager 返回服务端的处理结果;</li></ol><h2 id="Extensibility"><a href="#Extensibility" class="headerlink" title="Extensibility"></a>Extensibility</h2><p>Tensorflow Serving提供了几个可扩展的entry point，用户可以在其中添加自定义功能。</p><h3 id="Version-Policy"><a href="#Version-Policy" class="headerlink" title="Version Policy"></a>Version Policy</h3><p> Version Policy(版本策略)可以指定单个Servable stream中的版本加载和卸载顺序。它包括Availability Preserving Policy（在卸载旧版本之前加载并准备好新版本）和Resource Preserving Policy（在加载新版本之前先卸载旧版本）。</p><h3 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h3><p> New Sources可以支持新的文件系统，云产品和算法后端，这主要和创建自定义Source有关。</p><h3 id="Loaders-1"><a href="#Loaders-1" class="headerlink" title="Loaders"></a>Loaders</h3><p> Loaers是添加算法、数据后端的扩展点。Tensorflow就是这样一种算法后端。例如，用户将实现一个新的Loader，以便对新的Servable机器学习模型实例的访问和卸载。</p><h3 id="Batcher"><a href="#Batcher" class="headerlink" title="Batcher"></a>Batcher</h3><p> 将多个请求批处理为单个请求可以显着降低计算成本，尤其是在存在诸如GPU的硬件加速器的情况下。Tensorflow Serving包括一个请求批处理小部件，它允许客户端轻松地将请求中特定类型的计算进行批量处理。</p><h1 id="系统环境搭建"><a href="#系统环境搭建" class="headerlink" title="系统环境搭建"></a>系统环境搭建</h1><h2 id="系统及软硬件说明"><a href="#系统及软硬件说明" class="headerlink" title="系统及软硬件说明"></a>系统及软硬件说明</h2><p><strong>系统</strong>：Ubuntu16.04</p><p><strong>软件</strong></p><ul><li>驱动 450.23.05</li><li>cuda 11.1</li><li>cudnn 8.0.5</li><li>tensorflow nightly-gpu(2.4)</li><li>python 3.7.9</li></ul><p>硬件</p><ul><li>RTX 3090</li></ul><h2 id="导出Keras模型"><a href="#导出Keras模型" class="headerlink" title="导出Keras模型"></a>导出Keras模型</h2><p>将keras中以<code>model.save(filepath)</code>保存的模型h5文件，转为tensorflow的xx格式，加载模型时，使用:heavy_check_mark:<code>tf.keras.models.load_model</code>而不是:x:<code>keras.models.load_model</code>，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先使用tf.keras的load_model来导入模型h5文件</span></span><br><span class="line">model_path = <span class="string">&#x27;v3_resnet50_unet.h5&#x27;</span></span><br><span class="line">model = tf.keras.models.load_model(model_path, custom_objects=dependencies)</span><br><span class="line">model.save(<span class="string">&#x27;deploy/tfs/0&#x27;</span>, save_format=<span class="string">&#x27;tf&#x27;</span>)  <span class="comment"># 导出tf格式的模型文件</span></span><br></pre></td></tr></table></figure><p>导出之后，有以下目录结构</p><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082032836.png" alt="image-20210205165552193"></p><p>导出之后，使用以下命令查看模型的signature、input、output，后续客户端调用需要这些信息。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saved_model_cli show --dir tfs/0/ --all</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">signature_def[&#39;serving_default&#39;]:</span><br><span class="line">  The given SavedModel SignatureDef contains the following input(s):</span><br><span class="line">    inputs[&#39;input_1&#39;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 512, 1024, 3)</span><br><span class="line">        name: serving_default_input_1:0</span><br><span class="line">  The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">    outputs[&#39;activation_49&#39;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 524288, 1)</span><br><span class="line">        name: StatefulPartitionedCall:0</span><br><span class="line">  Method name is: tensorflow&#x2F;serving&#x2F;predict</span><br></pre></td></tr></table></figure><p>以上可以确定，signature、input、output分别为：serving_default，input_1，activation_49</p><p><strong>saved_model_cli</strong>为tensorflow的python工具包，位于<code>tensorflow/python/tools/saved_model_cli.py</code>下，一般安装了tensorflow，可以直接找到该命令。</p><h2 id="Docker部署模型"><a href="#Docker部署模型" class="headerlink" title="Docker部署模型"></a>Docker部署模型</h2><h3 id="拉取tfs的docker镜像"><a href="#拉取tfs的docker镜像" class="headerlink" title="拉取tfs的docker镜像"></a>拉取tfs的docker镜像</h3><p><strong>1.安装docker</strong></p><p>安装过程参考官方安装文档<a href="https://docs.docker.com/engine/install/ubuntu/">Install Docker Engine on Ubuntu</a></p><p><strong>2.安装nvidia-docker</strong></p><p>在docker上安装nvidia插件，以便使得应用在GPU上运行，安装过程参考：<a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker">Installation Guide</a></p><p><strong>3.拉取tfs镜像</strong></p><p><a href="https://hub.docker.com/r/tensorflow/serving/tags/?page=1&amp;ordering=last_updated">docker官网</a>上包含不同版本的tfs镜像，根据需求需要版本，使用以下命令拉取tfs镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker pull tensorflow/serving:nightly-gpu</span><br></pre></td></tr></table></figure><h3 id="启动tfs容器"><a href="#启动tfs容器" class="headerlink" title="启动tfs容器"></a>启动tfs容器</h3><p>使用以下命令启动tfs容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo nvidia-docker run -p 8500:8500  \</span><br><span class="line">  -v <span class="string">&quot;[path]/tfs:/models/resnet50_unet&quot;</span> \</span><br><span class="line">  -e MODEL_NAME=resnet50_unet \</span><br><span class="line">  -e CUDA_VISIBLE_DEVICES=1 \</span><br><span class="line">  -t 9e73a1470b72&amp;</span><br></pre></td></tr></table></figure><blockquote><p>其中9e73a1470b72为tfs镜像的id，可通过docker image ls查看</p></blockquote><p>tfs容器可用参数解释：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">--port &#x3D; 8500                     用于侦听gRPC API的端口</span><br><span class="line">--rest_api_port &#x3D; 0               用于侦听HTTP &#x2F; REST API的端口。如果设置为零，将不会导出HTTP </span><br><span class="line">                                    &#x2F; REST API。此端口必须与--port中指定的端口不同。</span><br><span class="line">--rest_api_num_threads &#x3D; 160      用于HTTP &#x2F; REST API处理的线程数。如果未设置，</span><br><span class="line">                                    将根据CPU数量自动设置。</span><br><span class="line">--rest_api_timeout_in_ms &#x3D; 30000  HTTP &#x2F; REST API调用超时。</span><br><span class="line">--enable_batching &#x3D; false bool    启用批处理</span><br><span class="line">--batching_parameters_file &#x3D;“”    字符串如果非空，请从提供的文件名读取ascii BatchingParameters </span><br><span class="line">                                    protobuf，并使用包含的值代替默认值。</span><br><span class="line">--model_config_file &#x3D;“”           字符串如果非空，请从提供的文件名读取ascii ModelServerConfig</span><br><span class="line">                                    协议，然后在该文件中提供模型。此配置文件可用于指定要使用的</span><br><span class="line">                                    多个模型以及其他高级参数，包括非默认版本策略。 </span><br><span class="line">                                   （如果使用了--model_name和--model_base_path，则将被忽略。）</span><br><span class="line">--model_name &#x3D;“ default”          模型的字符串名称（如果设置了--model_config_file标志，则忽略</span><br><span class="line">--model_base_path &#x3D;“”             导出的字符串路径（如果设置了--model_config_file标志，</span><br><span class="line">                                    则忽略该字符串，否则为必需）</span><br><span class="line">--file_system_poll_wait_seconds &#x3D; 1 以秒为单位的两次新模型版本的文件系统每次轮询之间的间隔</span><br><span class="line">--flush_filesystem_caches&#x3D;true bool 如果为true（默认值），则在所有可服务对象的初始加载之后以及</span><br><span class="line">                                      随后的每个可服务对象重新加载之后（如果加载线程数为1），</span><br><span class="line">                                      将刷新文件系统缓存。如果在加载可服务对象之后访问模型文件，</span><br><span class="line">                                      则可以减少模型服务器的内存消耗，并以潜在的高速缓存未命中为                                       代价。</span><br><span class="line">--tensorflow_session_parallelism&#x3D;0  用于运行Tensorflow会话的线程数。默认情况下自动配置。请注意，</span><br><span class="line">                                      如果--platform_config_file为非空，则将忽略此选项。</span><br><span class="line">--ssl_config_file &#x3D;“”               字符串如果非空，请从提供的文件名读取ascii SSLConfig协议</span><br><span class="line">                                      并设置安全的gRPC通道</span><br><span class="line">--platform_config_file &#x3D;“”         字符串如果非空，请从提供的文件名读取ascii PlatformConfigMap </span><br><span class="line">                                      protobuf，然后使用该平台配置而不是Tensorflow平台。 </span><br><span class="line">                                      （如果使用，则--enable_batching将被忽略。）</span><br><span class="line">--per_process_gpu_memory_fraction&#x3D;0.00 float每个进程占用GPU内存空间的分数，</span><br><span class="line">                                         该值介于0.0和1.0之间（默认值为0.0）。如果为1.0，则服务</span><br><span class="line">                                         器将在服务器启动时分配所有内存;如果为0.0，</span><br><span class="line">                                         则Tensorflow将自动选择一个值。</span><br><span class="line">--saved_model_tags &#x3D;“ serve”        字符串对应于要从SavedModel加载的元图def的逗号分隔的标记集。</span><br><span class="line">--grpc_channel_arguments &#x3D;“”        字符串要传递给grpc服务器的参数的逗号分隔列表。 </span><br><span class="line">                                     （例如grpc.max_connection_age_ms &#x3D; 2000）</span><br><span class="line">--enable_model_warmup &#x3D; true bool   启用模型预热，该预热在加载时触发延迟初始化（例如TF优化），</span><br><span class="line">                                      以减少第一个请求的延迟。</span><br><span class="line">--version &#x3D; false bool              显示版本</span><br></pre></td></tr></table></figure><p><strong>客户端的编写</strong></p><p>基于Python编写客户端时，需要安装tensorflow_serving、grpc库包</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> grpc</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> ops</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow_serving.apis <span class="keyword">import</span> predict_pb2, prediction_service_pb2_grpc</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">request_server_grpc</span>(<span class="params">img_resized, server_url</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    用于向TensorFlow Serving服务请求推理结果的函数。</span></span><br><span class="line"><span class="string">    :param img_resized: 经过预处理的待推理图片数组，numpy array，shape：(h, w, 3)</span></span><br><span class="line"><span class="string">    :param server_url: TensorFlow Serving的地址加端口，str，如：&#x27;0.0.0.0:8500&#x27; </span></span><br><span class="line"><span class="string">    :return: 模型返回的结果数组，numpy array</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># Request.</span></span><br><span class="line">    channel = grpc.insecure_channel(server_url)</span><br><span class="line">    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)</span><br><span class="line">    request = predict_pb2.PredictRequest()</span><br><span class="line">    request.model_spec.name = <span class="string">&quot;resnet50_unet&quot;</span>  <span class="comment"># 模型名称，启动容器命令的model_name参数</span></span><br><span class="line">    request.model_spec.signature_name = <span class="string">&quot;serving_default&quot;</span>  <span class="comment"># 签名名称，刚才叫你记下来的</span></span><br><span class="line">    <span class="comment"># &quot;input_1&quot;是你导出模型时设置的输入名称，刚才叫你记下来的</span></span><br><span class="line">    request.inputs[<span class="string">&quot;input_1&quot;</span>].CopyFrom(</span><br><span class="line">        tf.make_tensor_proto(img_resized, shape=[<span class="number">1</span>, ] + <span class="built_in">list</span>(img_resized.shape)))</span><br><span class="line">    <span class="comment"># print(request)</span></span><br><span class="line">    </span><br><span class="line">    response = stub.Predict(request, <span class="number">5.0</span>)  <span class="comment"># 5 secs timeout</span></span><br><span class="line">    <span class="keyword">return</span> np.asarray(response.outputs[<span class="string">&quot;activation_49&quot;</span>].float_val) <span class="comment"># fc2为输出名称，刚才叫你记下来的</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    img=cv2.imread([imgpath_xx],cv2.COLOR_BGR2RGB)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#预处理</span></span><br><span class="line">    res_image=cv2.resize(img,(<span class="number">1024</span>,<span class="number">512</span>))</span><br><span class="line">    res_image = res_image / <span class="number">255</span></span><br><span class="line">    res_image=res_image.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#向tfs发送请求</span></span><br><span class="line">    port=<span class="string">&#x27;8500&#x27;</span></span><br><span class="line">    server_url = <span class="string">r&#x27;0.0.0.0:&#x27;</span>+port</span><br><span class="line">    response=request_server_grpc(res_image, server_url) <span class="comment">#调用服务端</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#处理返回结果</span></span><br><span class="line">    predict=response.copy()</span><br><span class="line">    <span class="comment">#....后处理</span></span><br></pre></td></tr></table></figure><h3 id="Web服务"><a href="#Web服务" class="headerlink" title="Web服务"></a>Web服务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TensorFlow模型的计算图，一般输入的类型都是张量，你需要提前把你的图像、文本或者其它数据先进行预处理，转换成张量才能输入到模型当中。而一般来说，这个数据预处理过程不会写进计算图里面，因此当你想使用TensorFlow Serving的时候，需要在客户端上写一大堆数据预处理代码，然后把张量通过gRPC发送到serving，最后接收结果。现实情况是你不可能要求每一个用户都要写一大堆预处理和后处理代码，用户只需使用简单POST一个请求，然后接收最终结果即可。因此，这些预处理和后处理代码必须由一个“中间人”来处理，这个“中间人”就是Web服务。</span><br></pre></td></tr></table></figure><p>可以使用Tornado来搭建web服务</p><h3 id="版本管理"><a href="#版本管理" class="headerlink" title="版本管理"></a>版本管理</h3><p><em>待完善</em></p><h2 id="应用例子"><a href="#应用例子" class="headerlink" title="应用例子"></a>应用例子</h2><blockquote><p>例子来源于美团技术团队<a href="https://tech.meituan.com/2018/10/11/tfserving-improve.html">基于TensorFlow Serving的深度学习在线预估</a></p></blockquote><p>该例子针对广告精排的业务场景，使用tfs进行模型部署，针对高速的推断进行逐步的优化，并突破现有tfs的束缚，解决模型切换的毛刺问题，使得tfs部署后在性能上满足业务场景</p><p><strong>性能优化措施</strong></p><ul><li><p>请求端优化：使用OpenMP多线程并行处理请求，时间从5ms降低到2ms</p></li><li><p>构建模型的ops优化：分析构建模型的ops中的耗时操作，将其分离出去，或者使用低阶API替代高阶API</p></li><li><p>XLA,JIT优化：优化Tensorflow的计算图，剪除荣誉的计算</p></li></ul><p><strong>模型切换毛刺问题</strong></p><p>模型切换时，大量的请求超时，原因有两个：一是更新、加载模型和处理请求的线程共用线程池，切换模型时无法处理请求；二是模型采用Lazy Initialization加载，第一次请求需要等待计算图初始化。</p><p>问题一的解决办法：</p><p>将<code>uint32 num_load_threads = 0; uint32 num_unload_threads = 0;</code>设置为1，</p><p>问题2的解决办法：</p><p>模型加载后进行一次预热</p><p><strong>参考资料</strong></p><p><a href="://blog.csdn.net/jeffery0207/article/details/86072456">Tensorflow Serving部署tensorflow、keras模型详解_jeffery0207的博客-CSDN博客</a><br><a href="://zhuanlan.zhihu.com/p/52096200">TensorFlow Serving + Docker + Tornado机器学习模型生产级快速部署 - 知乎</a><br><a href="://tensorflow.google.cn/tfx/serving/signature_defs?hl=en">SignatureDefs in SavedModel for TensorFlow Serving  |  TFX</a><br><a href="://zhuanlan.zhihu.com/p/96917543">使用tensorflow serving部署keras模型（tensorflow 2.0.0） - 知乎</a><br><a href="://tech.meituan.com/2018/10/11/tfserving-improve.html">基于TensorFlow Serving的深度学习在线预估 - 美团技术团队</a><br><a href="://www.jianshu.com/p/afe80b2ed7f0">TensorFlow Serving入门 - 简书</a><br><a href="://blog.csdn.net/weixin_40922744/article/details/102872607">TensorFlow Serving 入门教程（Windows）_I’m George 的博客-CSDN博客</a><br><a href="://blog.csdn.net/leiflyy/article/details/110003671">TensorFlow Serving 使用 及 部署_Eric’s Blog-CSDN博客</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;TensorFlow Serving是用于机器学习的灵活，高性能的服务系统，针对生产环境而设计。 TensorFlow服务 可以轻松部署新算法和实验，同时保持不变服务器体系结构和API。TensorFlow Serving开箱即用 与TensorFlow模型集成，但可以轻松扩展以服务于其他  模型类型&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://shaogui.life/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="tensorflow" scheme="https://shaogui.life/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Linux上利用gitee为Hexo配置图床服务</title>
    <link href="https://shaogui.life/2021/02/21/Hexo%E5%9B%BE%E5%BA%8A%E6%90%AD%E5%BB%BA/"/>
    <id>https://shaogui.life/2021/02/21/Hexo%E5%9B%BE%E5%BA%8A%E6%90%AD%E5%BB%BA/</id>
    <published>2021-02-21T10:10:44.000Z</published>
    <updated>2023-04-08T12:45:08.307Z</updated>
    
    <content type="html"><![CDATA[<p>本文通过小书匠为hexo搭建图床服务，图床搭建在gitee中，国内访问速度可以</p><a id="more"></a><p>搭建的步骤主要分为两个步骤</p><h2 id="新建图床仓库并获取令牌"><a href="#新建图床仓库并获取令牌" class="headerlink" title="新建图床仓库并获取令牌"></a>新建图床仓库并获取令牌</h2><p><strong>新建仓库</strong></p><p>打开gitee，新建一个<code>公开的</code>仓库，用于做图床的存储库</p><p><strong>获得令牌</strong></p><p>令牌运行应用自行上传图片到gitee，所以需要提前获得，在gitee个人设置选项找到<code>私人令牌</code>，按下图配置新建一个令牌</p><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082044872.png" alt="image-20210221172720292"></p><h2 id="配置Markdown编辑器"><a href="#配置Markdown编辑器" class="headerlink" title="配置Markdown编辑器"></a>配置Markdown编辑器</h2><h3 id="配置小书匠1"><a href="#配置小书匠1" class="headerlink" title="配置小书匠1"></a>配置小书匠<sup><a href="#fn_1" id="reffn_1">1</a></sup></h3><p>打开小书匠的配置界面，选择绑定，添加一个<code>码云图床</code>，出现以下添加界面</p><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082044149.png" alt=""></p><h2 id="配置Typora2"><a href="#配置Typora2" class="headerlink" title="配置Typora2"></a>配置Typora<sup><a href="#fn_2" id="reffn_2">2</a></sup></h2><p>打开Typora偏好设置，依次配置下面内容：</p><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082044237.png" alt="image-20210221173149180"></p><p>第5步需要下载<code>PicGo-Core</code>，耐心等到下载完成；</p><p>第6步配置文件按以下配置</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;picBed&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;current&quot;</span>: <span class="string">&quot;gitee&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;uploader&quot;</span>: <span class="string">&quot;gitee&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;gitee&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;branch&quot;</span>: <span class="string">&quot;master&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;customPath&quot;</span>: <span class="string">&quot;yearMonth&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;customUrl&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;path&quot;</span>: <span class="string">&quot;PicGo/&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;repo&quot;</span>: <span class="string">&quot;xxxx&quot;</span>, <span class="comment">//用户名/仓库名称</span></span><br><span class="line">      <span class="attr">&quot;token&quot;</span>: <span class="string">&quot;xxxx&quot;</span> <span class="comment">// gitee上面的私人令牌</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;picgoPlugins&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;picgo-plugin-gitee-uploader&quot;</span>: <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>配置文件的<code>repo</code>在仓库的浏览地址找到</p><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082044729.png" alt="image-20210221174346189"></p><p>配置文件的<code>path</code>为仓库下的一个文件夹，<code>token</code>为第一步申请的</p><p>点击第7步前需要安装picgo的<code>gitee-uploader</code>插件，安装下面的命令安装<code>gitee-uploader</code>插件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> C:\Users\%用户名%\AppData\Roaming\Typora\picgo\win64</span><br><span class="line">picgo.exe install gitee-uploader</span><br><span class="line">picgo.exe install super-prefix</span><br></pre></td></tr></table></figure><p>点击第7步，出现下面提示，则配置成功</p><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082045314.png" alt="image-20210221175128485"></p><p>注意：在linux上已经试过用<code>PicGo-2.2.2.AppImage</code>配置<code>Typora</code>，但是出现一个难以忍受的bug，图片上传成功后居然不重命名，其实图片链接已经保存在粘贴板中，需要手动复制上去</p><p><strong>参考资料：</strong></p><blockquote id="fn_1"><sup>1</sup>. <a href="https://blog.csdn.net/qq_39231769/article/details/96709675">无水印上传本地图片到网络获取地址+小书匠使用 +github和 七牛云 图床绑定—菜鸟小回</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. <a href="https://blog.csdn.net/in_the_road/article/details/105733292">Typora自动上传图片配置，集成PicGo-Core，文件以时间戳命名</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文通过小书匠为hexo搭建图床服务，图床搭建在gitee中，国内访问速度可以&lt;/p&gt;</summary>
    
    
    
    <category term="软件工具" scheme="https://shaogui.life/categories/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="Hexo" scheme="https://shaogui.life/tags/Hexo/"/>
    
    <category term="Next" scheme="https://shaogui.life/tags/Next/"/>
    
    <category term="博客" scheme="https://shaogui.life/tags/%E5%8D%9A%E5%AE%A2/"/>
    
  </entry>
  
</feed>
