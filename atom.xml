<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>年轻人起来冲</title>
  
  
  <link href="https://shaogui.life/atom.xml" rel="self"/>
  
  <link href="https://shaogui.life/"/>
  <updated>2023-05-20T02:34:20.949Z</updated>
  <id>https://shaogui.life/</id>
  
  <author>
    <name>绍桂</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Segment Anything</title>
    <link href="https://shaogui.life/2023/05/20/Segment%20Anything/"/>
    <id>https://shaogui.life/2023/05/20/Segment%20Anything/</id>
    <published>2023-05-20T11:36:23.000Z</published>
    <updated>2023-05-20T02:34:20.949Z</updated>
    
    <content type="html"><![CDATA[<p>SAM通过transformer将点、框、Mask、文本等prompt和图片进行编码学习，可以实现对图片任意目标的分割</p><a id="more"></a><h1 id="什么是-SAM-？"><a href="#什么是-SAM-？" class="headerlink" title="什么是 SAM ？"></a>什么是 SAM ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAM-3.png" alt=""></li><li>a)SAM 利用“图片-分割提示”实现对图片上任意目标的分割，分割提示包括：点、框、Mask、文本</li><li>b) SAM 首先利用 prompt encoder 编码”分割提示”，利用 image encoder 编码“图片”，然后通过 Mask decoder 解析输出 Mask</li><li>c)SAM 利用数据驱动去做模型训练，模型输出结果后再输入模型训练</li></ul><h1 id="SAM-的网络结构？"><a href="#SAM-的网络结构？" class="headerlink" title="SAM 的网络结构？"></a>SAM 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAM.png" alt=""></li><li><strong>image encoder</strong>：类似 VIT 的过程，输入 image (1,3, H, W), 输出 image_embedding (1, C, H/16, W/16)，即 (1, HW/256, C)的 tokens 表示</li><li><strong>mask</strong>：mask prompt，直接和image_embedding相加即可</li><li><strong>prompt encoder</strong>：包含3种提示的编码过程，其中点、框按位置被编码为Pos embedding(1,N,C)，文本通过clip模型被编码为Pos embedding(1,M,C)</li><li><strong>mask decoder</strong>：根据image_embedding和prompt encoder输出，结合IOU tokens(1,1,C)和mask tokens(1,P,C)，解析出目标mask(1,1+P+N+M, H/16, W/16)和iou(1,1+P+N+M)</li></ul><h1 id="SAM-的-image-encoder？"><a href="#SAM-的-image-encoder？" class="headerlink" title="SAM 的 image encoder？"></a>SAM 的 image encoder？</h1><ul><li>类似 VIT 的 encoder 过程，输入 image (1,3, H, W), 输出 image_embedding (1, C, H/16, W/16)，即 (1, HW/256, C)的 tokens 表示</li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> image_encoder=ImageEncoderViT(..)</span><br><span class="line"> <span class="comment"># batched_input=&#123;List,List&#125; -&gt; torch.Size([2, 3, 1024, 1024])</span></span><br><span class="line"> input_images = torch.stack([preprocess(x[<span class="string">&quot;image&quot;</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> batched_input], dim=<span class="number">0</span>) </span><br><span class="line"><span class="comment"># torch.Size([2, 3, 1024, 1024]) -&gt; torch.Size([2, 256, 64, 64])</span></span><br><span class="line"> image_embeddings = image_encoder(input_images)</span><br></pre></td></tr></table></figure><h1 id="SAM-的-prompt-encoder"><a href="#SAM-的-prompt-encoder" class="headerlink" title="SAM 的 prompt encoder?"></a>SAM 的 prompt encoder?</h1></li><li><p>包含3种提示的编码过程，其中点、框按位置被编码为 Pos embedding (1, N, C)，文本通过 clip 模型被编码为 Pos embedding (1, M, C)，最终输出（1,N+M,C )的稀疏编码sparse_embeddings</p></li><li><strong>point&amp;box</strong>：每个点编码为1个 pos embedding，每个 box 编码为2个 pos embedding（box 被两个点定义）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"> embed_dim=<span class="number">256</span></span><br><span class="line"> num_point_embeddings: <span class="built_in">int</span> = <span class="number">4</span>  <span class="comment"># pos/neg point + 2 box corners</span></span><br><span class="line"> point_embeddings = [nn.Embedding(<span class="number">1</span>, embed_dim) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_point_embeddings)]</span><br><span class="line"> point_embeddings = nn.ModuleList(point_embeddings)</span><br><span class="line"> not_a_point_embed = nn.Embedding(<span class="number">1</span>, embed_dim)</span><br><span class="line"><span class="comment"># point prompt</span></span><br><span class="line">points = points + <span class="number">0.5</span>  <span class="comment"># Shift to center of pixel  </span></span><br><span class="line"><span class="comment"># 根据点位置points，在输入(1024,1024)的基础上生成pos embedding</span></span><br><span class="line"> point_embedding = pe_layer.forward_with_coords(points, input_image_size) <span class="comment">#torch.Size([1,3,2])+(1024,1024)-&gt;torch.Size([1,3,256])</span></span><br><span class="line"> <span class="comment"># 点有3类，-1表示非嵌入点，此时不使用pos embedding，0表示正样本点，1表示负样本点</span></span><br><span class="line">point_embedding[labels == -<span class="number">1</span>] = <span class="number">0.0</span></span><br><span class="line"> point_embedding[labels == -<span class="number">1</span>] += not_a_point_embed.weight</span><br><span class="line"> point_embedding[labels == <span class="number">0</span>] += point_embeddings[<span class="number">0</span>].weight</span><br><span class="line"> point_embedding[labels == <span class="number">1</span>] += point_embeddings[<span class="number">1</span>].weight</span><br><span class="line"><span class="comment"># box prompt</span></span><br><span class="line"> boxes = boxes + <span class="number">0.5</span>  <span class="comment"># Shift to center of pixel</span></span><br><span class="line"> coords = boxes.reshape(-<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>) <span class="comment"># 一个框肯定2个点</span></span><br><span class="line"> corner_embedding = pe_layer.forward_with_coords(coords, input_image_size)</span><br><span class="line"> corner_embedding[:, <span class="number">0</span>, :] += point_embeddings[<span class="number">2</span>].weight <span class="comment">#框第一个点</span></span><br><span class="line"> corner_embedding[:, <span class="number">1</span>, :] += point_embeddings[<span class="number">3</span>].weight <span class="comment">#框第二个点</span></span><br><span class="line"><span class="comment"># 汇总point、box编码</span></span><br><span class="line">sparse_embeddings = torch.empty((<span class="number">1</span>, <span class="number">0</span>, embed_dim))</span><br><span class="line">sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=<span class="number">1</span>)</span><br><span class="line">sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><strong>text</strong>：通过CLIP模型将文本编码到(1,M,C)</li></ul><h1 id="SAM的mask-prompt如何处理？"><a href="#SAM的mask-prompt如何处理？" class="headerlink" title="SAM的mask prompt如何处理？"></a>SAM的mask prompt如何处理？</h1><ul><li>mask利用CNN输出和image_embedding(1,C,H/16,W/16)一样大小的编码，后续直接相加</li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mask_downscaling = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, mask_in_chans // <span class="number">4</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    LayerNorm2d(mask_in_chans // <span class="number">4</span>),</span><br><span class="line">    activation(),</span><br><span class="line">    nn.Conv2d(mask_in_chans // <span class="number">4</span>, mask_in_chans, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    LayerNorm2d(mask_in_chans),</span><br><span class="line">    activation(),</span><br><span class="line">    nn.Conv2d(mask_in_chans, embed_dim, kernel_size=<span class="number">1</span>),</span><br><span class="line">)</span><br><span class="line">mask_input_size = (<span class="number">4</span> * image_embedding_size[<span class="number">0</span>], <span class="number">4</span> * image_embedding_size[<span class="number">1</span>])</span><br><span class="line">no_mask_embed = nn.Embedding(<span class="number">1</span>, embed_dim) </span><br><span class="line"><span class="keyword">if</span> masks <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    dense_embeddings = self._embed_masks(masks) <span class="comment"># 利用CNN生成mask embedding</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    dense_embeddings = self.no_mask_embed.weight.reshape(<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).expand(</span><br><span class="line">        bs, -<span class="number">1</span>, self.image_embedding_size[<span class="number">0</span>], self.image_embedding_size[<span class="number">1</span>]</span><br><span class="line">    ) <span class="comment"># 随机初始化生成mask embedding</span></span><br></pre></td></tr></table></figure><h1 id="SAM-的-mask-decoder"><a href="#SAM-的-mask-decoder" class="headerlink" title="SAM 的 mask decoder?"></a>SAM 的 mask decoder?</h1></li><li><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAM-4.png" alt=""></p></li><li><strong>输入</strong>:image_embedding(1, C, H/16, W/16)、image_embedding大小的位置编码image_pe(1, C, H/16, W/16)、稀疏提示编码sparse_prompt_embeddings(1, N, C)、密集提示编码dense_prompt_embeddings(1,C,H/16, W/16)</li><li><strong>(1)tansformer整合所有编码</strong>:将image_embedding+dense_prompt_embeddings视为transformer encoder的k,image_pe视为pos embedding,sparse_prompt_embeddings视为decoder的q，并且参考VIT的class_token，不直接使用sparse_prompt_embeddings输出作为最终结果，而是另外生成1个iou token和P个mask token作为最终结果，所以输入transformer decoder的token变为(1,1+P+N,C)，经过transformer后decoder和encoder分别输出hs(1,1+P+N,C), src(1,HW/256,C)；<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"> num_multimask_outputs=<span class="number">3</span></span><br><span class="line">transformer_dim=<span class="number">256</span></span><br><span class="line"> iou_token = nn.Embedding(<span class="number">1</span>, transformer_dim)</span><br><span class="line"> num_mask_tokens = num_multimask_outputs + <span class="number">1</span></span><br><span class="line"> mask_tokens = nn.Embedding(num_mask_tokens, transformer_dim)</span><br><span class="line"> <span class="comment"># Concatenate output tokens</span></span><br><span class="line"> output_tokens = torch.cat([iou_token.weight, mask_tokens.weight], dim=<span class="number">0</span>) <span class="comment"># torch.Size([5, 256])</span></span><br><span class="line"> output_tokens = output_tokens.unsqueeze(<span class="number">0</span>).expand(sparse_prompt_embeddings.size(<span class="number">0</span>), -<span class="number">1</span>, -<span class="number">1</span>) <span class="comment"># torch.Size([1, 5, 256])</span></span><br><span class="line"> tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=<span class="number">1</span>) <span class="comment"># torch.Size([1, 12, 256])</span></span><br><span class="line"> <span class="comment"># Expand per-image data in batch direction to be per-mask</span></span><br><span class="line"> src = torch.repeat_interleave(image_embeddings, tokens.shape[<span class="number">0</span>], dim=<span class="number">0</span>) <span class="comment"># torch.Size([1, 256, 64, 64]) -》torch.Size([1, 256, 64, 64])</span></span><br><span class="line"> src = src + dense_prompt_embeddings <span class="comment"># torch.Size([1, 256, 64, 64])+torch.Size([1, 256, 64, 64])</span></span><br><span class="line"> pos_src = torch.repeat_interleave(image_pe, tokens.shape[<span class="number">0</span>], dim=<span class="number">0</span>) <span class="comment"># torch.Size([1, 256, 64, 64])</span></span><br><span class="line"> b, c, h, w = src.shape</span><br><span class="line"> <span class="comment"># Run the transformer torch.Size([1, 256, 64, 64])，torch.Size([1, 256, 64, 64])，torch.Size([1, 12, 256])</span></span><br><span class="line"> hs, src = transformer(src, pos_src, tokens) <span class="comment"># torch.Size([1, 12, 256]) torch.Size([1, 4096, 256]) = q,k</span></span><br><span class="line"> iou_token_out = hs[:, <span class="number">0</span>, :] <span class="comment"># torch.Size([1, 256])</span></span><br><span class="line"> mask_tokens_out = hs[:, <span class="number">1</span> : (<span class="number">1</span> + num_mask_tokens), :] <span class="comment"># torch.Size([1, 4, 256])</span></span><br></pre></td></tr></table></figure></li><li><strong>(2)生成Mask预测</strong>：取hs的第1-P个token作为预测结果mask_tokens_out，src经过反卷积上采样4倍，输出upscaled_embedding(1,HW/16,C’)，mask_tokens_out经过MLP操作，将隐变量长度变为C’,即输出hyper_in(1,P,C’)，hyper_in与upscaled_embedding点乘后输出masks(1,P,HW/16)，表示p个mask<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">self.output_upscaling = nn.Sequential(</span><br><span class="line">    nn.ConvTranspose2d(transformer_dim, transformer_dim // <span class="number">4</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    LayerNorm2d(transformer_dim // <span class="number">4</span>),</span><br><span class="line">    activation(),</span><br><span class="line">    nn.ConvTranspose2d(transformer_dim // <span class="number">4</span>, transformer_dim // <span class="number">8</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    activation(),</span><br><span class="line">)</span><br><span class="line">self.output_hypernetworks_mlps = nn.ModuleList(</span><br><span class="line">    [MLP(transformer_dim, transformer_dim, transformer_dim // <span class="number">8</span>, <span class="number">3</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_mask_tokens)])  </span><br><span class="line"><span class="comment"># Upscale mask embeddings and predict masks using the mask tokens</span></span><br><span class="line">src = src.transpose(<span class="number">1</span>, <span class="number">2</span>).view(b, c, h, w) <span class="comment"># torch.Size([1, 256, 64, 64])</span></span><br><span class="line">upscaled_embedding = self.output_upscaling(src) <span class="comment"># torch.Size([1, 256, 64, 64]) -》torch.Size([1, 32, 256, 256])</span></span><br><span class="line">hyper_in_list: List[torch.Tensor] = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_mask_tokens):</span><br><span class="line">    hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :])) <span class="comment"># torch.Size([1, 32])x4</span></span><br><span class="line">hyper_in = torch.stack(hyper_in_list, dim=<span class="number">1</span>) <span class="comment"># torch.Size([1, 4, 32])</span></span><br><span class="line">b, c, h, w = upscaled_embedding.shape <span class="comment"># torch.Size([1, 32, 256, 256])</span></span><br><span class="line"><span class="comment"># 运算符@表示矩阵的点乘</span></span><br><span class="line">masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -<span class="number">1</span>, h, w) <span class="comment"># torch.Size([1, 4, 32]) @ torch.Size([1, 32, 256, 256]) -&gt; torch.Size([1, 4, 256, 256])</span></span><br></pre></td></tr></table></figure></li><li><p><strong>(3)生成IOU预测</strong>：取hs的第1个token作为预测结果iou_token_out，然后使用MLP将隐变量长度变为P，表示P各mask的iou预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">iou_prediction_head = MLP(transformer_dim, iou_head_hidden_dim, num_mask_tokens, iou_head_depth)</span><br><span class="line"><span class="comment"># Generate mask quality predictions</span></span><br><span class="line">iou_pred = iou_prediction_head(iou_token_out) <span class="comment"># torch.Size([1,256]) -&gt; torch.Size([1, 4])  </span></span><br></pre></td></tr></table></figure><h1 id="SAM-如何直接分割所有目标？"><a href="#SAM-如何直接分割所有目标？" class="headerlink" title="SAM 如何直接分割所有目标？"></a>SAM 如何直接分割所有目标？</h1></li><li><p>以原图所有cell作为point prompt输入，输出Mask和iou后，通过iou阈值过滤mask,得到所有目标的mask</p></li></ul><p>参考：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/621040230">模型方法—-真的分割任何东西(Segment Anything) - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;SAM通过transformer将点、框、Mask、文本等prompt和图片进行编码学习，可以实现对图片任意目标的分割&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
    <category term="分割" scheme="https://shaogui.life/tags/%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>我的语义分割学习路线</title>
    <link href="https://shaogui.life/2023/05/15/%E6%88%91%E7%9A%84%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    <id>https://shaogui.life/2023/05/15/%E6%88%91%E7%9A%84%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/</id>
    <published>2023-05-15T15:30:22.000Z</published>
    <updated>2023-05-20T11:12:59.719Z</updated>
    
    <content type="html"><![CDATA[<p><img src="../source/Excalidraw/Drawing%202023-02-14%2011.31.41.excalidraw.svg" alt="Drawing 2023-02-14 11.31.41.excalidraw"></p><p>本文总结自己目前对语义分割的认识，和学习过程</p><a id="more"></a><h1 id="什么是语义分割？"><a href="#什么是语义分割？" class="headerlink" title="什么是语义分割？"></a>什么是语义分割？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2%E9%80%9A%E7%94%A8%E7%9F%A5%E8%AF%86-20230408143823.jpeg" alt=""></li><li>语义分割是针对图片像素点的分类，1个像素的类别可以是单标签，也可以是多标签</li></ul><h1 id="语义分割原理"><a href="#语义分割原理" class="headerlink" title="语义分割原理"></a>语义分割原理</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-19%2022.02.18.excalidraw.svg" alt="Drawing 2023-03-19 22.02.18.excalidraw"></li><li>图片经过 CNN 提取特征后，得到的是分辨率变小的2D featrue map，通过上采样将 featrue map 的分辨率变大，然后预测每个 grid 的类别（可以是单标签预测，也可以是多标签）</li></ul><h1 id="语义分割难点"><a href="#语义分割难点" class="headerlink" title="语义分割难点"></a>语义分割难点</h1><ol><li><strong>分辨率下降</strong>：CNN提取的特征平移不变性，这对分类任务很有用，但是对分割来说，希望原图目标移动后，其特征的响应也在移动，因此分辨率下降导致最后featrue map包含位置信息少。通常使用“跳跃连接”解决，即将包含位置信息的高分辨率特征和包含语义信息的低分辨率融合解决</li><li><strong>感受野小</strong>：CNN下采样倍率一般比较小，所以随后featrue map上每个grid的感受野一般不大，这就意味着每个特征接收少部分其他像素的信息，这对大尺度的目标来说是非常不利的。通常使用“空洞卷积”解决，即在不加深网络的情况下提高感受野</li><li><strong>目标多尺度</strong>：图片存在多尺度的目标，如果仅使用一种分辨率去做最后分类，对其他分辨率效果不佳。通常使用多尺度特征融合来解决，比如PSP、ASPP模块</li><li><strong>依赖距离短</strong>：这和感受野的影响类似，但是即使感受野再大，也不能大过原图，所以像素之间的长程依赖还不够。通常使用“自注意力机制”去解决，比如构建特征的通道注意力、空间注意力去创建这种依赖</li></ol><h1 id="语义分割模型的种类"><a href="#语义分割模型的种类" class="headerlink" title="语义分割模型的种类"></a>语义分割模型的种类</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-19%2022.20.10.excalidraw.svg" alt="Drawing 2023-03-19 22.20.10.excalidraw"></li><li><strong>金字塔模型</strong>：通过构建并融合多尺度特征，实现对不同尺度目标的分割，代表模型有 DeepLab 系列，PSPNet，DANet，APCNet</li><li><strong>编码器-解码器</strong>：使用 CNN 下采样提取特征，然后使用线性插值、反卷积、反池化操作实现上采样，并通过跳跃连接将高分辨率的位置信息联通到低分辨率的语义特征</li><li><strong>“自注意力”系列</strong>：通过引入“自注意力”机制，构建像素之间的远程连接，解决感受野解决不了的尺度问题</li></ul><h1 id="语义分割的上采样类型"><a href="#语义分割的上采样类型" class="headerlink" title="语义分割的上采样类型"></a>语义分割的上采样类型</h1><p>语义分割在还原分辨率时，通常使用上采样，不同的上采样在速度、精度有不同区别</p><div class="table-container"><table><thead><tr><th>方法</th><th>描述</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>线性插值</td><td>通过相邻的元素决定待插值点的值，如最近邻插值、线性插值、双3次线性插值</td><td>快速、无需学习</td><td>-</td></tr><tr><td>反池化</td><td>记录池化时的激活位置，上采样时直接将值赋值给这个位置</td><td>无需学习</td><td>需要额外存储记录激活；上采样效果不好</td></tr><tr><td>反卷积</td><td>通过反卷积上采样</td><td>可以被学习优化</td><td>增加模型计算，有网格效应</td></tr></tbody></table></div><h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-20230408153654.png" alt="机器学习常见评价指标-20230408153654"></p><ul><li>语义分割常使用 mIOU 作为统计指标，注意：<strong>统计某类 TP、FP、FN 指标时，是针对所有图片的所有<mark style="background: #BBFABBA6;">像素</mark>预测结果、而不是具体一张图片</strong></li><li>首先统计某个类别在所有图片上的累计 TP、FP、FN 像素数量、然后计算这个类别的 IOU ，再算所有类别的平均<script type="math/tex">I o U=\frac{T P}{F P+T P+F N}</script></li></ul><h1 id="学习路线"><a href="#学习路线" class="headerlink" title="学习路线"></a>学习路线</h1><p><img src="../source/Excalidraw/Drawing%202023-02-14%2011.31.41.excalidraw.svg" alt="Drawing 2023-02-14 11.31.41.excalidraw"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;../source/Excalidraw/Drawing%202023-02-14%2011.31.41.excalidraw.svg&quot; alt=&quot;Drawing 2023-02-14 11.31.41.excalidraw&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文总结自己目前对语义分割的认识，和学习过程&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="语义分割" scheme="https://shaogui.life/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>我的人体姿态估计学习路线</title>
    <link href="https://shaogui.life/2023/05/14/%E6%88%91%E7%9A%84%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    <id>https://shaogui.life/2023/05/14/%E6%88%91%E7%9A%84%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/</id>
    <published>2023-05-14T14:30:22.000Z</published>
    <updated>2023-05-20T08:00:42.331Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-13%2023.28.32.excalidraw.svg" alt=""></p><p>本文总结自己目前对人体姿态估计的认识，和学习过程</p><a id="more"></a><h1 id="什么是人体姿态检测？"><a href="#什么是人体姿态检测？" class="headerlink" title="什么是人体姿态检测？"></a>什么是人体姿态检测？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-20230417230640.png" alt=""></li><li><strong>在人体关节连接的所有姿势空间中搜索某个特定姿势，本质为关节点的定位</strong></li><li>点之间形成的连接必须很重要，这意味着并非所有点都可以形成一对。从一开始，HPE 的目标就是形成类似骨骼的人体表示，然后针对特定任务的应用对其进行进一步处理</li></ul><h1 id="人体姿态检测的方法-？"><a href="#人体姿态检测的方法-？" class="headerlink" title="人体姿态检测的方法 ？"></a>人体姿态检测的方法 ？</h1><ul><li><strong>从上到下 (二阶段)</strong> ：主要包含两个部分，目标检测和单人人体骨骼关键点检测，对于目标检测算法，这里不再进行描述，而对于关键点检测算法，首先需要注意的是关键点局部信息的区分性很弱，即背景中很容易会出现同样的局部区域造成混淆，所以需要考虑较大的感受野区域；其次人体不同关键点的检测的难易程度是不一样的，对于腰部、腿部这类关键点的检测要明显难于头部附近关键点的检测，所以不同的关键点可能需要区别对待；最后自上而下的人体关键点定位依赖于检测算法的提出的 Proposals，会出现检测不准和重复检测等现象。如：CPM、CPN、RMPE</li><li><strong>从下到上 (单阶段)</strong> ：主要包含两个部分，关键点检测和关键点聚类，其中关键点检测和单人的关键点检测方法上是差不多的，区别在于这里的关键点检测需要将图片中所有类别的所有关键点全部检测出来，然后对这些关键点进行聚类处理，将不同人的不同关键点连接在一块，从而聚类产生不同的个体。如：PAFs</li></ul><h1 id="人体姿态检测的评价标准PCK？"><a href="#人体姿态检测的评价标准PCK？" class="headerlink" title="人体姿态检测的评价标准PCK？"></a>人体姿态检测的评价标准PCK？</h1><ul><li><strong>预测关键点与 GT 关键点的归一化距离，如果小于某个阈值，该关键点预测正确，否则失败，PCK 等于预测正确的比例</strong>，这个值和阈值有关，有 PCK@0.2 、 PCKh@0.5 ，PCK 用于 2D 和 3D ，值越高越好</li><li>PCK 值是针对一个人的关键点来说的，也就说<strong>不存在预测结果多于 gt 的问题，或者说这个问题在计算 PCK 前已经解决</strong>，计算多人的 PCK 值是所有人 PCK 值的平均</li><li>1）<strong>计算某个关键点的的 PCK 值</strong>：即以下公式，其中 i 表示 id=i 的关键点，k 表示第 k 个阈值 $T^k$ ，p 表示第 p 个行人，$d_{pi}$ 表示第 p 个人的 id=1 的预测关键点与 GT 关键点之间的欧式距离，$d_p^{def}$ 是第 p 个人的尺度因子，$T_k \in [0:0.001:0.1]$ 表示人工设定的阈值 <script type="math/tex; mode=display">PCK_i^k=\dfrac{\sum_p\delta(\frac{d_{pi}}{d_p^{def}}\leq T_k)}{\sum_p1}</script></li><li>2）<strong>计算 PCK 值</strong>：所有人所有关键点的 PCK 值的平均是模型的 PCK 值 <script type="math/tex; mode=display">PCK^k_{mean}=\dfrac{\sum_p\sum_i\delta(\frac{d_{pi}}{d_p^{def}}\leq T_k)}{\sum_p\sum_i1}</script></li><li>不同的数据集 $d_p^{def}$ 计算方法不同，MPII 中是以头部长度（头部矩形框的左上点与右下点的欧式距离）作为归一化参考，即 <strong>PCKh</strong></li></ul><h1 id="人体姿态检测的评价标准Oks？"><a href="#人体姿态检测的评价标准Oks？" class="headerlink" title="人体姿态检测的评价标准Oks？"></a>人体姿态检测的评价标准Oks？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Pasted%20image%2020230505085804.png" alt=""></li><li>PCK 仅通过归一化的欧式距离就决定关键点的预测准确度，OKs 在此基础上，加入关键点的标注难易偏差。上图是两个关键点的 OKS 值，</li><li>以下公式是 OKs 指标，其中 p 表示第 p 个行人，$p^i$ 表示第 p 个行人的关键点 i，$d<em>{pi}$ 是第 p 个行人的关键点 i 的预测位置与 GT 位置的欧式距离，$v</em>{p^i}=1、v_{p^i}=2$ 表示关键点无遮挡且标注、有遮挡且标注，$S_p$ 表示第 p 个行人的尺度因子，其值为行人检测框 (w, h)面积平方根 $S_p=\sqrt{wh}$，$\sigma_i$ 表示类型为 i 的关键点归一化因子，<script type="math/tex; mode=display">OKS_p=\dfrac{\sum_i exp\{-d_{pi}^2/2S_p^2\sigma_i^2\}\delta(v_{pi}>0)}{\sum_i\delta(v_{pi}>0)}</script></li><li>$\sigma_i$ 这个因子是通过对所有的样本的 GT 关键点由人工标注与真实值存在的标准差， $\sigma$  越大表示此类型的关键点越难标注。coco 数据集统计出17类关键点的归一化因子：{鼻子：0.026，眼睛：0.025，耳朵：0.035，肩膀：0.079，手肘：0.072，手腕：0.062，臀部：0.107，膝盖：0.087，脚踝：0.089}</li></ul><h1 id="人体姿态检测的评价标准mAp？"><a href="#人体姿态检测的评价标准mAp？" class="headerlink" title="人体姿态检测的评价标准mAp？"></a>人体姿态检测的评价标准mAp？</h1><ul><li>OKS 在物体检测中扮演的角色与 IoU 的作用相同。它是根据预测点和由人的比例对地面实况点之间的距离计算得出的。论文常出现以下指标 AP50（OKS = 0.50 的 AP）、AP（OKS 在[0.5:0.05:0.95]10 个位置的 AP 值）</li><li><strong>单人姿态估计 AP：</strong> 计算出 groundtruth 与检测得到的关键点的相似度<strong>oks</strong>为一个标量，然后人为的给定一个阈值<strong>T</strong>，然后可以通过所有图片的<strong>oks</strong>计算<strong>AP</strong><script type="math/tex; mode=display">AP=\dfrac{\sum_p\delta(oks_p>T)}{\sum_p1}</script></li><li><strong>多人姿态估计 AP：</strong> 1）如果采用的检测方法是自顶向下，先把所有的人找出来再检测关键点，那么其<strong>AP</strong>计算方法如同<strong>单人姿态估计 AP</strong>；2）如果采用的检测方法是自底向上，先把所有的关键点找出来然后再组成人。假设一张图片中有 M 个人，预测出 N 个人，由于不知道预测出的<strong>N</strong>个人与 groundtruth 中的<strong>M</strong>个人的一一对应关系，因此需要计算 groundtruth 中每一个人与预测的<strong>N</strong>个人的<strong>oks</strong>，那么可以获得一个大小为<em>M</em>×<em>N</em>的矩阵，矩阵的每一行为 groundtruth 中的一个人与预测结果的<strong>N</strong>个人的<strong>oks</strong>，然后找出每一行中<strong>oks</strong>最大的值作为当前<strong>GT</strong>的<strong>oks</strong>。最后每一个<strong>GT</strong>行人都有一个标量<strong>oks</strong>，然后人为的给定一个阈值<strong>T</strong>，然后可以通过所有图片中的所有行人计算<strong>AP</strong><script type="math/tex; mode=display">AP=\dfrac{\sum_m\sum_p\delta(oks_p>T)}{\sum_m\sum_p1}</script></li></ul><h1 id="学习路线"><a href="#学习路线" class="headerlink" title="学习路线"></a>学习路线</h1><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-13%2023.28.32.excalidraw.svg" alt=""></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-13%2023.28.32.excalidraw.svg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文总结自己目前对人体姿态估计的认识，和学习过程&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="人体姿态估计" scheme="https://shaogui.life/tags/%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>我的实例分割学习路线</title>
    <link href="https://shaogui.life/2023/05/13/%E6%88%91%E7%9A%84%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    <id>https://shaogui.life/2023/05/13/%E6%88%91%E7%9A%84%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/</id>
    <published>2023-05-13T15:55:34.000Z</published>
    <updated>2023-05-20T05:34:56.582Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2013.39.58.excalidraw.svg" alt="Drawing 2023-03-22 13.39.58.excalidraw"></p><p>本文总结自己目前对实例分割 的认识，和学习过程</p><a id="more"></a><h1 id="什么是实例分割？"><a href="#什么是实例分割？" class="headerlink" title="什么是实例分割？"></a>什么是实例分割？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2010.13.11.excalidraw.svg" alt="Drawing 2023-03-22 10.13.11.excalidraw"></li><li>目标检测针对的是目标，语义分割针对的是像素，而实例分割针对的是实例。所谓实例就是一个不管类别、不管是否连续的 1 个目标</li><li>上图是对一张图上 3 类 4 个实例的分割示意图，最后的输出结果是<strong>每个实例的语义分割图</strong></li></ul><h1 id="实例分割的原理？"><a href="#实例分割的原理？" class="headerlink" title="实例分割的原理？"></a>实例分割的原理？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2010.27.55.excalidraw.png" alt="Drawing 2023-03-22 10.27.55.excalidraw"></li><li><strong>Two-satge</strong>：既然实例分割是针对实例的语义分割，最直接的办法就是先找出单个实例的区域，然后对这个区域进行语义分割即可</li><li><strong>one-stage</strong>：不找实例的区域，而是针对每个 grid 生成 1 个语义分割预测，通过后处理获得实例的类别及分割结果</li></ul><h1 id="实例分割的方法？"><a href="#实例分割的方法？" class="headerlink" title="实例分割的方法？"></a>实例分割的方法？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2009.38.03.excalidraw.svg" alt="Drawing 2023-03-22 09.38.03.excalidraw"></li><li><mark style="background: #FF5582A6;">two-satge</mark>：类似于目标检测的 two-satge 模型，即<strong>先检测出目标的获选框，然后对候选框进行语义分割和位置回归</strong></li><li><mark style="background: #FFB86CA6;">one-stage</mark>：类似于目标检测的 one-stage 模型，即<strong>先通过 grid 确保这个位置有目标，然后对这个位置进行位置回归 (目标检测)或者 mask 生成 (实例分割)</strong></li><li>two-stage 的方法原始直接，但是对重叠目标的 Mask 预测比较麻烦，而且速度较慢；one-stage 类似 YOLO 系列的思想，速度较快</li></ul><h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-14%2020.11.25.excalidraw.svg" alt="Drawing 2023-03-14 20.11.25.excalidraw"></li><li>和目标检测一样，实例分割使用 mAP 评价模型性能，注意：<strong>统计某类的 TP、FP、FN 时，是针对所有图片的<mark style="background: #FF5582A6;">实例</mark>预测结果进行，不针对具体图片</strong></li><li>AP 是指某个类别预测情况的平均精准率，mAP 指所有类别 AP 的平均</li><li>AP 可以通过求解 PR 曲线下的面积得到，求解方式包括11个点和矩形求解</li><li>每个 PR 区域是某个 IOU 阈值绘制的，并且这个 IOU 阈值已经由单阈值发展到多阈值</li></ul><h1 id="two-stage"><a href="#two-stage" class="headerlink" title="two-stage"></a>two-stage</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2013.46.18.excalidraw.svg" alt="Drawing 2023-03-22 13.46.18.excalidraw"></li><li>自上而下的实例分割方法，首先按获取候选框，然后在此基础上进行目标框回归和 Mask 生成</li><li><strong>Mask RCNN</strong>：使用 RPN 获得候选框、使用 Faster RCNN 预测目标类别、使用 FCN 生成 Mask</li><li><strong>PAN</strong>：通过在 FPN 的基础上引入 bottom-up 路径，让底层信息更快传递到高层，其思想和 Mask RCNN 一致</li></ul><h1 id="One-stage"><a href="#One-stage" class="headerlink" title="One-stage"></a>One-stage</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2013.47.15.excalidraw.svg" alt="Drawing 2023-03-22 13.47.15.excalidraw"></li><li>自下而上的实例分割方法类似 YOLO 系列将每个 grid 看作 1 个目标，这类实例分割方法将每个 grid 视为一个实例，并为每个 grid 预测 1 个 Mask</li><li><strong>SOLO</strong>：输出包含 2 个分支，一个是 heatmap 分支，判定该 grid 是否包含实例，另一个分支是为该 grid 生成 Mask</li><li><strong>SOLOv2</strong>：在 SOLOv1 的基础上，将 Mask 分支解藕卷积核生成、卷积特征生成 2 个分支，监督网络学习卷积核，使得网络能动态学习实例的特征</li></ul><h1 id="学习路线"><a href="#学习路线" class="headerlink" title="学习路线"></a>学习路线</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2013.39.58.excalidraw.svg" alt="Drawing 2023-03-22 13.39.58.excalidraw"></li></ul><ol><li><a href="MaskRCNN.md">MaskRCNN</a>：在 Faster RCNN 的基础上增加 FCN 分支，提出 ROIAlign 对齐 ROI 下采样</li><li><a href="PAN.md">PAN</a>：类似 MaskRCNN 过程，BackBone 使用 PAN 进行特征融合，最后融合所有尺度目标进行目标定位与分割</li><li><a href="yolact.md">yolact</a>：首先生成一批 prototype mask，然后目标分支生成一组权重，加权得到每个 grid 的分割结果</li><li><a href="SOLO.md">SOLO</a>：首先 Category 分支对 grid 进行分类，然后 Mask 分支生成每个 grid 的分割，实际使用通过解藕头构建 Mask 分支</li><li><a href="SOLOv2.md">SOLOv2</a>：将 SOLO 的 Mask 生成分支解藕为<strong>掩码核预测分支</strong>和掩码特征学习分支，分别负责生成卷积核和需要卷积的特征映射</li><li><a href="yolactplusplus.md">yolactplusplus</a>：基本和 yolact 类似，生成更多 anchor 、重新生成的 Mask scoreing 分支</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2013.39.58.excalidraw.svg&quot; alt=&quot;Drawing 2023-03-22 13.39.58.excalidraw&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文总结自己目前对实例分割 的认识，和学习过程&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="实例分割" scheme="https://shaogui.life/tags/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>我的OCR学习路线</title>
    <link href="https://shaogui.life/2023/05/10/%E6%88%91%E7%9A%84OCR%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    <id>https://shaogui.life/2023/05/10/%E6%88%91%E7%9A%84OCR%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/</id>
    <published>2023-05-10T05:09:54.000Z</published>
    <updated>2023-05-20T06:22:51.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-11%2018.42.57.excalidraw.svg" alt="Drawing 2023-04-11 18.42.57.excalidraw"></p><p>本文总结自己目前对 OCR 的认识，和学习过程</p><a id="more"></a><h1 id="什么是-OCR-？"><a href="#什么是-OCR-？" class="headerlink" title="什么是 OCR ？"></a>什么是 OCR ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/FOTS-20230409214506.png" alt=""></li><li>OCR （Optical Character Recognition，光学字符识别）指将打字、手写或印刷文本的图像电子或机械转换为机器编码文本的过程</li><li><strong>文本检测 (Text detection)</strong> ：<strong>检测文本的所在位置和范围及其布局</strong>，可以使用传统的 ROI 提取实现，也可使用目标检测去实现，如 Faster R-CNN，[[FCN]]</li><li><strong>文本识别 (Text recognition)</strong>：<strong>对文本内容进行识别</strong>，将图像中的文本信息转化为文本信息</li><li><strong>文本定位 (Text Spotting)</strong> ：分文本检测 (Text detection) 文本识别 (Text recognition)统一到一起的简称</li></ul><h1 id="OCR-的方法？"><a href="#OCR-的方法？" class="headerlink" title="OCR 的方法？"></a>OCR 的方法？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-11%2018.42.57.excalidraw.svg" alt="Drawing 2023-04-11 18.42.57.excalidraw"></li><li><strong>文本检测 (Text detection)</strong> ：其实就是检测文本行<strong>实例</strong>，可以使用目标检测的方法，也可使用语义分割的方法</li><li><strong>文本识别 (Text recognition)</strong> ：这个是 OCR 的重点，主要有 3 条路线</li><li><strong>文本定位 (Text Spotting)</strong> ：分为两种，但阶段和双阶段</li></ul><h1 id="评价指标-字符评价"><a href="#评价指标-字符评价" class="headerlink" title="评价指标-字符评价"></a>评价指标-字符评价</h1><ul><li>以字符 （文字和标点符号） 为单位的统计和分析，适用于通用印刷体、手写体类非结构化数据的OCR应用评测</li><li><strong>字符召回率</strong>：预测正确的字符总数占<strong>总符号</strong>的比例</li><li><strong>字符准确率</strong>：预测正确的字符占<strong>总测试结果</strong>的比例</li><li><strong>F-socre</strong>：字符召回率和字符准确率的综合评价指标</li></ul><h1 id="评价指标-文本段评价"><a href="#评价指标-文本段评价" class="headerlink" title="评价指标-文本段评价"></a>评价指标-文本段评价</h1><ul><li>以字段为单位的统计和分析，适用于卡证类、票据类等结构化程度较高的 OCR 应用评测</li><li><strong>字段召回率</strong>：完全识别准确的字段总数占<strong>总字段</strong>的比例</li><li><strong>字段准确率</strong>：完全识别准确的字段占<strong>总测试结果</strong>的比例</li><li><strong>最小编辑距离</strong>：编辑距离是针对二个字符串（例如英文字）的差异程度的量化量测，通过替换、插入、删除，将预测结果修正为gt所需操作步骤，最小编辑距离表示最少操作步数</li><li><strong>全图编辑距离</strong>：整个文本段的编辑距离</li><li><strong>归一化编辑距离</strong>: 编辑距离除以字符串长度</li></ul><h1 id="学习路线"><a href="#学习路线" class="headerlink" title="学习路线"></a>学习路线</h1><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2014.25.52.excalidraw.svg" alt="Drawing 2023-03-22 14.25.52.excalidraw"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-11%2018.42.57.excalidraw.svg&quot; alt=&quot;Drawing 2023-04-11 18.42.57.excalidraw&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文总结自己目前对 OCR 的认识，和学习过程&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
  </entry>
  
  <entry>
    <title>HSSN：Deep Hierarchical Semantic Segmentation</title>
    <link href="https://shaogui.life/2023/04/15/HSSN%EF%BC%9ADeep%20Hierarchical%20Semantic%20Segmentation/"/>
    <id>https://shaogui.life/2023/04/15/HSSN%EF%BC%9ADeep%20Hierarchical%20Semantic%20Segmentation/</id>
    <published>2023-04-15T14:54:52.000Z</published>
    <updated>2023-05-20T11:04:41.694Z</updated>
    
    <content type="html"><![CDATA[<p>HSSN抛弃传统的每个像素进行扁平分类的思想，在借鉴层次聚类的想法后，在网络输出端增加”类别树”约束，使得网络学习到的特征更加鲁棒</p><a id="more"></a><h1 id="什么是-HSSN？"><a href="#什么是-HSSN？" class="headerlink" title="什么是 HSSN？"></a>什么是 HSSN？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/HSSN-20230408143142.png" alt="HSSN-20230408143142"></li><li>人类能够在观察中识别结构化关系，使我们能够将复杂的场景分解为更简单的部分，并在多个层次上抽象视觉世界。然而，人类感知的这种分层推理能力在当前的语义分割文献中仍然在很大程度上没有得到探索。现有工作通常知道扁平化标签，并专门针对每个像素预测目标类</li><li>在本文中，我们讨论了分层语义分割（HSS），它旨在根据类层次结构对视觉观察进行结构化的像素级描述。我们设计了 HSSN，这是一个通用的 HSS 框架，解决了这项任务中的两个关键问题：i）如何有效地使现有的与层次结构无关的分段网络适应 HSS 设置，以及 ii）如何利用层次结构信息来规范 HSS 网络学习</li><li>了解决第1个问题，HSSN直接将HSS视为一个像素级多标签分类问题，因此相比于现在的分割模型只引入了极小的改动，对于第2个问题，HSSN网络首先将探索语义层次作为训练目标，这将会迫使分割结果遵从语义结构，同时，通过施加类别间的边缘约束，HSSN将会对像素映射空间进行重新构造，最终产生更好的像素表示并提升模型的效果</li></ul><h1 id="HSSN-的网络结构？"><a href="#HSSN-的网络结构？" class="headerlink" title="HSSN 的网络结构？"></a>HSSN 的网络结构？</h1><ul><li><strong>Hierarchical Semantic Segmentation Networks</strong>：这一部分确保了类别的连贯性以及一致性（推测在取类别最大值的时候会做一些额外的处理，来确保所有取到的最大值都处于同一个类别树的分支上保证类别的连贯，但是作者在正文中没给出）</li><li><strong>Hierarchy-Aware Segmentation Learning</strong>：<ul><li><strong>pixel-wise hierarchical segmentation learning strategy</strong>：像素级层次分割学习策略，确保预测能够在层次关系上保持一致<ul><li>每个像素分配的标签都应该具有层次上的一致性，因此需要遵守以下的两个原则 <img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/HSSN-20230408143142-1.png" alt="HSSN-20230408143142-1"></li><li><strong>Tree-Min Loss</strong>：如果子节点是对的，就取概率最小的节点路径（保证子节点要比父节点概率小，这条路径还没到当前的节点，也保证了之前的路径最短），如果子节点是错的，就取概率最大的节点路径（因为是错的取最大，最大最小一样的概率很小，所以就能够防止在这条路径上走下去，另外这条路径已经包括了当前的节点，或者说这条路径的最后一个节点就是当前这个节点）。相比于直接使用BCE Loss，Tree-Min Loss得出的score能确保完全符合作者设定的两个限制条件，并能够加大对不符合条件预测的惩罚</li><li><strong>Focal Tree-Min Loss</strong>：受到focal Loss的启发，作者在其中增加了调节因子，以便能够对困难的例子更好的学习</li></ul></li><li><strong>pixel-wise hierarchical representation learning strategy</strong>：像素级层次表示学习策略，确保在表示空间中能够将不同类别的表示有效地重构，从而学习到更好的表示。类似于对比学习，作者会挑选标准图、正样本、负样本三张图作为一组，通过多组计算损失，里面的参数 m 会迫使正负样本的距离超过 m <img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/HSSN-20230408143143.png" alt="HSSN-20230408143143"></li></ul></li></ul><h1 id="HSSN-如何更新预测分数的？"><a href="#HSSN-如何更新预测分数的？" class="headerlink" title="HSSN 如何更新预测分数的？"></a>HSSN 如何更新预测分数的？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/HSSN-20230408143143-1.png" alt="HSSN-20230408143143-1"></li><li>上图是计算图像某一个像素点类别时画出的树状图，图（a）中将正确的分类路径用红色标注出来，错误的用蓝色标注（实心代表 positive class，空心代表 negative class），图（b）则显示了相对应的 BCE 损失，作者将两个不合理的点单独做了标注（在数字上添加了方框），可以看到在使用 BCE Loss 时，第二层节点的损失是差不多的，图（c）则采用了作者的 $L^{tm}$ 损失（后面会提到，这个损失可以保证在计算时让每个点都符合作者给出的两个限制条件），能够成功地在不同类别上显示出区分性，保证模型训练的层次结构较为合理</li><li>假设模型直接输出是 s，更新后是 p，其中 $A_v$ 与 $C_v$ 代表 $v$ 的父集和子类集，第二行也可以更新为 $p_v=max(s_u)$ ，按照以下公式更新得到图 c 的新分数 <script type="math/tex; mode=display">\begin{cases}\quad p_v=\min\limits_{u\in\mathcal{A}_v}(s_u)&\text{if}\hat{l}_v=1,\\ 1-p_v=\min\limits_{u\in\mathcal{C}_v}(1-s_u)=1-\max\limits_{u\in\mathcal{C}_v}(s_u)&\text{if}\hat{l}_v=0,\end{cases}</script></li><li><strong>总结</strong>：在父节点 $u$ 往下辨别子节点 $v$ 时，如果子节点是对的，就取概率最小的节点路径（保证子节点要比父节点概率小，这条路径还没到当前的节点，也保证了之前的路径最短），如果子节点是错的，就取概率最大的节点路径（因为是错的取最大，最大最小一样的概率很小，所以就能够防止在这条路径上走下去，另外这条路径已经包括了当前的节点，或者说这条路径的最后一个节点就是当前这个节点）</li></ul><h1 id="HSSN-如何强制预测符合类别层次关系的？"><a href="#HSSN-如何强制预测符合类别层次关系的？" class="headerlink" title="HSSN 如何强制预测符合类别层次关系的？"></a>HSSN 如何强制预测符合类别层次关系的？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/HSSN-20230408143143.png" alt="HSSN-20230408143143"></li><li>挑选标准图、正样本、负样本三张图作为一组，通过多组计算损失，里面的参数 m 会迫使正负样本的距离超过 m，从而使得损失为0 <script type="math/tex; mode=display">\mathcal{L}^{\mathrm{TT}}(\boldsymbol{i},\boldsymbol{i}^+,\boldsymbol{i}^-)=\max\{\langle\boldsymbol{i},\boldsymbol{i}^+\rangle-\langle\boldsymbol{i},\boldsymbol{i}^-\rangle+m,0\},</script></li></ul><h1 id="HSSN-的-TML-损失函数？"><a href="#HSSN-的-TML-损失函数？" class="headerlink" title="HSSN 的 TML 损失函数？"></a>HSSN 的 TML 损失函数？</h1><ul><li><strong>Tree-Min Loss</strong>：如果子节点是对的，就取概率最小的节点路径（保证子节点要比父节点概率小，这条路径还没到当前的节点，也保证了之前的路径最短），如果子节点是错的，就取概率最大的节点路径（因为是错的取最大，最大最小一样的概率很小，所以就能够防止在这条路径上走下去，另外这条路径已经包括了当前的节点，或者说这条路径的最后一个节点就是当前这个节点）。相比于直接使用 BCE Loss，Tree-Min Loss 得出的 score 能确保完全符合作者设定的两个限制条件，并能够加大对不符合条件预测的惩罚<script type="math/tex; mode=display">\begin{aligned}\mathcal{L}^{\mathtt{TM}}(p)&=\sum\limits_{v\in\mathcal{V}}-\hat{l}_v\log(p_v)-(1-\hat{l}_v)\log(1-p_v),\\ &=\sum\limits_{v\in\mathcal{V}}-\hat{l}_v\log(\min_{u\in\mathcal{A}_v}(s_u))-\\ &(1-\hat{l}_v)\log(1-\max_{u\in\mathcal{C}_v}(s_u)).\end{aligned}</script></li><li><strong>Focal Tree-Min Loss</strong>：受到 focal Loss 的启发，作者在其中增加了调节因子，以便能够对困难的例子更好的学习<script type="math/tex; mode=display">\begin{array}{l}\mathcal{L}^{\mathrm{TM}}(p)=\sum\limits_{v\in\mathcal{P}}-\hat{l}_v(1-p_v)^\gamma\log(p_v)-(1-\hat{l}_v)(p_v)^\gamma\log(1-p_v),\\ =\sum\limits_{v\in\mathcal{V}}-\hat{l}_v(1-\operatorname*{min}_{u\in\mathcal{A}_v}(s_u))^\gamma\log(\operatorname*{min}_{u\in A_v}(s_u))-(1-\hat{l}_v)(\max_{u\in\mathcal{C}_v}(s_u))^\gamma\log(1-\max_{u\in\mathcal{C}_v}(s_u)),\end{array}</script></li></ul><h1 id="HSSN-的-TTL-损失函数？"><a href="#HSSN-的-TTL-损失函数？" class="headerlink" title="HSSN 的 TTL 损失函数？"></a>HSSN 的 TTL 损失函数？</h1><ul><li>挑选标准图、正样本、负样本三张图作为一组，通过多组计算损失，里面的参数 m 会迫使正负样本的距离超过 m，从而使得损失为0 <script type="math/tex; mode=display">\mathcal{L}^{\mathrm{TT}}(\boldsymbol{i},\boldsymbol{i}^+,\boldsymbol{i}^-)=\max\{\langle\boldsymbol{i},\boldsymbol{i}^+\rangle-\langle\boldsymbol{i},\boldsymbol{i}^-\rangle+m,0\},</script></li></ul><p>参考：<br><a href="https://zhuanlan.zhihu.com/p/551064469">Papers - Deep Hierarchical Semantic Segmentation - 知乎</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;HSSN抛弃传统的每个像素进行扁平分类的思想，在借鉴层次聚类的想法后，在网络输出端增加”类别树”约束，使得网络学习到的特征更加鲁棒&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="语义分割" scheme="https://shaogui.life/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>EAST：An Efficient and Accurate Scene Text Detector</title>
    <link href="https://shaogui.life/2023/04/11/EAST%EF%BC%9AAn%20Efficient%20and%20Accurate%20Scene%20Text%20Detector/"/>
    <id>https://shaogui.life/2023/04/11/EAST%EF%BC%9AAn%20Efficient%20and%20Accurate%20Scene%20Text%20Detector/</id>
    <published>2023-04-11T14:35:22.000Z</published>
    <updated>2023-05-20T04:09:46.170Z</updated>
    
    <content type="html"><![CDATA[<p>EAST基于FCN输出，对每个grid进行文本行预测，可实现旋转矩形框、任意四边形框的预测</p><a id="more"></a><h1 id="什么是-EAST-？"><a href="#什么是-EAST-？" class="headerlink" title="什么是 EAST ？"></a>什么是 EAST ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/EAST-20230408144126.png" alt="EAST-20230408144126"></li><li>（a）、（b）、（c）、（d）都是几种常见 stat-of-the-art 的文本检测过程，算法思想遵循之前 two-stage 的方法，一般都需要先提出候选框，过滤后对剩下的候选框要进行回归操作得出更精细的边框信息，然后再合并候选框等</li><li><strong>EAST</strong>基于 FCN 输出特征，类似 anchor-free 的目标检测模型，预测每个 grid 的代表的文本行信息，然后使用 NMS（非极大值抑制）合并预测后的信息，可实现矩形、选择矩阵和四边形的文本检测，不能实现弯曲文本的检测</li></ul><h1 id="EAST-的网络结构？"><a href="#EAST-的网络结构？" class="headerlink" title="EAST 的网络结构？"></a>EAST 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/EAST-20230408144126-1.png" alt="EAST-20230408144126-1"></li><li><strong>FCN 特征提取</strong>：通过<strong>特征提取</strong>和<strong>特征融合</strong>两个步骤，最后取 C 2 特征输入预测头</li><li><strong>预测结果的输出层</strong>：假设 C 2 特征大小为 CHW，对于 HW 的每个 grid 输出 2 个分支，第一个分支是置信度 (1)，第二个分支是框位置，框位置如果是旋转矩形，则输出 4 (xyxy)+1（angle），如果是任意的四边形, 则输出 8 (xy * 4)</li></ul><h1 id="EAST-的标签分配？"><a href="#EAST-的标签分配？" class="headerlink" title="EAST 的标签分配？"></a>EAST 的标签分配？</h1><ul><li>检测 head 没有设置 anchor，直接按映射位置确定正样本，文本行比较大，可以按照多正样本匹配</li></ul><h1 id="EAST-的标签生成？"><a href="#EAST-的标签生成？" class="headerlink" title="EAST 的标签生成？"></a>EAST 的标签生成？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/EAST-20230408144127.png" alt="EAST-20230408144127"></li><li><strong>垂直或水平矩阵框 (AABB)</strong>：只需要 4 个值就可描述；<strong>旋转矩形框 (RBOX)</strong> ： AABB 的基础上增加角度，共 5 个值描述；<strong>任意四边形（QUAD）</strong>：需要 4 个点 8 个值去描述</li></ul><h1 id="EAST-的损失函数？"><a href="#EAST-的损失函数？" class="headerlink" title="EAST 的损失函数？"></a>EAST 的损失函数？</h1><ul><li><script type="math/tex; mode=display">L=L_\mathrm{s}+\lambda_\mathfrak{g}L_\mathfrak{g}</script></li><li><strong>分割损失 $L_s$</strong>：使用 blance 的交叉熵</li><li><strong>位置损失 $L_g$</strong>：直接使用 L 1 或者 L 2 损失去回归文本区域将导致损失偏差朝更大更长, 所以使用 IOU loss 监督 AABB 或 RBOX 类型框的位置；对于 QUAD 类型的回归框，使用尺度归一化的 smooth L 1 损失</li></ul><h1 id="EAST-如何解析模型输出"><a href="#EAST-如何解析模型输出" class="headerlink" title="EAST 如何解析模型输出"></a>EAST 如何解析模型输出</h1><ul><li>模型输出包括 2 部分，1）score map：检测框的置信度，1 个参数；2）text boxes：对于检测形状为 RBOX，检测框的位置（x, y, w, h）+旋转角度 (angle)，5 个参数；对于检测形状为 QUAD，则输出任意四边形检测框的位置坐标，(x 1, y 1), (x 2, y 2), (x 3, y 3), (x 4, y 4)，8 个参数</li><li>取 topK 的 score map 对应的预测框，然后采用 Locality-Aware NMS 过滤这些预测框，得到最终结果</li></ul><p>参考：</p><ol><li><a href="https://cloud.tencent.com/developer/article/1542875">05. OCR学习路径之文本检测（下）EAST算法简介 - 腾讯云开发者社区-腾讯云</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;EAST基于FCN输出，对每个grid进行文本行预测，可实现旋转矩形框、任意四边形框的预测&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文本检测" scheme="https://shaogui.life/tags/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>SAST：A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning</title>
    <link href="https://shaogui.life/2023/04/10/SAST%EF%BC%9AA%20Single-Shot%20Arbitrarily-Shaped%20Text%20Detector%20based%20on%20Context%20Attended%20Multi-Task%20Learning/"/>
    <id>https://shaogui.life/2023/04/10/SAST%EF%BC%9AA%20Single-Shot%20Arbitrarily-Shaped%20Text%20Detector%20based%20on%20Context%20Attended%20Multi-Task%20Learning/</id>
    <published>2023-04-10T15:20:53.000Z</published>
    <updated>2023-04-10T15:30:27.798Z</updated>
    
    <content type="html"><![CDATA[<p>属于EAST的演进版本，还是类似 anchor-free 的方式预测文本行，除了输出 grid 的 score +边框预测外，还输出更多的文本实例信息，比如 grid 到实例矩形四角、中心点、四边的距离，使得SAST可以检测弯曲文本行、中间有间隔的文本行</p><a id="more"></a><h1 id="什么是-SAST-？"><a href="#什么是-SAST-？" class="headerlink" title="什么是 SAST ？"></a>什么是 SAST ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204412.png" alt=""></li><li>属于 <a href="EAST.md">EAST</a> 的演进版本，还是类似 anchor-free 的方式预测文本行，但是除了输出 grid 的 score +边框预测外，还输出更多的文本实例信息，比如 grid 到实例矩形四角、中心点、四边的距离</li><li>每个 grid 更加复杂的输出，可以让 SAST 检测更为复杂场景下的文本行，比如弯曲文本行、中间有间隔的文本行</li></ul><h1 id="SAST-的网络结构？"><a href="#SAST-的网络结构？" class="headerlink" title="SAST 的网络结构？"></a>SAST 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204443.png" alt=""></li><li><strong>Featrue Extractor</strong>：BackBone 部分，通过类似 SegNet 的过程提取特征</li><li><strong>CABs</strong>：交叉注意力模块，用于整合 BackBone 的特征</li><li><strong>TCL map (1 xHxW)</strong>: grid 属于文本中心线像素点的概率</li><li><strong>TCO map (2 xHxW)</strong>: 文本中心点偏置，grid 距其所属的文本实例矩形框中心的 xy 方向距离</li><li><strong>TVO map (8 xHxW)</strong>: 文本四顶点偏置，grid 距其所属的文本实例矩形框四顶点的 xy 方向距离</li><li><strong>TBO map (4 xHxW)</strong>: 文本边界偏置，grid 距其所属的文本实例上下边界框的 xy 方向距离</li></ul><h1 id="SAST-的-CAB-模块？"><a href="#SAST-的-CAB-模块？" class="headerlink" title="SAST 的 CAB 模块？"></a>SAST 的 CAB 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204535.png" alt=""></li><li>交叉注意力模块，用于整合 BackBone 的特征，该模块分为上下两部分，上部分构建水平方向注意力，下部分构建垂直方向注意力，整合水平方向注意力和垂直方向注意力得到<strong>全局注意力</strong></li></ul><h1 id="SAST-样本制作？"><a href="#SAST-样本制作？" class="headerlink" title="SAST 样本制作？"></a>SAST 样本制作？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204602.png" alt=""> </li><li><strong>a). TCL map (1 xHxW)</strong>：文本中心线区域，文本行上下边界收缩 20%后得到的区域，而左右边界仍保持不变</li><li><strong>b). TBO map (4 xHxW)</strong>：文本边界偏置，首先计算斜率 k 1 (v 1, v 2)与斜率 k 1 (v 4, v 3)的平均值，对于一个给定的点 P 0，可容易地计算出斜率为 (k 1+k 2)/2、过点 P 0 的直线，由此该直线与线段 (v 1, v 4)和线段 (v 2, v 3)的交点 P 1 与 P 2 很容易得出，故 P 0 的上下边界点 $P<em>{upper}$ 和 $P</em>{lower}$ 的坐标可由线段比例关系得到，整理得到 P 0 点到四边距离的 TBO 为{$P<em>0^x-P_1^x$、 $P_0^x-P</em>{lower}^x$ 、$P<em>2^y-P_0^y$、$P</em>{upper}^y-P_0^y$}</li><li><strong>c). TVO map (8 xHxW)</strong>：文本顶点偏置，文本最小矩形框按根据一定规则由文本标注信息计算得到，计算文本中心区域中某像素点到文本矩形框四顶点的直线距离（包括 x 方向和 y 方向），所以共计给每个 grid 生成 8 个 TVO 预测</li><li><strong>d). TCO map (2 xHxW)</strong>：文本中心点偏置，计算文本中心区域内某像素点到文本最小矩形框中心点的距离 (x 方向和 y 方向)</li></ul><h1 id="SAST-的损失函数？"><a href="#SAST-的损失函数？" class="headerlink" title="SAST 的损失函数？"></a>SAST 的损失函数？</h1><ul><li><script type="math/tex; mode=display">L_{total}=\lambda_1L_{tcl}+\lambda_2L_{tco}+\lambda_3L_{tvo}+\lambda_4L_{tbo},</script></li><li><strong>(1) TCL map:</strong> 使用 Minimizing the Dice loss 作为分割 loss, 用于描述两个轮廓的相似程度</li><li><strong>(2) TVO/TCO/TBO:</strong> 使用 Smooth L 1 Loss 作为几何图 geometry map 的回归 loss</li></ul><h1 id="解析-SAST-的输出-1-生成文本实例？"><a href="#解析-SAST-的输出-1-生成文本实例？" class="headerlink" title="解析 SAST 的输出 1-生成文本实例？"></a>解析 SAST 的输出 1-生成文本实例？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204913.png" alt=""></li><li><strong>a)</strong> <strong>根据 TCL 获得文本实例包含的像素点-&gt;文本行 Mask</strong>，阈值过滤将置信率低于某值的假阳性像素点剔除，得到合适的 TCL map;</li><li><strong>b) 根据 TVO+NMS 获得文本实例-&gt;文本矩形框</strong>：将经过处理的 TCL map 中每个像素点，根据 TVO 文本实例顶点偏置图，得到对应的文本矩形框四顶点坐标，并进行非最大值抑制 NMS，得到所需的文本实例矩形框及其中心点</li><li><strong>b+c=d) 根据 TCO 合并文本实例-&gt;文本行 Mask</strong> ：计算 TCL 中属于文本的像素点的所属文本实例的几何中心点，该中心点将作为低层级像素信息，当步骤 c 计算所得的几何中心点与步骤 b 所得矩形框中心点重合或相近时，该像素点将被归类给步骤 b 中矩形框对应的文本实例，通过此步骤重新合并断开的文本行</li></ul><h1 id="解析-SAST-的输出-2-生成文本边框？"><a href="#解析-SAST-的输出-2-生成文本边框？" class="headerlink" title="解析 SAST 的输出 2-生成文本边框？"></a>解析 SAST 的输出 2-生成文本边框？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204412.png" alt=""></li><li><strong>a)</strong> 前面解析得到的文本实例</li><li><strong>b)</strong> 对文本中心线采样，采样点的间距相同，则得到的采样点数目与文本线的长度有关，故称之为自适应采样</li><li><strong>c)</strong> 根据文本边界偏置图 TBO 所提供的信息，计算文本中心线的采样点上的上下边界定位点</li><li><strong>d)</strong> 将步骤 b 所得的边界定位点按照从左上角开始的顺时针方向依次进行连接，得到最终的文本边界框</li></ul><p>参考：</p><ol><li><a href="https://blog.csdn.net/qq_37546096/article/details/102909850">SAST : Single-Shot Arbitrarily-Shaped Text Detector论文阅读笔记_text center line sampling_litchi9854的博客-CSDN博客</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;属于EAST的演进版本，还是类似 anchor-free 的方式预测文本行，除了输出 grid 的 score +边框预测外，还输出更多的文本实例信息，比如 grid 到实例矩形四角、中心点、四边的距离，使得SAST可以检测弯曲文本行、中间有间隔的文本行&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文本检测" scheme="https://shaogui.life/tags/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>FOTS：Fast Oriented Text Spotting with a Unified Network</title>
    <link href="https://shaogui.life/2023/04/09/FOTS%EF%BC%9AFast%20Oriented%20Text%20Spotting%20with%20a%20Unified%20Network/"/>
    <id>https://shaogui.life/2023/04/09/FOTS%EF%BC%9AFast%20Oriented%20Text%20Spotting%20with%20a%20Unified%20Network/</id>
    <published>2023-04-09T13:42:24.000Z</published>
    <updated>2023-04-09T14:28:39.081Z</updated>
    
    <content type="html"><![CDATA[<p>第一个<strong>端到端</strong>解决文本识别的模型，相比较两阶段的文本定位方法，它的检测速度更快，基本思路是通过文本检测分支实现文本行区域的提取，然后通过 RoIRotate 模块实现文本行的“摆正”，最后使用 CRNN+CTC 的模式实现文本行的字符识别</p><a id="more"></a><h1 id="什么是-FOTS-？"><a href="#什么是-FOTS-？" class="headerlink" title="什么是 FOTS ？"></a>什么是 FOTS ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304092146678.png" alt=""></li><li>第一个<strong>端到端</strong>解决文本识别的模型，相比较两阶段的文本定位方法，它的检测速度更快，基本思路是通过文本检测分支实现文本行区域的提取，然后通过 RoIRotate 模块实现文本行的“摆正”，最后使用 CRNN+CTC 的模式实现文本行的字符识别</li><li>RoIRotate 模块要通过仿射变换转换文本区域，所以 FOTS 只能识别文字中心在一个线上的文本行，无法处理弯曲文本行</li></ul><h1 id="FOTS-的网络结构？"><a href="#FOTS-的网络结构？" class="headerlink" title="FOTS 的网络结构？"></a>FOTS 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304092146065.png" alt=""></li><li><strong>shared convolutions</strong>：使用 Resnet 搭建，首先使用下采样，然后使用反卷积上采样，并且使用类似 SegNet 的高分辨率连接到低分辨率的连接</li><li><strong>文本检测分支</strong>：使用类似 anchor-free 的目标检测方式，将 shared convolutions 的每个 grid 视为文本行中心，然后预测其文本行的宽高</li><li><strong>RoIRotate</strong>：根据文本检测分支的输出+shared convolutions 输出，将文本行转为横向文本</li><li><strong>文字识别分支</strong>：基于 CRNN+CTC 的方式学习和识别文本行</li></ul><h1 id="FOTS-的-shared-convolutions-模块？"><a href="#FOTS-的-shared-convolutions-模块？" class="headerlink" title="FOTS 的 shared convolutions 模块？"></a>FOTS 的 shared convolutions 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/FOTS-20230409194643.png" alt=""></li><li>首先通过 ResNet 提取特征，然后通过反卷积上采样，类似 SegNet 一样中间使用残差连接，最后输出 C 2 特征</li></ul><h1 id="FOTS-的文本检测分支？"><a href="#FOTS-的文本检测分支？" class="headerlink" title="FOTS 的文本检测分支？"></a>FOTS 的文本检测分支？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-09%2020.21.26.excalidraw.svg" alt="Drawing 2023-04-09 20.21.26.excalidraw"></li><li>使用类似 anchor-free 的目标检测方式，将 shared convolutions 的每个 grid 视为文本行中心，然后预测其文本行的宽高</li><li>假设 shared convolutions 输出是 $C\times H \times W$ 的特征，文本检测分支输出 3 个分支，分别表示文本行的得分、该 grid 到四边的距离和该文本行的旋转角度</li></ul><h1 id="FOTS-的-RoIRotate-模块？"><a href="#FOTS-的-RoIRotate-模块？" class="headerlink" title="FOTS 的 RoIRotate 模块？"></a>FOTS 的 RoIRotate 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/FOTS-20230409194714.png" alt=""></li><li>使用目标检测的后处理获得文本行，根据文本行的宽高及旋转角得到四个角点的位置，假设四个点是 ($x_1$, $y_1$)、($x_2$, $y_2$)、($x_3$, $y_3$)、($x_4$, $y_4$)，现在要将这个区域转到 (0,0)起点，宽高 (wh)的区域，可以通过仿射变换实现</li><li>仿射变换矩阵需要变换前后的 3 对点求得，不妨取 ($x_1$, $y_1$)-&gt;(0,0)、($x_2$, $y_2$)-&gt;（w, 0）、($x_3$, $y_3$)-&gt;(w, h)，求取方法是调用 opencv 的 getAffineTransform 函数即可，仿射矩阵变换后，文本的中心线平行 x 轴</li></ul><h1 id="FOTS-的文字识别分支？"><a href="#FOTS-的文字识别分支？" class="headerlink" title="FOTS 的文字识别分支？"></a>FOTS 的文字识别分支？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/CRNN-20230408144101.png" alt=""></li><li>文字识别是在 RoIRotate 模块输出的基础上进行的，就是得到平行文本行的基础上进行的，其过程有 4 个</li><li><strong>CNN 提取特征</strong>：使用轻量化网络 MobileNetv 3，其中输入图像的高度统一设置为 32，宽度可以为任意长度，经过 CNN 网络后，特征图的高度缩放为 1</li><li><strong>双向 LSTM（BiLSTM）对特征序列进行预测</strong>：学习序列中的每个特征向量并输出预测标签分布。这里其实相当于把特征向量的宽度视为 LSTM 中的时间维度</li><li><strong>全连接层分类</strong>：使用全连接层对每个序列进行 N+1 类别预测，获取模型的预测结果</li><li><strong>CTC</strong>：解码模型输出的预测结果，得到最终输出</li></ul><h1 id="FOTS-的损失函数？"><a href="#FOTS-的损失函数？" class="headerlink" title="FOTS 的损失函数？"></a>FOTS 的损失函数？</h1><ul><li>网络的损失分为两部分，即文本行识别损失 $L<em>{detect}$ 、文本行字符识别损失 $L</em>{recog}$，通过参数 $\lambda_{recog}$ 控制两者的权重<script type="math/tex; mode=display">L=L_{\mathbf{detect}}+\lambda_{\mathbf{recog}}L_{\mathbf{recog}}</script></li></ul><p>参考：</p><ol><li><a href="https://blog.csdn.net/sol_data12/article/details/113501530">场边文字检测——FOTS模型详解及其代码实现_ManManMan池的博客-CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/195248125">[论文笔记] FOTS - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;第一个&lt;strong&gt;端到端&lt;/strong&gt;解决文本识别的模型，相比较两阶段的文本定位方法，它的检测速度更快，基本思路是通过文本检测分支实现文本行区域的提取，然后通过 RoIRotate 模块实现文本行的“摆正”，最后使用 CRNN+CTC 的模式实现文本行的字符识别&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文本定位" scheme="https://shaogui.life/tags/%E6%96%87%E6%9C%AC%E5%AE%9A%E4%BD%8D/"/>
    
  </entry>
  
  <entry>
    <title>文本检测之DB和DB++</title>
    <link href="https://shaogui.life/2023/04/08/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E4%B9%8BDB%E5%92%8CDB++/"/>
    <id>https://shaogui.life/2023/04/08/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E4%B9%8BDB%E5%92%8CDB++/</id>
    <published>2023-04-08T07:03:50.000Z</published>
    <updated>2023-05-20T04:05:32.477Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091513112.png" alt=""></p><p>本文介绍两个文本检测模型： DB 及它的升级版本 DB++，主要的原理就是将“二值化”的过程做成网络可学习、可微分的一个模块，然后向网络中插入这个模块自适应去学习二值化阈值，最终预测文字区域的一个核心部分，然后再通过公式放大这个核心部分，得到目标区域</p><a id="more"></a><h1 id="什么是-DB-？"><a href="#什么是-DB-？" class="headerlink" title="什么是 DB ？"></a>什么是 DB ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091513112.png" alt=""></li><li><strong>传统意义二值化</strong>：基于分割的文本检测算法其流程如图2中的蓝色箭头所示。在传统方法中得到分割结果之后采用一个<strong>固定阈值</strong>得到二值化的分割图</li><li><strong>DB 二值化</strong>：如图2中红色箭头所示的，通过网络去预测图片每个位置处的阈值，而不是采用一个固定的值，这样就可以很好将背景与前景分离出来，但是这样的操作会给训练带来梯度不可微的情况，对此对于二值化提出了一个叫做 <strong>Differentiable Binarization 模块</strong>来解决</li></ul><h1 id="DB-的网络结构？"><a href="#DB-的网络结构？" class="headerlink" title="DB 的网络结构？"></a>DB 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091633772.png" alt=""></li><li><strong>BackBone</strong>：可以使用类似 resnet 下采样 5 次，得到 5 个层次的特征</li><li><strong>FPN 类似结构</strong>：对 C4、C3、C2 特征采样类似 FPN 的连接，输出时是 C5、 F4、F3、F2 一共 4 个层次的特征</li><li><strong>DB 模块</strong>：以 probability map 减去 threshold map (T) 差值，输入到 DB 模块进行可微的二值化学习</li></ul><h1 id="DB-的“可微的二值化模块-DB-”？"><a href="#DB-的“可微的二值化模块-DB-”？" class="headerlink" title="DB 的“可微的二值化模块 (DB)”？"></a>DB 的“可微的二值化模块 (DB)”？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091634515.png" alt=""></li><li>上图 a、b、c 分别是标准二值化与可微二值化输出、可微二值化对正样本的梯度，可微二值化对负样本的梯度，k 是放大倍数</li><li><strong>标准二值化 (SB)</strong>：通过预先设置的阈值 t 去对概率图 $P_{i,j}$ 二值化<script type="math/tex; mode=display">B_{i,j}=\begin{cases}1\quad P_{i,j}>=t\\ 0\quad otherwise\end{cases}</script></li><li><strong>可微二值化 (DB)</strong>：借鉴 sigmoid 输出输出，将 $P<em>{i,j}-T{i,j}$ 作为 sigmoid 输入，并 K 扩大输出，使得 $\hat{B}</em>{i,j}$ 趋向 0 或 1，即通过学习每个位置的阈值 $T<em>{i, j}$ 对概率图 $P</em>{i, j}$ 二值化 <script type="math/tex; mode=display">\hat{B}_{i, j}=\dfrac{1}{1+\exp^{-k (P_{i, j}-T_{i, j})}}</script></li><li><strong>正负样本梯度比较</strong>：DB 改进性能的原因可以通过梯度的反向传播来解释，可知正负样本的梯度被 k 放大 <script type="math/tex; mode=display">\begin{aligned}l_{+}=-log\frac{1}{1+e^{-}k x}\quad =>  \frac{\partial l_{+}}{\partial x}=-k f(x)e^{-k x}\\\\ l_{-}=-log(1-\frac{1}{1+e^{-}k x})\quad\quad =>  \frac{\partial l_{-}}{\partial x}=k f(x)\end{aligned}</script></li></ul><h1 id="DB-的自适应阈值？"><a href="#DB-的自适应阈值？" class="headerlink" title="DB 的自适应阈值？"></a>DB 的自适应阈值？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091635325.png" alt=""></li><li>a、b、c、d 分别是原图、probability map、无监督的 threshold map、有监督的 threshold map</li><li>c 图表明即使没有对 threshold map 监督，其结果也会表现出突出显示文本边界区域。这表明如果加入类似边界的监督，以提供更好的指导，d 图的结果证明了这一点</li></ul><h1 id="DB-的标签生成过程？"><a href="#DB-的标签生成过程？" class="headerlink" title="DB 的标签生成过程？"></a>DB 的标签生成过程？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091635472.png" alt=""></li><li><strong>probability map</strong>：使用 Vatti clipping algorithm 将 G 缩减到 Gs（蓝线内部），A 是面积，r 是 shrink ratio，设置为0.4，L 是周长 <script type="math/tex">D=\dfrac{A(1-r^2)}{L}</script></li><li><strong>threshold map</strong>：使用生成 probability map 一样的方法，向外进行扩张，得到绿线和蓝线中间的区域，根据到红线的距离制作标签</li><li><strong>binary map</strong>：蓝色标注线以内</li></ul><p>总结：以上 3 个标签的值范围</p><div class="table-container"><table><thead><tr><th>-</th><th>蓝线以内</th><th>蓝蓝绿之间</th><th>其他</th></tr></thead><tbody><tr><td>probability map</td><td>1</td><td>0</td><td>0</td></tr><tr><td>threshold map</td><td>0.3</td><td>越靠近红线 0.7，越远离红线 0.</td><td>0.3</td></tr><tr><td>binary map</td><td>1</td><td>0</td><td>0</td></tr></tbody></table></div><p><strong><mark style="background: #FF5582A6;">从上面标签制作可知，DB 没有直接去学习文本的边缘（图红线），而是去学习比文本边缘更小的区域 (图绿线)，我觉得这点是除了”可微二值化模块”外，尤其需要关注的地方。这里说一下自己的理解</mark></strong></p><h1 id="DB-为什么不直接学习文本外轮廓，而是学习轮廓缩小的轮廓？"><a href="#DB-为什么不直接学习文本外轮廓，而是学习轮廓缩小的轮廓？" class="headerlink" title="DB 为什么不直接学习文本外轮廓，而是学习轮廓缩小的轮廓？"></a>DB 为什么不直接学习文本外轮廓，而是学习轮廓缩小的轮廓？</h1><ul><li><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091649041.png" alt="Drawing 2023-04-09 15.43.02.excalidraw"></p></li><li><p>上图是两种学习路线下的 probability map、threshold map 及他们学习的 binary map，其中红线是直接学习文本边缘（下文称直觉模式），绿线学习文本边缘小一圈的轮廓（下文称 DB 模式）</p></li><li><strong>观察 probability map</strong>，“直觉模式”比 DB 模式范围更大，这对极度弯曲的小文本是不友好的，可以想象文本在 C2特征已经辨别不出弯曲，更小的学习区域可以有更强的能力</li><li><strong>观察 threshold map</strong>，因为文本行占据了图片大部分区域，所以“直觉模式”主要优化背景到 0.7， threshold map 计算 L1 损失，相比较 DB 模式大部分优化背景到 0.3，“直觉模式”更难优化</li><li><strong>观察 binary map</strong>：除了和优化 threshold map 同样的问题外，由于“直觉模式”对文本行内、外的梯度大小一样，说明两个区域优化权重一样。而 DB 模式内部梯度比外部梯度更大，相当于增大正样本的梯度权重</li><li><strong>总结</strong>：“直觉模式”比 DB 模式更难优化，而且 DB 模式对弯曲小文本性能更好</li></ul><h1 id="DB-的损失函数？"><a href="#DB-的损失函数？" class="headerlink" title="DB 的损失函数？"></a>DB 的损失函数？</h1><ul><li>$L_s$ 是 probability map 的 loss，$L_b$ 是 binary map 的 loss，$L_t$ 是 threshold map 的 loss，$\alpha$ 和 $\beta$ 设置为1和10，$L_s$ 和 $L_b$ 使用交差熵计算损失<script type="math/tex; mode=display">L=L_s+\alpha\times L_b+\beta\times L_t</script></li><li>$S_l$ 表示使用 OHEM 进行采样，正负样本的比例为1：3, $L_t$ 使用 L 1 loss，$R_d$ 表示绿线内的区域，<script type="math/tex; mode=display">L_t=\sum_{i\in R_d}|y_i^*-x_i^*|</script></li></ul><h1 id="DB-如何解析输出的？"><a href="#DB-如何解析输出的？" class="headerlink" title="DB 如何解析输出的？"></a>DB 如何解析输出的？</h1><ul><li>在推理阶段，可以使用 binary map 或者 probability map</li><li><strong>使用 binary map</strong>：需要 probability map+threshold map 两个分支计算得到，其结果就是文本实例</li><li><strong>使用 probability map</strong>：不需要 threshold map、binary map 分支，直接按照 Vatti clipping algorithm 公式还原回去即可，即1)使用0.3的阈值进行二值化；2)将 pixel 连接成不同的文本实例；3)将文本实例进行扩张，得到最终的文本框<script type="math/tex; mode=display"> D^{'}=\dfrac{A^{'}(1-r^{'})}{L^{'}}</script></li><li>使用第二种方法，网络计算更少，论文使用第二种方法</li></ul><h1 id="DB-的网络结构？-1"><a href="#DB-的网络结构？-1" class="headerlink" title="DB++ 的网络结构？"></a>DB++ 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091649944.png" alt=""></li><li><strong>BackBone</strong>：可以使用类似 resnet 下采样 5 次，得到 5 个层次的特征</li><li><strong>ASF 模块</strong>：ASF 特征融合模块其实就是 FPN，只不过在此基础上增加Spatial Attention</li><li><strong>DB 模块</strong>：和 DB 一样，以 probability map 减去 threshold map (T) 差值，输入到 DB 模块进行可微的二值化学习</li></ul><h1 id="DB-的自适应多尺度特征融合模块-ASF-？"><a href="#DB-的自适应多尺度特征融合模块-ASF-？" class="headerlink" title="DB++的自适应多尺度特征融合模块 ASF ？"></a>DB++的自适应多尺度特征融合模块 ASF ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091650873.png" alt=""></li><li><strong>输入输出</strong>：输入是 BackBone 4 个层次的特征，输出是经过加权的特征</li><li><strong>Spatial Attention</strong>：对特征加空间注意力，使用空间（沿通道方向）进行池化，得到注意力矩阵 $1\times H\times W$</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091513112.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文介绍两个文本检测模型： DB 及它的升级版本 DB++，主要的原理就是将“二值化”的过程做成网络可学习、可微分的一个模块，然后向网络中插入这个模块自适应去学习二值化阈值，最终预测文字区域的一个核心部分，然后再通过公式放大这个核心部分，得到目标区域&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文本检测" scheme="https://shaogui.life/tags/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>CNN之结构重参数化</title>
    <link href="https://shaogui.life/2023/04/07/CNN%E4%B9%8B%E7%BB%93%E6%9E%84%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96/"/>
    <id>https://shaogui.life/2023/04/07/CNN%E4%B9%8B%E7%BB%93%E6%9E%84%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96/</id>
    <published>2023-04-07T04:40:44.000Z</published>
    <updated>2023-05-20T04:05:40.772Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是结构重参数化？"><a href="#什么是结构重参数化？" class="headerlink" title="什么是结构重参数化？"></a>什么是结构重参数化？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005953.png" alt=""></li><li>我们知道模型要变好，就必须构建得更加复杂，但是这来这带来一个坏处，就是模型部署的耗时会增长，这两者是相互矛盾的，<strong>结构从参数化</strong>就是两者都可以做到，在训练的时候，通过复杂的神经网络去训练，提升模型的性能，但是在推理的时候，我通过对模型结构的重参数化生成了一个更加精简的结构，使推理的时候速度更快</li><li><strong>RepVGG 的结构重参数化过程</strong>：上图是左边是训练时的卷积网络，右边通过对结构进行重参数化，得到一个只有 1 个分支的结构，因此可以做到训练时提升性能，推理时提升速度</li><li><strong>结构从参数化的基本原理</strong>：<strong>卷积的可加性</strong>，对于同一个输入，只要其扫描频率一致（相同的通道数、kernel size、stride、padding），其卷积可过程可以融合。如下公式 1 是一个实数乘特征图和乘卷积是等效的，公式 2卷积核 F1 与 F2 可以被融合为 1 个卷积，卷积核为 (F1+F2) </li><li><script type="math/tex; mode=display">\begin{matrix}I\otimes(pF)=p(I\otimes F) \quad&(1)\\I\otimes F^{(1)}+I\otimes F^{(2)}=I\otimes(F^{(1)}+F^{(1)})\quad&(2)\end{matrix}</script></li></ul><h1 id="ACNet-的网络结构？"><a href="#ACNet-的网络结构？" class="headerlink" title="ACNet 的网络结构？"></a>ACNet 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005292.png" alt=""></li><li>上图左边展示 3 个分支的卷积融合为一个等效卷积的过程；右边是卷积融合的过程，主要包括融合 BN (BN fusion) 和融合分支 (branch fusion)两个步骤</li><li><strong>融合 BN (BN fusion)</strong> ：所有 BN 对输入操作一样，不改变输入分辨率，所以利用卷积的线性可加性，将 BN 的过程融合进卷积</li><li><strong>融合分支 (branch fusion)</strong>：同样利用卷积的线性可加性，将多分支的卷积融合为 1个卷积</li></ul><h1 id="RepVGG-如何进行“结构重参数化”？"><a href="#RepVGG-如何进行“结构重参数化”？" class="headerlink" title="RepVGG 如何进行“结构重参数化”？"></a>RepVGG 如何进行“结构重参数化”？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005549.png" alt=""></li><li>1.<strong>卷积层参数</strong>：上图是 REP-VGG 块的结构重参数化过程。为了易于可视化，我们假设 C2 = C1 = 2，因此3×3层具有四个3×3矩阵，而1×1层的核为2×2矩阵</li><li>2.<strong>BN 层参数</strong>：(1)可知 BN 层为每个通道的数据进行规范化，每个通道需要 4 个参数 $\:\mu,\:\sigma,\:\gamma,\:\beta$，输入通道 2 个则有 8 个参数；(2) 当 $\:\mu,\:\sigma,\:\gamma,\:\beta$ 均为 0 时，规范化后的数据还是原来的值，这可以用于模拟 identity 路径</li><li>3.<strong>融合卷积与 BN 层</strong>：(1)最难理解的是虚线红框部分，由原来的 $2\times 1 \times 1\times 2$ 变为 $2\times 3 \times 3\times 2$ ，也就是单个卷积核由 $1\times 1$ 变为 $3\times 3$，这是通过在 $1\times 1$ 四周补 0 做到的，因为补 0 后得到的卷积和是不变的；(2) 类似 [[ACNet#^udwpgu|ACNet的网络结构]]的过程，$\:\mu,\:\sigma,\:\gamma,\:\beta$ 的部分参数用于重构卷积核的值，部分参数组合成卷积的偏置值 $b$，并且每个通道 1 个值 <img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005037.png" alt=""></li><li>4.<strong>利用卷积的可加性，融合多路径</strong>：对应同 size 卷积核的，可以利用卷积的可加性，将卷积融合，具体来说是卷积核矩阵对应相加，偏置值对应相加</li></ul><h1 id="DBB-的网络结构？"><a href="#DBB-的网络结构？" class="headerlink" title="DBB 的网络结构？"></a>DBB 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005802.png" alt=""></li><li>参考 [[GoogleNetv1]] 的 Inception block 的概念，结合结构重参数划的理论，设计了 DBB block</li><li>每个 DBB block 包含 4 个并行的路径，推理时融合成 1 个路径</li></ul><h1 id="DBB-的-6-种模块可以等价转为单个卷积？"><a href="#DBB-的-6-种模块可以等价转为单个卷积？" class="headerlink" title="DBB 的 6 种模块可以等价转为单个卷积？"></a>DBB 的 6 种模块可以等价转为单个卷积？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082006464.png" alt=""></li><li><ol><li>Conv-BN 合并：经典的卷积层融合 BN 层的结构</li></ol></li><li><ol><li>并行合并：参考ACNet的网络结构，卷积核可线性相加</li></ol></li><li><ol><li>串行合并：参考ACNet的网络结构，卷积核可线性相加</li></ol></li><li><ol><li>并行拼接：参考ACNet的网络结构，卷积核 kernel size 保持不变，数量是两个分支相加</li></ol></li><li><ol><li>平均池化转换：平均池化很像卷积核的过程，只不过是求和后加平均而已，直接对卷积核的值除 KxK，后面得到的卷积和就是 AVG 后的值了</li></ol></li><li><ol><li>多尺度卷积合并：参考ACNet的网络结构，同一将卷积核扩充为 KxK，再进行融合</li></ol></li></ul>]]></content>
    
    
    <summary type="html">结构重参数化的原理</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="CNN" scheme="https://shaogui.life/tags/CNN/"/>
    
    <category term="结构重参数化" scheme="https://shaogui.life/tags/%E7%BB%93%E6%9E%84%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>人工神经网络(ANN)的理解</title>
    <link href="https://shaogui.life/2023/03/01/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(ANN)%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://shaogui.life/2023/03/01/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(ANN)%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2023-03-01T02:23:01.000Z</published>
    <updated>2023-05-20T04:32:47.236Z</updated>
    
    <content type="html"><![CDATA[<p>本文用于解释最基础的神经网络-感知机的原理，它是后续 CNN、RNN 的基础，CNN 只是在感知机的概念加入“局部连接”的思想，每个局部执行的还是加权和；RNN 只是给每个隐藏层加一个隐状态</p><p>本文按照：感知机-&gt;多层感知机-&gt;全连接层-&gt;人工神经网络的步骤去理解 Linear 层</p><a id="more"></a><h1 id="什么是感知机-Perceptron-？"><a href="#什么是感知机-Perceptron-？" class="headerlink" title="什么是感知机 (Perceptron)？"></a>什么是感知机 (Perceptron)？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-26%2010.55.05.excalidraw.png" alt="Drawing 2023-03-26 10.55.05.excalidraw"></li><li><strong>一种用于单类别分类监督学习的算法</strong>，输入一组特征向量，然后通过一组等长权重计算线性加权和，最后通过判别函数输出结果。如果使用以下判别函数，则是一个二元分类器 <script type="math/tex; mode=display">f(x)=\operatorname{sign}(w * x+b)=\left\{\begin{array}{ll} +1 & x \geq 0 \\ -1 & x<0 \end{array}\right.</script></li><li>感知机模拟的是人的神经细胞，单个神经细胞有 2 种状态，激活与不激活，当信号总量超过某个阈值时，该神经细胞处于激活状态，否则不激活，激活时向其他连接的神经元传递信息</li></ul><p>一个感知机包含等于输入数据长度的权重 w 和 1 个偏置 b（因为输出是 1），所谓感知，就是利用权重对所有输入进行加权和操作。接下来同时使用多个感知机去感知输入，就构成“单层感知机 (SLP)”</p><h1 id="什么是单层感知机-SLP"><a href="#什么是单层感知机-SLP" class="headerlink" title="什么是单层感知机 (SLP)?"></a>什么是单层感知机 (SLP)?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-26%2010.55.05.excalidraw.png" alt="Drawing 2023-03-26 10.55.05.excalidraw"></li><li>所谓单层感知机，就是<strong>由感知机组成的一层网络，没有多层感知机的 hidden layer，每个感知机连接都连接所有输入</strong>，注意：感知机本身可以理解为只有 1 个节点的单层感知机</li></ul><p>除了在单层使用多个感知机外，还可以使用多层感知机，每层感知机的输入是前一层前一层感知机的输出，由此出现“多层感知机 (MLP)”</p><h1 id="什么是多层感知机-MLP"><a href="#什么是多层感知机-MLP" class="headerlink" title="什么是多层感知机 (MLP)?"></a>什么是多层感知机 (MLP)?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-26%2010.55.05.excalidraw.png" alt="Drawing 2023-03-26 10.55.05.excalidraw"></li><li><strong>将多个单层感知机堆叠起来，其中每个感知机机与所有输入或者所有感知机输出连接，输入层与输出层之间的所有层是隐藏层</strong></li><li><p>MLP 至少由 3 层感知机组成，即<strong>输入层、隐藏层、输出层</strong></p></li><li><p>感知机、单层感知机、多层感知机的区别？</p><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(ANN" alt="人工神经网络(ANN)-20230408150310">-20230408150310.png)</li><li><strong>感知机 (Perceptron)</strong>：左图是一个 2 输入的感知机模型，经过参数加权累加和后，其值在一个<strong>平面</strong>上</li><li><strong>单层感知机 (SLP)</strong>：中图是一个 2 输入、包含 3 个输出节点的单层感知机，经过参数加权累加和后输出，输出是超平面</li><li><strong>多层感知机 (MLP)</strong>：多个单层感知机组成，至少包括 1 层隐藏层，输出结果是多个多个<strong>超平面</strong>的组合</li><li><strong>感知机无论叠加多少层，只要没使用<mark style="background: #FF5582A6;">非线性激活函数</mark>，其结果都是线性可分的，无法处理非线性问题</strong></li></ul></li></ul><p>但是使用任意个、任意层感知机都是线性函数 $Wx+b$ 的线性叠加，其处理的问题都是线性可分的，但是对于线性不可分问题无法解决</p><h1 id="什么是线性可分？"><a href="#什么是线性可分？" class="headerlink" title="什么是线性可分？"></a>什么是线性可分？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-26%2014.23.58.excalidraw.svg" alt="Drawing 2023-03-26 14.23.58.excalidraw"></li><li><strong>线性可分</strong>：计算所有样本的凸包，如果凸包内值包含 1 个类别，就是线性可分，本质就是数据可以被有限个超平面区分，在 2D 数据上就是被有限个直线做划分</li><li><strong>线性不可分</strong>：计算所有样本的凸包，如果凸包内值包含 2 个类别以上，则是非线性可分</li></ul><p>日常构建网络时，都是使用 <code>torch.Linear</code> 去构建全连接层，它和感知机是什么关系呢，实际上它是确定输入维度、输出维度的单层感知机</p><h1 id="什么是全连接层-Fully-Connected-Layer"><a href="#什么是全连接层-Fully-Connected-Layer" class="headerlink" title="什么是全连接层 (Fully Connected Layer)?"></a>什么是全连接层 (Fully Connected Layer)?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-26%2015.29.48.excalidraw.svg" alt="Drawing 2023-03-26 15.29.48.excalidraw"></li><li>其实<strong>就是单层的感知机</strong>，任意一个感知机都与所有输入连接形成，定义时需要指定输入数据维度和感知机数量，其中感知机的数量等于输出的维度</li><li><strong>参数量</strong>：每个感知机都与输入一一相连，加上偏置，共用参数 $\mathrm{Param}<em>{\text {linear }}=C</em>{\text {in }} \times C<em>{\text {out }}+C</em>{\text {out }}$</li><li><strong>计算量</strong>：全连接层的<strong>每个输出</strong>需要经过 $C<em>{in}$ 的乘法和加法运算，没考虑 bias 需要减1，$F L O P</em>{\text {linear }}=(2 \times C<em>{\text {in }}) \times C</em>{\text {out }} \times Batchsize$</li><li>Pytorch 可以使用接口快速定义该层<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=torch.randn([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">fc = torch.nn.Linear(in_features = <span class="number">2</span>, out_features = <span class="number">3</span>)</span><br><span class="line">y=fc(x) // y=[<span class="number">1</span>,<span class="number">3</span>]</span><br></pre></td></tr></table></figure>单层感知机加上激活函数，就可以构建人工神经网络 (ANN) </li></ul><h1 id="什么是人工神经网络-ANN-？"><a href="#什么是人工神经网络-ANN-？" class="headerlink" title="什么是人工神经网络 (ANN) ？"></a>什么是人工神经网络 (ANN) ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/MenuqjAChEyAwrQdOWJoWi4FHpKQ551d1d2kcUoW7jknzv9F_i4-MHztszxy1H4fPSoqPKdjNO51C3mZKtlppVQljk-iYKTbFmNPxm19CYwxSlf0o6oMtwmI4Uq1ma2p.gif" alt=""></li><li>ANN 其实就是在多层感知器的基础上引入”非线性”激活函数，使得 ANN 可以模拟函数逼近器的作用，用于学习数据特征</li><li>缺点：1）随着图像大小的增加，可训练参数增多；2）会丢失图像的空间特征；3）处理序列数据无法记忆历史信息</li><li>Pytorch 可以堆叠多层的”FC+relu”层实现<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=torch.randn([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">fc = torch.nn.Linear(in_features = <span class="number">2</span>, out_features = <span class="number">3</span>)</span><br><span class="line">y=fc(x) // y=[<span class="number">1</span>,<span class="number">3</span>]</span><br><span class="line">y=torch.nn.functional.relu(y)</span><br></pre></td></tr></table></figure></li></ul><p>参考：</p><ol><li><a href="https://luweikxy.gitbook.io/machine-learning-notes/artificial-neural-network#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%9C%E7%94%A8">ANN人工神经网络 - machine-learning-notes</a></li><li><a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multilayer perceptron - Wikipedia</a></li><li><a href="https://www.cnblogs.com/lgdblog/p/6858832.html">线性可分 与线性不可分 - Amazing_Man - 博客园</a></li><li><a href="https://www.zhihu.com/question/593941226">transformer为什么有利于并行计算？ - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文用于解释最基础的神经网络-感知机的原理，它是后续 CNN、RNN 的基础，CNN 只是在感知机的概念加入“局部连接”的思想，每个局部执行的还是加权和；RNN 只是给每个隐藏层加一个隐状态&lt;/p&gt;
&lt;p&gt;本文按照：感知机-&amp;gt;多层感知机-&amp;gt;全连接层-&amp;gt;人工神经网络的步骤去理解 Linear 层&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="ANN" scheme="https://shaogui.life/tags/ANN/"/>
    
    <category term="Linear" scheme="https://shaogui.life/tags/Linear/"/>
    
  </entry>
  
  <entry>
    <title>PP-LiteSeg：A Superior Real-Time Semantic Segmentation Model</title>
    <link href="https://shaogui.life/2023/01/28/PP-LiteSeg%EF%BC%9AA%20Superior%20Real-Time%20Semantic%20Segmentation%20Model/"/>
    <id>https://shaogui.life/2023/01/28/PP-LiteSeg%EF%BC%9AA%20Superior%20Real-Time%20Semantic%20Segmentation%20Model/</id>
    <published>2023-01-28T12:42:20.000Z</published>
    <updated>2023-05-20T11:03:07.544Z</updated>
    
    <content type="html"><![CDATA[<p>为搭建一个轻量化的语义分割网络，PP-LiteSeg 设计了灵活轻便的 encoder 模块、统一的注意力模块和高效的金字塔特征融合模块</p><a id="more"></a><h1 id="什么是-PP-LiteSeg-？"><a href="#什么是-PP-LiteSeg-？" class="headerlink" title="什么是 PP-LiteSeg ？"></a>什么是 PP-LiteSeg ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PP-LiteSeg-20230408143341.png" alt="PP-LiteSeg-20230408143341"></li><li>为搭建一个轻量化的语义分割网络，PP-LiteSeg 设计了灵活轻便的 encoder 模块、统一的注意力模块和高效的金字塔特征融合模块</li></ul><h1 id="PP-LiteSeg-的网络结构？"><a href="#PP-LiteSeg-的网络结构？" class="headerlink" title="PP-LiteSeg 的网络结构？"></a>PP-LiteSeg 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PP-LiteSeg-20230408143341.png" alt="PP-LiteSeg-20230408143341"></li><li><strong>Flexible and Lightweight Decoder</strong>(FLD)：灵活轻便的 encoder 模块，用于提取图片的特征，主要特点是通道数不增反减</li><li><strong>Unified Attention Fusion Module</strong>(UAFM)：使用 2 种空间注意力和通道注意力，构建输入特征的空间之间以及通道之间的关系</li><li><strong>Simple Pyramid Pooling Module</strong>(SPPM)：简单的特征融合模块，它减少了中间通道和输出通道，消除了Shortcut，并用一个add操作替换了concat操作</li></ul><h1 id="PP-LiteSeg-的-Flexible-and-Lightweight-Decoder-FLD-？"><a href="#PP-LiteSeg-的-Flexible-and-Lightweight-Decoder-FLD-？" class="headerlink" title="PP-LiteSeg 的 Flexible and Lightweight Decoder (FLD)？"></a>PP-LiteSeg 的 Flexible and Lightweight Decoder (FLD)？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PP-LiteSeg-20230408143412.png" alt="PP-LiteSeg-20230408143412"></li><li>一般来说，<strong>编码器</strong>利用一系列分成几个阶段的层来提取层次特征。对于从 low-level 到 high-level 的特征，通道的数量逐渐增加，特征的空间尺寸逐渐减小</li><li><strong>解码器</strong>也有几个阶段，负责对 low-level 到 high-level 的特征融合和上采样特征，为了提高解码器的效率，FLD逐渐将特征的通道从high-level减少到low-level层次</li></ul><h1 id="PP-LiteSeg-的-Unified-Attention-Fusion-Module-UAFM-？"><a href="#PP-LiteSeg-的-Unified-Attention-Fusion-Module-UAFM-？" class="headerlink" title="PP-LiteSeg 的 Unified Attention Fusion Module (UAFM)？"></a>PP-LiteSeg 的 Unified Attention Fusion Module (UAFM)？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PP-LiteSeg-20230408143413.png" alt="PP-LiteSeg-20230408143413"></li><li>输入特征被表示为 $F<em>{hight}$ 和 $F</em>{low}$。$F<em>{hight}$ 是深层模块的输出，而 $F</em>{low}$ 是编码器的对应模块。请注意，它们有相同的通道数量</li><li>UAFM 首先利用双线性插值操作将 $F<em>{hight}$ 上采样到 $F</em>{low}$ 相同大小，而上采样特征记为 $F<em>{up}$。然后，注意力模块以 $F</em>{up}$ 和 $F_{low}$ 作为输入，产生权重 $\alpha$</li><li>注意，注意力模块可以是一个插件，如空间注意力模块、通道注意力模块等</li></ul><h1 id="PP-LiteSeg-的-Simple-Pyramid-Pooling-Module-SPPM-？"><a href="#PP-LiteSeg-的-Simple-Pyramid-Pooling-Module-SPPM-？" class="headerlink" title="PP-LiteSeg 的 Simple Pyramid Pooling Module (SPPM)？"></a>PP-LiteSeg 的 Simple Pyramid Pooling Module (SPPM)？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PP-LiteSeg-20230408143414.png" alt="PP-LiteSeg-20230408143414"></li><li>与原始的 金字塔池化模块（PPM） 相比，SPPM 减少了中间通道和输出通道，删除了 shortcut，并用 add 操作替换了 cat 操作。因此，SPPM 更有效，也更适合用于实时模型</li></ul><p>参考：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/540851314">PP-LiteSeg：轻量级实时语义分割模型 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/506936919">高精度轻量级图像分割SOTA模型PP-LiteSeg重磅开源！ - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;为搭建一个轻量化的语义分割网络，PP-LiteSeg 设计了灵活轻便的 encoder 模块、统一的注意力模块和高效的金字塔特征融合模块&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="语义分割" scheme="https://shaogui.life/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
    <category term="轻量" scheme="https://shaogui.life/tags/%E8%BD%BB%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>CILP：Learning Transferable Visual Models From Natural Language Supervision</title>
    <link href="https://shaogui.life/2022/12/20/CILP%EF%BC%9ALearning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/"/>
    <id>https://shaogui.life/2022/12/20/CILP%EF%BC%9ALearning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/</id>
    <published>2022-12-20T11:33:45.000Z</published>
    <updated>2023-05-20T03:04:08.703Z</updated>
    
    <content type="html"><![CDATA[<p>CLIP 通过<strong>文本-图像对</strong>实现对模型预训练，上图是是 3 个文本-图像对，每个文本-图像对由一段文本+一张图片表示，文本描述了这个图片的内容（对象类别），CLIP 模型收集<strong>4亿（400 million）</strong>个**文本-图像对</p><a id="more"></a><h1 id="什么是-CILP-？"><a href="#什么是-CILP-？" class="headerlink" title="什么是 CILP ？"></a>什么是 CILP ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/CILP-20230510223614.png" alt=""></li><li><strong>Contrastive Language-Image Pre-training (CLIP)</strong> 是一个联通文本和图像的模型，比如输入图片和提示文本，模型输出图片类别</li><li>CLIP 通过<strong>文本-图像对</strong>实现对模型预训练，上图是是 3 个文本-图像对，每个文本-图像对由一段文本+一张图片表示，文本描述了这个图片的内容（对象类别），CLIP 模型收集<strong>4亿（400 million）</strong>个**文本-图像对</li></ul><h1 id="CILP-的网络结构？"><a href="#CILP-的网络结构？" class="headerlink" title="CILP 的网络结构？"></a>CILP 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/CILP-20230510215531.png" alt=""></li><li>CLIP 模型包含两部分，即<strong>文本编码器 (Text Encoder)</strong> 和<strong>图像编辑器 (Image Encoder)</strong>，Text Encoder 选择的是 Text Transformer 模型；Image Encoder 选择了两种模型，一是基于 CNN 的 ResNet（对比了不同层数的 ResNet），二是基于 Transformer 的 ViT</li><li><strong>编码器作用</strong>：假设一次输入 N 个文本对，N 个文本首先经过<strong>文本编码器 (Text Encoder)</strong> ，输出 [$T_1,T_2,T_3,…,T_N$]，每个文本的输出是长度为 $d_t$ 的向量，对应的 N 个图片经过<strong>图像编辑器 (Image Encoder)</strong>，输出[$I_1,I_2,I_3,…,I_N$]，每张图片输出也是长度为 $d_t$ 的向量</li><li><strong>自监督训练</strong>：得到[$T_1,T_2,T_3,…,T_N$]和[$I_1,I_2,I_3,…,I_N$]两两组合构成一个矩阵，其中 $T_i$ 与 $I_i$ 匹配，否则不匹配，将匹配的文本-图片对标记为正样本，共计 N 个，不匹配的文本-图像对标记为负样本，共计 N^2-N 个。通过正负样本可训练Text Encoder和Image Encoder</li><li><strong>损失函数</strong>：对于每个文本-图片对的输出，其都是长度为 $d_i$ 的向量，计算损失时通过余弦相似度计算损失即可，对于匹配的文本-图片对，其损失越小越好，对于不匹配的文本图片对，其损失越大越好，即 <script type="math/tex; mode=display">min(\sum_{i=1}^N\sum_{j=1}^N(I_i\cdot T_j)_{(i\neq j)}-\sum_{i=1}^N(I_i\cdot T_i))</script></li></ul><h1 id="文本编码器和图像编码器为什么只输出一维特征"><a href="#文本编码器和图像编码器为什么只输出一维特征" class="headerlink" title="文本编码器和图像编码器为什么只输出一维特征?"></a>文本编码器和图像编码器为什么只输出一维特征?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/CILP-20230510222400.png" alt=""></li><li><strong>编码器输出</strong>：对于每个文本-图片对，正常 N 个长度为 S 的文本输入 transformer，其输出是 (N, S, di)，同理图片输出应该是 (N, S’, dt)，如果只输出 (N, di)或 (N, dt)，那就是类似 Vit 的情况，增加一个 class token 汇总所有 tokens 的信息，或者平均所有的 S 作为输出；</li><li><strong>输出映射</strong>：即使文本编码器输出 (N, di)，图片编码器输出 (N, dt)，最后一维还是长度不一样的，此时分别学习一个 W_i（di, de）、W_t（dt, de）的嵌入，与前面两个输出点乘都得到长度为 (n, de)的输出，然后才能计算模型输出的余弦相似度损失</li><li>输出映射相当于 transformer 的 decoder 的输入，可以认为是文本或图片的 quies 查询向量，找出有用特征计算损失</li></ul><h1 id="CILP-的-zero-shot-分类？"><a href="#CILP-的-zero-shot-分类？" class="headerlink" title="CILP 的 zero-shot 分类？"></a>CILP 的 zero-shot 分类？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/CILP-20230510222948.png" alt=""></li><li><strong>生成类别特征</strong>：根据所迁移的数据集将所有类别转换为文本。这里以 Imagenet 有1000类为例，我们得到了1000个文本：<code>A photo of &#123;label&#125;</code>。我们将这1000个文本全部输入 Text Encoder 中，得到1000个编码后的向量 $T_i(i=1,2,…,N)(N=1000)$ , 这被视作文本特征</li><li><strong>生成图片特征</strong>：将需要分类的图像（单张图像）输入 Image Encoder 中，得到这张图像编码后的向量 $I_1$</li><li><strong>计算余弦相似度</strong>：将 $I_1$ 与得到的1000个文本特征分别计算余弦相似度。找出1000个相似度中最大的那一个（上图中对应的为 ($T_3$），那么评定要分类的图片与第三个文本标签（dog）最匹配，即可将其分类为狗</li></ul><p>参考：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/521151393">详解CLIP (一) | 打通文本-图像预训练实现ImageNet的zero-shot分类，比肩全监督训练的ResNet50/101 - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;CLIP 通过&lt;strong&gt;文本-图像对&lt;/strong&gt;实现对模型预训练，上图是是 3 个文本-图像对，每个文本-图像对由一段文本+一张图片表示，文本描述了这个图片的内容（对象类别），CLIP 模型收集&lt;strong&gt;4亿（400 million）&lt;/strong&gt;个**文本-图像对&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
    <category term="zero-shot" scheme="https://shaogui.life/tags/zero-shot/"/>
    
    <category term="分类" scheme="https://shaogui.life/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>MAE：Masked Autoencoders Are Scalable Vision Learners</title>
    <link href="https://shaogui.life/2022/11/13/MAE%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners/"/>
    <id>https://shaogui.life/2022/11/13/MAE%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners/</id>
    <published>2022-11-13T03:43:03.000Z</published>
    <updated>2023-05-20T02:57:27.245Z</updated>
    
    <content type="html"><![CDATA[<p>在深度学习模型中，通常会通过 BackBone 提取特征，这些 BackBone 通常使用大量的人工标注数据训练得到，但是人工标注成本很高，能不能在没有标注数据的情况下，训练得到一个预训练模型呢？MAE 通过自监督方法训练得到 BackBone 模型，MAE 开发了一种非对称编码器-解码器结构，其中的编码器仅对可见的 patch 子集进行操作，而轻量级解码器则从潜在表示和 mask token 重建原始图像。对输入图像的高比例（例如 75%）进行 mask 会产生一项困难且有意义的自监督任务</p><a id="more"></a><h1 id="什么是掩码自编码器-MAE-？"><a href="#什么是掩码自编码器-MAE-？" class="headerlink" title="什么是掩码自编码器 (MAE) ？"></a>什么是掩码自编码器 (MAE) ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Pasted%20image%2020230506134447.png" alt=""></li><li>在深度学习模型中，通常会通过 BackBone 提取特征，这些 BackBone 通常使用大量的人工标注数据训练得到，但是人工标注成本很高，能不能在没有标注数据的情况下，训练得到一个预训练模型呢？</li><li>MAE 通过自监督方法训练得到 BackBone 模型，MAE 开发了一种非对称编码器-解码器结构，其中的编码器仅对可见的 patch 子集进行操作，而轻量级解码器则从潜在表示和 mask token 重建原始图像。对输入图像的高比例（例如 75%）进行 mask 会产生一项困难且有意义的自监督任务</li></ul><h1 id="MAE-的网络结构？"><a href="#MAE-的网络结构？" class="headerlink" title="MAE 的网络结构？"></a>MAE 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Pasted%20image%2020230506134447.png" alt=""></li><li>首先将input image切分为patches，执行mask操作，然后只把可见的patches送入encoder中，再将encoder的输出（latent representations）以及mask tokens作为轻量级decoder的输入，decoder重构整张image</li><li><strong>encoder</strong>：一个标准的 ViT 结构，只把可见的 patches 送入 encoder 中，编码器输出 token 后，结合 mask tokens 组成完整的全套 token (encoder 和 decoder 处理的 token 数量一致)</li><li><strong>decoder</strong>：也是一个 vit 结构，目的是执行图像重建任务，将 mask tokens 和 encoder 的输出作为输入，加上位置编码。decoder 的最后一层是 linear projection，输出通道数量和一个 patch 内的 pixel 数量相同（方便重构），然后再 reshape，重构 image</li><li>MAE 中 decoder 的设计并不重要，因为预训练结束之后，只保留 encoder，decoder 只需要完成预训练时的图像重构任务，encoder 用于下游任务</li></ul><h1 id="MAE-的不同-Mask-ratio-大小对预训练、微调的影响？"><a href="#MAE-的不同-Mask-ratio-大小对预训练、微调的影响？" class="headerlink" title="MAE 的不同 Mask ratio 大小对预训练、微调的影响？"></a>MAE 的不同 Mask ratio 大小对预训练、微调的影响？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/MAE-20230507121754.png" alt=""></li></ul><h1 id="MAE-使用-Mask-token-对预训练、微调的影响？"><a href="#MAE-使用-Mask-token-对预训练、微调的影响？" class="headerlink" title="MAE 使用 Mask token 对预训练、微调的影响？"></a>MAE 使用 Mask token 对预训练、微调的影响？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Pasted%20image%2020230508092748.png" alt=""></li><li>实验表明，encoder 如果接收 mask tokens，performance 甚至会降低14%，因此 encoder 只接收 visible tokens，既能提升性能，又能降低计算量加速训练</li></ul><h1 id="MAE-不同深度、宽度的decoder-对-encoder-输出泛化性的影响？"><a href="#MAE-不同深度、宽度的decoder-对-encoder-输出泛化性的影响？" class="headerlink" title="MAE 不同深度、宽度的decoder 对 encoder 输出泛化性的影响？"></a>MAE 不同深度、宽度的decoder 对 encoder 输出泛化性的影响？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Pasted%20image%2020230508092124.png" alt=""></li><li><strong>深度</strong>：MAE decoder 的深度对 linear probing 影响非常大，可以从65.5提升到73.5，这是由于 MAE 预训练使用像素级重构任务，而 linear probing classifier 执行的是分类任务，两者之间有一个明显的 gap，表明 encoder 输出特征不够抽象。实验显示 8 blocks 的 decoder 使得 encoder 的 <strong>latent representation</strong> 的语义信息最抽象，更适合分类任务</li><li><strong>宽度</strong>：MAE decoder 的宽度也有类似结论，实验结果显示 512 的宽度性能最佳</li></ul><h1 id="MAE-使用不同重建策略对预训练、微调的影响？"><a href="#MAE-使用不同重建策略对预训练、微调的影响？" class="headerlink" title="MAE 使用不同重建策略对预训练、微调的影响？"></a>MAE 使用不同重建策略对预训练、微调的影响？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/MAE-20230507121352.png" alt=""></li></ul><h1 id="MAE-使用不同的目标重建策略？"><a href="#MAE-使用不同的目标重建策略？" class="headerlink" title="MAE 使用不同的目标重建策略？"></a>MAE 使用不同的目标重建策略？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Pasted%20image%2020230508092946.png" alt=""></li><li>MAE 最终目标构建策略使用的是像素级别的重建（pixel），作者还和 BEiT 那种预测 token 的方式以及 PCA 的方式</li><li>可以发现，预测归一化像素值的方式最强，其次，BEiT 那种预测 token 的玩法也不差</li></ul><p>参考：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/528720386">​一文看尽MAE最新进展！恺明的MAE已经提出大半年，目前发展如何？ - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/446761025">MAE(Masked Autoencoders) - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/439554945">别再无聊地吹捧了，一起来动手实现 MAE(Masked Autoencoders Are Scalable Vision Learners) 玩玩吧！ - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;在深度学习模型中，通常会通过 BackBone 提取特征，这些 BackBone 通常使用大量的人工标注数据训练得到，但是人工标注成本很高，能不能在没有标注数据的情况下，训练得到一个预训练模型呢？MAE 通过自监督方法训练得到 BackBone 模型，MAE 开发了一种非对称编码器-解码器结构，其中的编码器仅对可见的 patch 子集进行操作，而轻量级解码器则从潜在表示和 mask token 重建原始图像。对输入图像的高比例（例如 75%）进行 mask 会产生一项困难且有意义的自监督任务&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="自监督" scheme="https://shaogui.life/tags/%E8%87%AA%E7%9B%91%E7%9D%A3/"/>
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>SVTR：Scene Text Recognition with a Single Visual Model</title>
    <link href="https://shaogui.life/2022/10/25/SVTR%EF%BC%9AScene%20Text%20Recognition%20with%20a%20Single%20Visual%20Model/"/>
    <id>https://shaogui.life/2022/10/25/SVTR%EF%BC%9AScene%20Text%20Recognition%20with%20a%20Single%20Visual%20Model/</id>
    <published>2022-10-25T04:43:09.000Z</published>
    <updated>2023-05-20T03:35:46.487Z</updated>
    
    <content type="html"><![CDATA[<p>传统的文本识别算法是CNN+RNN，但是由于LSTM的效率较低，很多移动设备对LSTM的加速效果并不好，所以在实际的应用场景中也存在诸多限制，SVTR利用swin transformer替代构建局部和全局混合块，提取多尺度的特征，使得不需要RNN去构建序列依赖也能实现更好的效果</p><a id="more"></a><h1 id="什么是-SVTR-？"><a href="#什么是-SVTR-？" class="headerlink" title="什么是 SVTR ？"></a>什么是 SVTR ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SVTR-20230410205058.png" alt=""></li><li>a-d 代表 4 类文本识别模型，分别是传统 CRNN+CTC、CNN/多头自注意力模型+多头注意力、视觉语言模型、SVTR</li><li>传统的文本识别算法是CNN+RNN，但是由于LSTM的效率较低，很多移动设备对LSTM的加速效果并不好，所以在实际的应用场景中也存在诸多限制，SVTR利用swin transformer替代构建局部和全局混合块，提取多尺度的特征，使得不需要RNN去构建序列依赖也能实现更好的效果</li></ul><h1 id="SVTR-的网络结构？"><a href="#SVTR-的网络结构？" class="headerlink" title="SVTR 的网络结构？"></a>SVTR 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SVTR-20230410205119.png" alt=""></li><li><strong>输入</strong>：输入是已经检测好的文本区域</li><li><strong>Patch Embedding</strong>：对输入图像序列化，看图输出是（H/4, W/4），所以取输入的 4 x 4 区域作为一个 token，但是这里又做个改变，通过两层 CBR 取重叠的 3 x 3 卷积对其进行两次下采样，也可得到输出</li><li><strong>Mixing Block</strong>：对输入的 tokens 加自注意力机制，输入输出均是 $hw\times d_{i-1}$</li><li><strong>mergin</strong>：将 tokens 转为卷积输入的 4 D 形式，并使用卷积对其进行下采样，输入是 $hw\times d<em>{i-1}$，输出是 $\frac {h} {2}\times w\times d</em>{i-1}$</li><li><strong>Combing</strong>：使用”自适应全局平均池化”将高度 H 变为1，然后对每个 tokens 的隐变量变换 (D2-&gt;D3)</li></ul><h1 id="SVTR-的-Patch-Embedding？"><a href="#SVTR-的-Patch-Embedding？" class="headerlink" title="SVTR 的 Patch Embedding？"></a>SVTR 的 Patch Embedding？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SVTR-20230410205206.png" alt=""></li><li>已知 swin 是直接使用一个步长为4的4×4卷积进行无重叠的 patch embedding，而svtr则是使用两个步长为2的3×3卷积进行有重叠的patch embedding（延续的CNN的作风，感受野更大，提取局部信息的表达能力也会比swin的patch embedding要好）</li></ul><h1 id="SVTR-的-Mixing-Block？"><a href="#SVTR-的-Mixing-Block？" class="headerlink" title="SVTR 的 Mixing Block？"></a>SVTR 的 Mixing Block？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SVTR-20230410205244.png" alt=""></li><li>上图 (a)、（b）分别表示 Global Mixing、Local Mixing</li><li><strong>Global Mixing</strong>：类似 transformer ，在全部 tokens 上构建自注意力，首先特征图经过线性变换投影到三个空间，然后 q 矩阵和 k 矩阵的转置进行矩阵乘法、softmax 操作得到 attention 矩阵，最后和 v 矩阵进行矩阵乘法得到输出</li><li><strong>Local Mixing</strong>：类似 swin-transformer，在一个固定窗口建立自注意力，这样可以降低计算成本同时获取更多局部注意力。注意：swin 是通过 reshape 这种类似的方式来进行滑窗，并将不同的窗口累加到通道维度上，而 SVTR 则是直接使用值为0的 mask 操作。SVTR 这种做法和 swin 相比，计算复杂度还是比较高</li></ul><h1 id="SVTR-和-swin-transformer-的区别？"><a href="#SVTR-和-swin-transformer-的区别？" class="headerlink" title="SVTR 和 swin-transformer 的区别？"></a>SVTR 和 swin-transformer 的区别？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SVTR-20230410205649.png" alt=""></li><li>svtr 是一个三级逐步下采样的网络（和 swin transformer 一样，下采样三次），和 CNN 架构一样，由 block + 下采样模块组成</li><li>其 block 模块和普通的 swin 中的 block 模块一致，都是 self-attention + mlp。不同的是，SVTR 中 self-attention 的方式和 swin 的滑动窗口有一定的差异</li></ul><p>参考：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/522545062">SVTR论文学习 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/540738873">《SVTR: Scene Text Recognition with a Single Visual Model》解读 - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;传统的文本识别算法是CNN+RNN，但是由于LSTM的效率较低，很多移动设备对LSTM的加速效果并不好，所以在实际的应用场景中也存在诸多限制，SVTR利用swin transformer替代构建局部和全局混合块，提取多尺度的特征，使得不需要RNN去构建序列依赖也能实现更好的效果&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文字识别" scheme="https://shaogui.life/tags/%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>Swin Transformer：Hierarchical Vision Transformer using Shifted Windows</title>
    <link href="https://shaogui.life/2022/10/11/Swin%20Transformer%EF%BC%9AHierarchical%20Vision%20Transformer%20using%20Shifted%20Windows/"/>
    <id>https://shaogui.life/2022/10/11/Swin%20Transformer%EF%BC%9AHierarchical%20Vision%20Transformer%20using%20Shifted%20Windows/</id>
    <published>2022-10-11T15:50:11.000Z</published>
    <updated>2023-05-20T02:53:52.208Z</updated>
    
    <content type="html"><![CDATA[<p>为解决原始transformer在全局上构建注意力的成本巨大问题，Swin Transformer引入WIndows的概念，在每个Windows内构建全局注意力，使得成本由平方变为线性。同时借鉴CNN的层次特征，设计多层次的transformer block，提取图像的多尺度特征</p><a id="more"></a><h1 id="什么是-swin-trasformer"><a href="#什么是-swin-trasformer" class="headerlink" title="什么是 swin-trasformer?"></a>什么是 swin-trasformer?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/swin-transformer-20230408152156.png" alt="swin-transformer-20230408152156"></li><li>原始的 vit 构建全局注意力，在 patch 数量多的情况下，其构建成本很高，因此 swin-trasformer 引入<strong>locality 思想</strong>，对无重合的 <strong>window 区域内进行 self-attention 计算</strong>，并且为了不同 windows 的交流，设计了<strong>滑窗操作</strong></li><li>同时，借鉴 CNN 的层次化的思想构建层次的 trasformer</li><li>这种层级式的结构不仅非常灵活，可以提供各个尺度的特征信息，它的<strong>计算复杂度是随着图像大小而线性增长</strong>，而不是平方级增长</li></ul><h1 id="swin-trasformer-的结构？"><a href="#swin-trasformer-的结构？" class="headerlink" title="swin-trasformer 的结构？"></a>swin-trasformer 的结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/swin-transformer-20230408152157.png" alt="swin-transformer-20230408152157"></li><li>整个模型采取层次化的设计，一共包含4个 Stage，每个 stage 都会<strong>缩小输入特征图的分辨率</strong>，像 CNN 一样逐层扩大感受野</li><li><strong>Patch Embedding</strong>：将图片切成一个个Patch，并嵌入到Embedding</li><li><strong>Linear Embedding</strong>：将输入 (B, S, 48)转为 (B, S, C)</li><li><strong>Patch Merging</strong>：在每个 Stage 一开始<strong>降低图片分辨率</strong>，输出隐变量长度还是 2C，采用的方法是类似 YOLOv5 的输入，间隔采样 H, W，使得各缩小2倍，此时通道维度会变成原先的4倍，再通过一个全连接层再调整通道维度为 2C</li><li><strong>Swin Transformer Block</strong>：使用 Transformer 的 encoder 部分构建”windows”内所有 patch 的注意力</li></ul><h1 id="swin-trasformer-的-Patch-Merging-模块？"><a href="#swin-trasformer-的-Patch-Merging-模块？" class="headerlink" title="swin-trasformer 的 Patch Merging 模块？"></a>swin-trasformer 的 Patch Merging 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/swin-transformer-20230408152158.png" alt=""></li><li>采用的方法是类似 YOLOv5 的输入，间隔采样 H, W，使得各缩小2倍，此时通道维度会变成原先的4倍，再通过一个全连接层再调整通道维度为 2C</li></ul><h1 id="swin-trasformer-的-Swin-Transformer-Block？"><a href="#swin-trasformer-的-Swin-Transformer-Block？" class="headerlink" title="swin-trasformer 的 Swin Transformer Block？"></a>swin-trasformer 的 Swin Transformer Block？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/swin-transformer-20230408152158-1.png" alt="swin-transformer-20230408152158-1"></li><li>Swin Transformer Block 包含 2 部分，即窗口多头自注意层（window multi-head self-attention, W-MSA）和移位窗口多头自注意层（shifted-window multi-head self-attention, SW-MSA）</li><li><strong>窗口多头自注意层（W-MSA）</strong>：传统的 Transformer 都是基于全局来计算注意力的，因此计算复杂度十分高。而 Swin Transformer 则将注意力的计算限制在每个窗口内，进而减少了计算量<script type="math/tex; mode=display">\begin{array}{l}\hat{\mathbf{z}}^{l}=\mathrm{W-MSA}(\mathrm{LN}(\mathbf{z}^{l-1}))+\mathbf{z}^{l-1}\\ \mathbf{z}^{l}=\mathrm{MLP}(\mathrm{LN}(\mathbf{\hat{z}}^{l}))+\mathbf{\hat{z}}^{l}\end{array}</script></li><li><strong>移位窗口多头自注意层（SW-MSA）</strong>：为了保证<strong>不重叠窗口之间有联系</strong>，采用了<strong>shifted window self-attention</strong>的方式<strong>重新计算一遍窗口偏移之后的自注意力</strong><script type="math/tex; mode=display">\begin{array}{c}\hat{\mathbf{z}}^{l+1}=\mathrm{SW-MSA}(\mathrm{LN}(\mathbf{z}^{l}))+\mathbf{z}^{l}\\ \mathbf{z}^{l+1}=\mathrm{MLP}(\mathrm{LN}(\mathbf{\hat{z}}^{l+1}))+\mathbf{\hat{z}}^{l+1}\end{array}</script></li></ul><h1 id="swin-trasformer-的-W-MSA-模块"><a href="#swin-trasformer-的-W-MSA-模块" class="headerlink" title="swin-trasformer 的 W-MSA 模块?"></a>swin-trasformer 的 W-MSA 模块?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/swin-transformer-20230408152159.png" alt=""></li><li>左侧普通的 Multi-head Self-Attention（MSA）模块，计算 feature map 中的每个像素（或称作 token，patch）所有注意力。右侧 Windows Multi-head Self-Attention（W-MSA）模块，首先将 feature map 按照 MxM（M=2）大小划分成一个个 Windows，然后单独对每个 Windows 内部进行 Self-Attention</li></ul><h1 id="swin-trasformer-的-W-MSA-模块的“相对位置编码”？"><a href="#swin-trasformer-的-W-MSA-模块的“相对位置编码”？" class="headerlink" title="swin-trasformer 的 W-MSA 模块的“相对位置编码”？"></a>swin-trasformer 的 W-MSA 模块的“相对位置编码”？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/swin-transformer-20230408152200.png" alt=""></li><li><p>WindowAttention 与传统的 Attention 主要区别是在原始计算 Attention 的公式中的 Q, K 时<strong>添加一个可学习的相对位置参数 B</strong></p><script type="math/tex; mode=display">Attention(Q,K,V)=SoftMax(QK^T/\sqrt{d}+B)V</script></li><li><p>假设 window_size = 2*2即每个窗口有4个 token ，在计算 self-attention 时，每个 token 都要与所有的 token 计算 QK 值，如图2所示，当位置1的 token 计算 self-attention 时，要计算位置1与位置 (1,2,3,4)的 QK 值，即以位置1的 token 为中心点，中心点位置坐标 (0,0)，其他位置计算与当前位置坐标的偏移量</p></li></ul><h1 id="swin-trasformer-的-SW-MSA-模块？"><a href="#swin-trasformer-的-SW-MSA-模块？" class="headerlink" title="swin-trasformer 的 SW-MSA 模块？"></a>swin-trasformer 的 SW-MSA 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-06%2020.23.58.excalidraw.svg" alt="Drawing 2023-04-06 20.23.58.excalidraw"></li><li>上图首先绘制了 W-MSA 在 2 x 2 的 patch 上构建注意力示意图，然后通过 Shifted Windows 操作，在新的 2 x 2  patch 构建注意力</li><li><strong>Shifted Windows</strong>：W-MSA 在每个色块内构建全局注意力，如[1,2,3,4]，[5,6,7,8]，首先 patch 往左上角移动 M/2 个单位，然后通过下移、右移，得到新的 patch 矩阵</li><li><strong>SW-MSA</strong>：Shifted Windows 完成后，根据原始 patch 是否相邻构建注意力，比如对于第一个窗的[4,7,10,13]在原始 patch 矩阵上相邻，所以构建无 Mask 的 4 x 4 的注意力矩阵，而[8,3,14,9]只能在[8,3]、[14,9]之间构建注意力，得到有 Mask 的4 x 4 的注意力矩阵</li><li>通过移位重新构建注意力，可以让原始不同 Windows 之间得到交流，比如 W-MSA 上[4,7]没有构建注意力，而 SW-MSA 构建了它们之间的注意力，这类似 CNN，网络变深，感受野不断增强</li></ul><h1 id="swin-trasformer-的-SW-MSA-模块的-Mask-生成？"><a href="#swin-trasformer-的-SW-MSA-模块的-Mask-生成？" class="headerlink" title="swin-trasformer 的 SW-MSA 模块的 Mask 生成？"></a>swin-trasformer 的 SW-MSA 模块的 Mask 生成？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-06%2021.43.11.excalidraw.svg" alt="Drawing 2023-04-06 21.43.11.excalidraw"></li><li>经过 SW-MSA 模块时，每个 Windows 内不完全是构建全局注意力，这就需要使用 Mask 去掉那些不需要的位置，总体上 Shifted Windows 得到的 Windows 分为 4 种，每种 Mask 矩阵对应如上</li><li><strong>如何使用 Mask 呢？</strong>即在得到 QK^T 的指之后，将其乘上 Mask，对哪些无需计算注意力的位置赋予无穷小，使得 softmax 后趋向0</li></ul><h1 id="swin-trasformer-与-vit-的区别？"><a href="#swin-trasformer-与-vit-的区别？" class="headerlink" title="swin-trasformer 与 vit 的区别？"></a>swin-trasformer 与 vit 的区别？</h1><ul><li><strong>patch</strong>：swin-trasformer 的大小是 4 x 4，vit 是 16 x 16，不过 swin-trasformer 是指一个窗口内的</li><li><strong>emdedding</strong>：swin-trasformer 可选加，因为在计算 Attention 的时候做了一个<strong>相对位置编码</strong></li><li><strong>cls_token</strong>：swin-trasformer 直接拿所有 token 的平均，作为 cls_token，而不是像 vit 使用单独的位置</li></ul><p>参考资料：</p><ol><li><a href="https://blog.csdn.net/zhe470719/article/details/123395256">【深度学习】论文阅读：（ICCV-2021））Swin Transformer_swint模块,让swin-transformer 的使用变得和cnn一样方便快捷_sky_柘的博客-CSDN博客</a></li><li><a href="https://blog.csdn.net/qq_37541097/article/details/121119988">Swin-Transformer网络结构详解_swin transformer_太阳花的小绿豆的博客-CSDN博客</a></li><li><a href="https://github.com/MaoQiankun97/swin_transformer/tree/main">GitHub - MaoQiankun97/swin_transformer: SwinTransformer pytorch实现</a></li><li><a href="https://zhuanlan.zhihu.com/p/542675669">Swin Transformer中的mask机制 - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;为解决原始transformer在全局上构建注意力的成本巨大问题，Swin Transformer引入WIndows的概念，在每个Windows内构建全局注意力，使得成本由平方变为线性。同时借鉴CNN的层次特征，设计多层次的transformer block，提取图像的多尺度特征&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>OAA：Online Attention Accumulation for Weakly Supervised Semantic Segmentation</title>
    <link href="https://shaogui.life/2022/10/11/OAA%EF%BC%9AOnline%20Attention%20Accumulation%20for%20Weakly%20Supervised%20Semantic%20Segmentation/"/>
    <id>https://shaogui.life/2022/10/11/OAA%EF%BC%9AOnline%20Attention%20Accumulation%20for%20Weakly%20Supervised%20Semantic%20Segmentation/</id>
    <published>2022-10-11T14:52:22.000Z</published>
    <updated>2023-05-20T11:04:17.169Z</updated>
    
    <content type="html"><![CDATA[<p>OAA是一个弱监督语义分割网络，通过设计OAA模块，在训练过程中不断累积CAM，这比单纯使用最后一次的CAM更能体现物体轮廓</p><a id="more"></a><h1 id="什么是-OAA-？"><a href="#什么是-OAA-？" class="headerlink" title="什么是 OAA ？"></a>什么是 OAA ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/OAA-20230408143247.png" alt="OAA-20230408143247"></li><li>弱监督的语义分割一般从 类别激活映射图 (Class Activation Mapping,CAM)生成图片的 mask，OAA 发现在训练的不同阶段，CAM 响应的位置有差异，如果把不同训练阶段得到 CAM 融合在一起，会不会比只取最后阶段的 CAM 效果好？本文由此提出。</li><li><strong>个人理解</strong>：分类网络一般使用交叉熵学习，把“是”的概率推向 1，“否”的概率推向 0，比如刚开始区分养和人，需要养、人的全身特征，最后收敛羊只需要判断有角即可，所以训练过程，网络关注的地方越来越高度抽象集中，但是这样的 CAM 对语义分割来说，起步越来越差，所以积累不同阶段 CAM，获得更好起点成为可能</li><li>上图 a 是原图，b-d 是不同阶段的 CAM 图，可以看出，其 CAM 区域越来越包含整个目标，e 是最后的融合 CAM，它比 b-d 更体现目标的 mask 区域</li></ul><h1 id="OAA-的网络结构？"><a href="#OAA-的网络结构？" class="headerlink" title="OAA 的网络结构？"></a>OAA 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/OAA-20230408143248.png" alt="OAA-20230408143248"></li><li><strong>backbone</strong>：VGG-16 </li><li><strong>OAA</strong>：用于累积不同 epoch 的 CAM</li></ul><h1 id="OAA-的-OAA-模块？"><a href="#OAA-的-OAA-模块？" class="headerlink" title="OAA 的 OAA 模块？"></a>OAA 的 OAA 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/OAA-20230408143248.png" alt="OAA-20230408143248"></li><li>假设 class-aware convolutional layer 的输出是 F，首先使用 RELU 对其进行归一化处理</li><li>第一个 epoch 的 CAM ($A<em>1$) 用于初始化 $M</em>{1}$，第二个 epoch 则是 $M_{1}$ 和 $A_2$ 生成 $M_2$，其按照以下公式更新 <script type="math/tex; mode=display">M_t= AF (M_{t-1},A_t)</script></li><li>OAA 不断使用上述公式融合，AF 的操作其实是 element-wise maximum operation，即取两个矩阵所以位置最大的激活值 <script type="math/tex; mode=display">M_t= AF (M_{t-1},A_t)=max (M_{t-1},A_t)</script></li></ul><h1 id="OAA-的-OAA-如何训练？"><a href="#OAA-的-OAA-如何训练？" class="headerlink" title="OAA 的 OAA +如何训练？"></a>OAA 的 OAA +如何训练？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/OAA-20230408143314.png" alt="OAA-20230408143314"></li><li>OAA 的缺点在于有些目标区域的 attention values 很低，不足以被分类模型加强，对于这种情况，我们提出一个新的 loss 函数，把 cumulative attention maps 作为监督信息，去训练一个 attention module 以进一步改善 OAA，我们称之为 OAA+</li><li>具体来讲，我们将 cumulative attention maps 作为 soft labels。每个 attention value 被视作当前点属于目标类别的概率，去掉 GAP 和分类层，得到 integral attention model，将输入图像得到当前的 attention maps，cumulative attention maps 和 attention maps 计算多标签交叉熵损失</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;OAA是一个弱监督语义分割网络，通过设计OAA模块，在训练过程中不断累积CAM，这比单纯使用最后一次的CAM更能体现物体轮廓&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="语义分割" scheme="https://shaogui.life/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
    <category term="弱监督" scheme="https://shaogui.life/tags/%E5%BC%B1%E7%9B%91%E7%9D%A3/"/>
    
  </entry>
  
  <entry>
    <title>PRTR：Pose Recognition with Cascade Transformers</title>
    <link href="https://shaogui.life/2022/09/08/PRTR%EF%BC%9APose%20Recognition%20with%20Cascade%20Transformers/"/>
    <id>https://shaogui.life/2022/09/08/PRTR%EF%BC%9APose%20Recognition%20with%20Cascade%20Transformers/</id>
    <published>2022-09-08T03:33:37.000Z</published>
    <updated>2023-05-20T02:48:02.166Z</updated>
    
    <content type="html"><![CDATA[<p>PRTR 是针对2D Pose Estimation 提出了<strong>基于 cascade transformer 结构的人体姿态估计网络</strong>，该网络首先使用 CNN 提取特征，然后使用 transformer 学习注意力，并最终使用类似 transformer-decoder 的方式逐个输出特征点位置</p><a id="more"></a><h1 id="什么是-PRTR？"><a href="#什么是-PRTR？" class="headerlink" title="什么是 PRTR？"></a>什么是 PRTR？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PRTR-20230408154304.png" alt=""></li><li>PRTR 是针对2D Pose Estimation 提出了<strong>基于 cascade transformer 结构的人体姿态估计网络</strong>，该网络首先使用 CNN 提取特征，然后使用 transformer 学习注意力，并最终使用类似 transformer-decoder 的方式逐个输出特征点位置</li><li>PRTR 是一个两阶段的模型，第一阶段使用 DETR 找到 human 位置，第二阶段对每个人预测关键点</li></ul><h1 id="PRTR-的模型结构？"><a href="#PRTR-的模型结构？" class="headerlink" title="PRTR 的模型结构？"></a>PRTR 的模型结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PRTR-20230408154305.png" alt=""></li><li><strong>Person Detection Transformer</strong>：基于 DETR 的检测方法，用一个 CNN Backbone 提取 RGB feature，然后通过 encoder 编码上下文关系，decoder 预测 bbox，得到 bbox 后，对 original image 进行 crop</li><li><strong>Keypoint Detection Transformer</strong>：得到 crop 后的 image 和对应的 positional encoding 之后，送进 encoder 学习</li></ul><h1 id="PRTR-的”Keypoint-Detection-Transformer”部分如何训练、推理？"><a href="#PRTR-的”Keypoint-Detection-Transformer”部分如何训练、推理？" class="headerlink" title="PRTR 的”Keypoint Detection Transformer”部分如何训练、推理？"></a>PRTR 的”Keypoint Detection Transformer”部分如何训练、推理？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PRTR-20230408154307.png" alt=""></li><li><strong>训练</strong>：初始化<strong>可学习</strong>的 queries，然后训练 transformer，最后输出 N 个序列的预测集合，接着使用<strong>匈牙利匹配算法</strong>计算损失，更新网络</li><li><strong>推理</strong>：根据输入图片及学习到的 queries，得到图片所有关键点的位置</li></ul><h1 id="PRTR-的-end2end-的模型结构？"><a href="#PRTR-的-end2end-的模型结构？" class="headerlink" title="PRTR 的 end2end 的模型结构？"></a>PRTR 的 end2end 的模型结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PRTR-20230408154308.png" alt=""></li><li>PRTR 根据自己的思想设计的另一个端到端的模型，该模型也是 2 阶段的过程，和原始模型区别在于其融合了多尺度特征</li></ul><h1 id="PRTR-的-queries-与关键点的关系？"><a href="#PRTR-的-queries-与关键点的关系？" class="headerlink" title="PRTR 的 queries 与关键点的关系？"></a>PRTR 的 queries 与关键点的关系？</h1><ul><li><strong>queries 与关键点位置</strong>：Keypoint Detection Transformer 的 decoder 使用 100 个 queries，最后输出也是 100 个关键点的位置输出，将这些预测的关键点位置按类别可视化后，通过分析 queries 与类别的关系，可知 queries 输出的关键点一定程度反应其真实位置<img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PRTR-20230408154308-1.png" alt=""></li><li><strong>queries 与关键点类别</strong>：Keypoint Detection Transformer 的 decoder 使用 100 个 queries，最后输出也是 100 个关键点的类别输出，通过分析 queries 与类别的关系，可知特定的 queries 倾向输出特定的关键点<img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PRTR-20230408154309.png" alt=""></li></ul><p>参考：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/368067142">【CVPR 2021】PRTR：基于transformer的2D Human Pose Estimation - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;PRTR 是针对2D Pose Estimation 提出了&lt;strong&gt;基于 cascade transformer 结构的人体姿态估计网络&lt;/strong&gt;，该网络首先使用 CNN 提取特征，然后使用 transformer 学习注意力，并最终使用类似 transformer-decoder 的方式逐个输出特征点位置&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>SOTR：Segmenting Objects with Transformers</title>
    <link href="https://shaogui.life/2022/08/10/SOTR%EF%BC%9ASegmenting%20Objects%20with%20Transformers/"/>
    <id>https://shaogui.life/2022/08/10/SOTR%EF%BC%9ASegmenting%20Objects%20with%20Transformers/</id>
    <published>2022-08-10T14:16:48.000Z</published>
    <updated>2023-05-20T02:42:58.585Z</updated>
    
    <content type="html"><![CDATA[<p>SOTR 利用 transformer 将实例分割任务简化为 2 个过程，一是通过 transformer 预测每个实例的类别，二是通过多级上采样模块动态生成分割掩码</p><a id="more"></a><h1 id="什么是-SOTR？"><a href="#什么是-SOTR？" class="headerlink" title="什么是 SOTR？"></a>什么是 SOTR？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SOTR-20230408154925.png" alt=""></li><li>SOTR 利用 transformer 将实例分割任务简化为 2 个过程，一是通过 transformer 预测每个实例的类别，二是通过多级上采样模块动态生成分割掩码</li><li>图片经过 FPN 的特征，对其进行序列化后得到 NxN 的序列，经过 transformer 后输出 NxN 个序列的结果，将原图 gt 实例某个序列上，计算损失并更新网络</li></ul><h1 id="SOTR-的模型结构？"><a href="#SOTR-的模型结构？" class="headerlink" title="SOTR 的模型结构？"></a>SOTR 的模型结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SOTR-20230408154925.png" alt=""></li><li><strong>BackBone</strong>：使用 FPN 生成 P2-P6 的多尺度特征</li><li><strong>Transformer</strong>：P2-P6 特征添加添加 Positional Embedding 后，输入 Transformer 进行学习，得到每张图的预测集合</li><li><strong>Multi-Level Upsampling Module</strong>：取出 BackBone 的 P 2、P 3、P 4 和 Transformer 的 P5，一起上采样到 P 2 分辨率进行合并，然后输出</li></ul><h1 id="SOTR-的-Twin-attention-模块？"><a href="#SOTR-的-Twin-attention-模块？" class="headerlink" title="SOTR 的 Twin attention 模块？"></a>SOTR 的 Twin attention 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SOTR-20230408154921.png" alt=""></li><li>为了降低原始 transformer block 的计算成本，SOTR 将生成注意力的过程转为 2 次生成注意力的过程，第一次是生成行注意力，第二次生成列注意力。使得针对 HxW 个 token 生成序列时，复杂度由 O (HWxHW)变为 O (H^2 W+HW^2)</li><li>FFN 部分由 Linear 变为卷积实现</li></ul><h1 id="SOTR-的-Multi-Level-updampling-module？"><a href="#SOTR-的-Multi-Level-updampling-module？" class="headerlink" title="SOTR 的 Multi-Level updampling module？"></a>SOTR 的 Multi-Level updampling module？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SOTR-20230408154923.png" alt=""></li><li><strong>原始特征</strong>：取出 BackBone 的 P 2、P 3、P 4 和 Transformer 的 P5，一起上采样到 P 2 分辨率进行合并</li><li><strong>动态卷积核</strong>：动态卷积核由 transformer 生成</li></ul><h1 id="SOTR-的样本匹配？"><a href="#SOTR-的样本匹配？" class="headerlink" title="SOTR 的样本匹配？"></a>SOTR 的样本匹配？</h1><ul><li>图片经过 FPN 的特征，对其进行序列化后得到 NxN 的序列，经过 transformer 后输出 NxN 个序列的结果，将原图 gt 实例某个序列上，计算损失并更新网络</li><li>直接使用位置映射样本，是因为 SOTR 只使用 transformer 的 encoder 部分，transformer 不会打乱序列的顺序，只会使用自注意力机制更新自己的隐向量</li></ul><p>参考：</p><ol><li><a href="https://blog.csdn.net/m0_61899108/article/details/121598645">【论文笔记】SOTR: Segmenting Objects with Transformers_m0_61899108的博客-CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/424036708">【分割 Transformer】SOTR: Segmenting Objects with Transformers - 知乎</a></li><li><a href="[SOTR:Segmenting Objects with Transformers [ICCV 2021] | Tianliang](https://www.starlg.cn/2022/05/19/SOTR/">SOTR:Segmenting Objects with Transformers [ICCV 2021] | Tianliang (starlg.cn)</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;SOTR 利用 transformer 将实例分割任务简化为 2 个过程，一是通过 transformer 预测每个实例的类别，二是通过多级上采样模块动态生成分割掩码&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
  </entry>
  
</feed>
