<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>年轻人起来冲</title>
  
  
  <link href="https://shaogui.life/atom.xml" rel="self"/>
  
  <link href="https://shaogui.life/"/>
  <updated>2023-05-21T01:56:48.227Z</updated>
  <id>https://shaogui.life/</id>
  
  <author>
    <name>Shaogui</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Segment Anything</title>
    <link href="https://shaogui.life/2023/05/20/Segment%20Anything/"/>
    <id>https://shaogui.life/2023/05/20/Segment%20Anything/</id>
    <published>2023-05-20T11:36:23.000Z</published>
    <updated>2023-05-21T01:56:48.227Z</updated>
    
    <content type="html"><![CDATA[<p>SAM通过transformer将点、框、Mask、文本等prompt和图片进行编码学习，可以实现对图片任意目标的分割</p><span id="more"></span><h1 id="什么是-SAM-？"><a href="#什么是-SAM-？" class="headerlink" title="什么是 SAM ？"></a>什么是 SAM ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAM-3.png" alt=""></li><li>a)SAM 利用“图片-分割提示”实现对图片上任意目标的分割，分割提示包括：点、框、Mask、文本</li><li>b) SAM 首先利用 prompt encoder 编码”分割提示”，利用 image encoder 编码“图片”，然后通过 Mask decoder 解析输出 Mask</li><li>c)SAM 利用数据驱动去做模型训练，模型输出结果后再输入模型训练</li></ul><h1 id="SAM-的网络结构？"><a href="#SAM-的网络结构？" class="headerlink" title="SAM 的网络结构？"></a>SAM 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAM.png" alt=""></li><li><strong>image encoder</strong>：类似 VIT 的过程，输入 image (1,3, H, W), 输出 image_embedding (1, C, H/16, W/16)，即 (1, HW/256, C)的 tokens 表示</li><li><strong>mask</strong>：mask prompt，直接和image_embedding相加即可</li><li><strong>prompt encoder</strong>：包含3种提示的编码过程，其中点、框按位置被编码为Pos embedding(1,N,C)，文本通过clip模型被编码为Pos embedding(1,M,C)</li><li><strong>mask decoder</strong>：根据image_embedding和prompt encoder输出，结合IOU tokens(1,1,C)和mask tokens(1,P,C)，解析出目标mask(1,1+P+N+M, H/16, W/16)和iou(1,1+P+N+M)</li></ul><h1 id="SAM-的-image-encoder？"><a href="#SAM-的-image-encoder？" class="headerlink" title="SAM 的 image encoder？"></a>SAM 的 image encoder？</h1><ul><li>类似 VIT 的 encoder 过程，输入 image (1,3, H, W), 输出 image_embedding (1, C, H/16, W/16)，即 (1, HW/256, C)的 tokens 表示</li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> image_encoder=ImageEncoderViT(..)</span><br><span class="line"> <span class="comment"># batched_input=&#123;List,List&#125; -&gt; torch.Size([2, 3, 1024, 1024])</span></span><br><span class="line"> input_images = torch.stack([preprocess(x[<span class="string">&quot;image&quot;</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> batched_input], dim=<span class="number">0</span>) </span><br><span class="line"><span class="comment"># torch.Size([2, 3, 1024, 1024]) -&gt; torch.Size([2, 256, 64, 64])</span></span><br><span class="line"> image_embeddings = image_encoder(input_images)</span><br></pre></td></tr></table></figure></li></ul><h1 id="SAM-的-prompt-encoder"><a href="#SAM-的-prompt-encoder" class="headerlink" title="SAM 的 prompt encoder?"></a>SAM 的 prompt encoder?</h1><ul><li>包含3种提示的编码过程，其中点、框按位置被编码为 Pos embedding (1, N, C)，文本通过 clip 模型被编码为 Pos embedding (1, M, C)，最终输出（1,N+M,C )的稀疏编码sparse_embeddings</li><li><strong>point&amp;box</strong>：每个点编码为1个 pos embedding，每个 box 编码为2个 pos embedding（box 被两个点定义）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"> embed_dim=<span class="number">256</span></span><br><span class="line"> num_point_embeddings: <span class="built_in">int</span> = <span class="number">4</span>  <span class="comment"># pos/neg point + 2 box corners</span></span><br><span class="line"> point_embeddings = [nn.Embedding(<span class="number">1</span>, embed_dim) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_point_embeddings)]</span><br><span class="line"> point_embeddings = nn.ModuleList(point_embeddings)</span><br><span class="line"> not_a_point_embed = nn.Embedding(<span class="number">1</span>, embed_dim)</span><br><span class="line"><span class="comment"># point prompt</span></span><br><span class="line">points = points + <span class="number">0.5</span>  <span class="comment"># Shift to center of pixel  </span></span><br><span class="line"><span class="comment"># 根据点位置points，在输入(1024,1024)的基础上生成pos embedding</span></span><br><span class="line"> point_embedding = pe_layer.forward_with_coords(points, input_image_size) <span class="comment">#torch.Size([1,3,2])+(1024,1024)-&gt;torch.Size([1,3,256])</span></span><br><span class="line"> <span class="comment"># 点有3类，-1表示非嵌入点，此时不使用pos embedding，0表示正样本点，1表示负样本点</span></span><br><span class="line">point_embedding[labels == -<span class="number">1</span>] = <span class="number">0.0</span></span><br><span class="line"> point_embedding[labels == -<span class="number">1</span>] += not_a_point_embed.weight</span><br><span class="line"> point_embedding[labels == <span class="number">0</span>] += point_embeddings[<span class="number">0</span>].weight</span><br><span class="line"> point_embedding[labels == <span class="number">1</span>] += point_embeddings[<span class="number">1</span>].weight</span><br><span class="line"><span class="comment"># box prompt</span></span><br><span class="line"> boxes = boxes + <span class="number">0.5</span>  <span class="comment"># Shift to center of pixel</span></span><br><span class="line"> coords = boxes.reshape(-<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>) <span class="comment"># 一个框肯定2个点</span></span><br><span class="line"> corner_embedding = pe_layer.forward_with_coords(coords, input_image_size)</span><br><span class="line"> corner_embedding[:, <span class="number">0</span>, :] += point_embeddings[<span class="number">2</span>].weight <span class="comment">#框第一个点</span></span><br><span class="line"> corner_embedding[:, <span class="number">1</span>, :] += point_embeddings[<span class="number">3</span>].weight <span class="comment">#框第二个点</span></span><br><span class="line"><span class="comment"># 汇总point、box编码</span></span><br><span class="line">sparse_embeddings = torch.empty((<span class="number">1</span>, <span class="number">0</span>, embed_dim))</span><br><span class="line">sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=<span class="number">1</span>)</span><br><span class="line">sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><strong>text</strong>：通过CLIP模型将文本编码到(1,M,C)</li></ul><h1 id="SAM的mask-prompt如何处理？"><a href="#SAM的mask-prompt如何处理？" class="headerlink" title="SAM的mask prompt如何处理？"></a>SAM的mask prompt如何处理？</h1><ul><li>mask利用CNN输出和image_embedding(1,C,H/16,W/16)一样大小的编码，后续直接相加</li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mask_downscaling = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, mask_in_chans // <span class="number">4</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    LayerNorm2d(mask_in_chans // <span class="number">4</span>),</span><br><span class="line">    activation(),</span><br><span class="line">    nn.Conv2d(mask_in_chans // <span class="number">4</span>, mask_in_chans, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    LayerNorm2d(mask_in_chans),</span><br><span class="line">    activation(),</span><br><span class="line">    nn.Conv2d(mask_in_chans, embed_dim, kernel_size=<span class="number">1</span>),</span><br><span class="line">)</span><br><span class="line">mask_input_size = (<span class="number">4</span> * image_embedding_size[<span class="number">0</span>], <span class="number">4</span> * image_embedding_size[<span class="number">1</span>])</span><br><span class="line">no_mask_embed = nn.Embedding(<span class="number">1</span>, embed_dim) </span><br><span class="line"><span class="keyword">if</span> masks <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    dense_embeddings = self._embed_masks(masks) <span class="comment"># 利用CNN生成mask embedding</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    dense_embeddings = self.no_mask_embed.weight.reshape(<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).expand(</span><br><span class="line">        bs, -<span class="number">1</span>, self.image_embedding_size[<span class="number">0</span>], self.image_embedding_size[<span class="number">1</span>]</span><br><span class="line">    ) <span class="comment"># 随机初始化生成mask embedding</span></span><br></pre></td></tr></table></figure></li></ul><h1 id="SAM-的-mask-decoder"><a href="#SAM-的-mask-decoder" class="headerlink" title="SAM 的 mask decoder?"></a>SAM 的 mask decoder?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAM-4.png" alt=""></li><li><strong>输入</strong>:image_embedding(1, C, H/16, W/16)、image_embedding大小的位置编码image_pe(1, C, H/16, W/16)、稀疏提示编码sparse_prompt_embeddings(1, N, C)、密集提示编码dense_prompt_embeddings(1,C,H/16, W/16)</li><li><strong>(1)tansformer整合所有编码</strong>:将image_embedding+dense_prompt_embeddings视为transformer encoder的k,image_pe视为pos embedding,sparse_prompt_embeddings视为decoder的q，并且参考VIT的class_token，不直接使用sparse_prompt_embeddings输出作为最终结果，而是另外生成1个iou token和P个mask token作为最终结果，所以输入transformer decoder的token变为(1,1+P+N,C)，经过transformer后decoder和encoder分别输出hs(1,1+P+N,C), src(1,HW/256,C)；<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"> num_multimask_outputs=<span class="number">3</span></span><br><span class="line">transformer_dim=<span class="number">256</span></span><br><span class="line"> iou_token = nn.Embedding(<span class="number">1</span>, transformer_dim)</span><br><span class="line"> num_mask_tokens = num_multimask_outputs + <span class="number">1</span></span><br><span class="line"> mask_tokens = nn.Embedding(num_mask_tokens, transformer_dim)</span><br><span class="line"> <span class="comment"># Concatenate output tokens</span></span><br><span class="line"> output_tokens = torch.cat([iou_token.weight, mask_tokens.weight], dim=<span class="number">0</span>) <span class="comment"># torch.Size([5, 256])</span></span><br><span class="line"> output_tokens = output_tokens.unsqueeze(<span class="number">0</span>).expand(sparse_prompt_embeddings.size(<span class="number">0</span>), -<span class="number">1</span>, -<span class="number">1</span>) <span class="comment"># torch.Size([1, 5, 256])</span></span><br><span class="line"> tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=<span class="number">1</span>) <span class="comment"># torch.Size([1, 12, 256])</span></span><br><span class="line"> <span class="comment"># Expand per-image data in batch direction to be per-mask</span></span><br><span class="line"> src = torch.repeat_interleave(image_embeddings, tokens.shape[<span class="number">0</span>], dim=<span class="number">0</span>) <span class="comment"># torch.Size([1, 256, 64, 64]) -》torch.Size([1, 256, 64, 64])</span></span><br><span class="line"> src = src + dense_prompt_embeddings <span class="comment"># torch.Size([1, 256, 64, 64])+torch.Size([1, 256, 64, 64])</span></span><br><span class="line"> pos_src = torch.repeat_interleave(image_pe, tokens.shape[<span class="number">0</span>], dim=<span class="number">0</span>) <span class="comment"># torch.Size([1, 256, 64, 64])</span></span><br><span class="line"> b, c, h, w = src.shape</span><br><span class="line"> <span class="comment"># Run the transformer torch.Size([1, 256, 64, 64])，torch.Size([1, 256, 64, 64])，torch.Size([1, 12, 256])</span></span><br><span class="line"> hs, src = transformer(src, pos_src, tokens) <span class="comment"># torch.Size([1, 12, 256]) torch.Size([1, 4096, 256]) = q,k</span></span><br><span class="line"> iou_token_out = hs[:, <span class="number">0</span>, :] <span class="comment"># torch.Size([1, 256])</span></span><br><span class="line"> mask_tokens_out = hs[:, <span class="number">1</span> : (<span class="number">1</span> + num_mask_tokens), :] <span class="comment"># torch.Size([1, 4, 256])</span></span><br></pre></td></tr></table></figure></li><li><strong>(2)生成Mask预测</strong>：取hs的第1-P个token作为预测结果mask_tokens_out，src经过反卷积上采样4倍，输出upscaled_embedding(1,HW/16,C’)，mask_tokens_out经过MLP操作，将隐变量长度变为C’,即输出hyper_in(1,P,C’)，hyper_in与upscaled_embedding点乘后输出masks(1,P,HW/16)，表示p个mask<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">self.output_upscaling = nn.Sequential(</span><br><span class="line">    nn.ConvTranspose2d(transformer_dim, transformer_dim // <span class="number">4</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    LayerNorm2d(transformer_dim // <span class="number">4</span>),</span><br><span class="line">    activation(),</span><br><span class="line">    nn.ConvTranspose2d(transformer_dim // <span class="number">4</span>, transformer_dim // <span class="number">8</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    activation(),</span><br><span class="line">)</span><br><span class="line">self.output_hypernetworks_mlps = nn.ModuleList(</span><br><span class="line">    [MLP(transformer_dim, transformer_dim, transformer_dim // <span class="number">8</span>, <span class="number">3</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_mask_tokens)])  </span><br><span class="line"><span class="comment"># Upscale mask embeddings and predict masks using the mask tokens</span></span><br><span class="line">src = src.transpose(<span class="number">1</span>, <span class="number">2</span>).view(b, c, h, w) <span class="comment"># torch.Size([1, 256, 64, 64])</span></span><br><span class="line">upscaled_embedding = self.output_upscaling(src) <span class="comment"># torch.Size([1, 256, 64, 64]) -》torch.Size([1, 32, 256, 256])</span></span><br><span class="line">hyper_in_list: <span class="type">List</span>[torch.Tensor] = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_mask_tokens):</span><br><span class="line">    hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :])) <span class="comment"># torch.Size([1, 32])x4</span></span><br><span class="line">hyper_in = torch.stack(hyper_in_list, dim=<span class="number">1</span>) <span class="comment"># torch.Size([1, 4, 32])</span></span><br><span class="line">b, c, h, w = upscaled_embedding.shape <span class="comment"># torch.Size([1, 32, 256, 256])</span></span><br><span class="line"><span class="comment"># 运算符@表示矩阵的点乘</span></span><br><span class="line">masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -<span class="number">1</span>, h, w) <span class="comment"># torch.Size([1, 4, 32]) @ torch.Size([1, 32, 256, 256]) -&gt; torch.Size([1, 4, 256, 256])</span></span><br></pre></td></tr></table></figure></li><li><strong>(3)生成 IOU 预测</strong>：取 hs 的第1个 token 作为预测结果 iou_token_out，然后使用 MLP 将隐变量长度变为 P，表示 P 个mask 的 iou 预测<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">iou_prediction_head = MLP(transformer_dim, iou_head_hidden_dim, num_mask_tokens, iou_head_depth)</span><br><span class="line"><span class="comment"># Generate mask quality predictions</span></span><br><span class="line">iou_pred = iou_prediction_head(iou_token_out) <span class="comment"># torch.Size([1,256]) -&gt; torch.Size([1, 4])  </span></span><br></pre></td></tr></table></figure></li></ul><h1 id="SAM-如何直接分割所有目标？"><a href="#SAM-如何直接分割所有目标？" class="headerlink" title="SAM 如何直接分割所有目标？"></a>SAM 如何直接分割所有目标？</h1><ul><li>以原图所有cell作为point prompt输入，输出Mask和iou后，通过iou阈值过滤mask,得到所有目标的mask</li></ul><p>参考：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/621040230">模型方法—-真的分割任何东西(Segment Anything) - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;SAM通过transformer将点、框、Mask、文本等prompt和图片进行编码学习，可以实现对图片任意目标的分割&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
    <category term="分割" scheme="https://shaogui.life/tags/%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>我的目标检测习路线</title>
    <link href="https://shaogui.life/2023/05/16/%E6%88%91%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    <id>https://shaogui.life/2023/05/16/%E6%88%91%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%A0%E8%B7%AF%E7%BA%BF/</id>
    <published>2023-05-16T07:43:54.000Z</published>
    <updated>2023-05-21T08:24:06.235Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-02-09%2015.44.06.excalidraw.png" alt="Drawing 2023-02-09 15.44.06.excalidraw"></p><p>本文总结自己目前对 目标检测 的认识，和学习过程</p><span id="more"></span><h1 id="什么是目标检测？"><a href="#什么是目标检测？" class="headerlink" title="什么是目标检测？"></a>什么是目标检测？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-20230408141236.png" alt=""></li><li>目标检测：精确的定位出图像中某一物体<strong>类别信息</strong>和<strong>所在位置</strong></li><li>2014年，基于候选区域 (two stage)的目标检测算法代表算法RCNN 发表，2015年基于回归 (one stage)的目标检测算法代表 YOLOv1 发表。前者往往准确度更高但速度上较慢，后者往往更快但准头略差一些，目前在向 one-satge、anchor-free 的方向发展</li></ul><h1 id="“目标检测”的疑问"><a href="#“目标检测”的疑问" class="headerlink" title="“目标检测”的疑问"></a>“目标检测”的疑问</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-14%2020.55.52.excalidraw.png" alt="Drawing 2023-03-14 20.55.52.excalidraw"></li><li>以前不清楚目标检测的时候，老是对一个问题不明白：<strong>一张图片上目标数量不定，但神经网络的最后输出一定是定长的，那怎么预测一张图上的多个目标呢？</strong></li></ul><h1 id="检测目标检测的“朴素想法”"><a href="#检测目标检测的“朴素想法”" class="headerlink" title="检测目标检测的“朴素想法”"></a>检测目标检测的“朴素想法”</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-16%2021.34.10.excalidraw.png" alt="Drawing 2023-03-16 21.34.10.excalidraw"></li><li>就拿我们最熟悉的<strong>分类</strong>来说：已知分类模型的最后输出等于类别数量 (N)，而且计算损失的时候也是输出 (N)与 gt (N)等长度的向量计算，也就是说每个输出都参与损失的计算 (假设是多标签分类)</li><li>但是<strong>目标检测</strong>情况有点不一样，即图片上的目标数量是未知的，有2种思路去解决模型输出的问题：<strong>RCNN 系列</strong>：先生成能覆盖住目标<strong>大量的获选框</strong>，然后将网络输出固定为框的分类及定位预；<strong>YOLO 系列</strong>：直接输入全图，然后模型<strong>设计大量的输出</strong>，然后选择某些输出去监督网络的学习，最后过滤所有输出，找到预测框</li><li>无论是先生成大量获选框、还是设计大量的输出，都是为了使用“大数量”的输出确保覆盖目标数量不定的图片</li></ul><h1 id="目标检测常用的方法？"><a href="#目标检测常用的方法？" class="headerlink" title="目标检测常用的方法？"></a>目标检测常用的方法？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-15%2008.31.34.excalidraw.png" alt="Drawing 2023-03-15 08.31.34.excalidraw"></li><li>two-stage：先找出目标的大量获选框，然后对获选框进行分类和回归，实现对目标的检测</li><li>one-stage：从最后的 featrue map 开始，在 grid 设置多个 anchor，然后对这些 anchor 进行分类和位置检测，实现对目标的检测</li><li>anchor-free：从最后的 featrue map 开始，直接用每个 grid 去学习目标的类别及 gt 框</li></ul><h1 id="目标检测的难点"><a href="#目标检测的难点" class="headerlink" title="目标检测的难点"></a>目标检测的难点</h1><ol><li><strong>图片数据少</strong>：训练样本在尺度、角度、光照、位置等维度不够丰富，所以常使用数据增强，包括 CutMix、MixUp、Mosic 等</li><li><strong>BackBone 提取特征能力弱</strong>：BackBone 提取特征能力，决定检测的上限，依次使用 VGGNet (YOLOv1)-&gt;DarkNet 19 (YOLOv2)-&gt;DarkNet 53 (YOLOv3)-&gt;CSPDarkNet 53 (YOLOv4/v5)-&gt;RepVGG（YOLOv6）-&gt;E-ELAN（YOLOv7）</li><li><strong>Neck 特征整合能力弱</strong>：Neck 的特征整合能力是后续 Head 的基础，依次经历 FPN-&gt;PAN</li><li><strong>Head 解藕目标能力弱</strong>：开始时使用单 head 检测，但是由于图片上混淆大目标、小目标，发现多 head 对其进行分而治之的方法更有效</li><li><strong>样本分配不准确</strong>：多 head +多 anchor 的检测方式，导致模型有大量输出，如何将 gt 框准确分配到这些输出上，计算的损失能更有效监督网络学习，是比较难的问题。依次经历：静态分配 (位置、单 IOU 阈值、多 IOU 阈值、Shape)-&gt;动态分配（先计算损失再确定用那个检测器检测某个 gt），从正样本来说，依次经历单正样本、多正样本的过程</li><li><strong>损失函数不够准确</strong>：损失函数主要有 3 个，置信度损失和类别损失通常使用 L1 loss、交叉熵损失、focal loss；预测框损失改进比较多，依次是：L 1 loss (YOLOv1、YOLOv2、YOLOv3)-&gt;IOU loss（YOLOX）-&gt;GIOU loss (YOLOv5)-&gt;DIOU loss (YOLOv4)-&gt;CIOU loss (YOLOv7)-&gt;SIOU loss (YOLOv6)</li><li><strong>解析模型输出能力不强</strong>：目标检测模型大多都使用多 head 输出，如果还设置了 anchor，其输出的预测框远多于 gt 框，需要从这些预测框中解析出目标，依次经历 NMS-&gt;soft NMS-&gt;softer NMS</li></ol><h1 id="目标检测如何进行样本匹配？"><a href="#目标检测如何进行样本匹配？" class="headerlink" title="目标检测如何进行样本匹配？"></a>目标检测如何进行样本匹配？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-16%2023.06.28.excalidraw.png" alt="Drawing 2023-03-16 23.06.28.excalidraw"></li><li><strong>静态分配</strong>：按照位置、IOU、Shape 提前将 gt 框分配到某个检测器，<strong>所有的 anchor-base+单头输出的 anchor-free</strong>，比如：YOLOv1-YOLOv5、SSD、RetinaNet、CornerNet、CenterNet、FOCS</li><li><strong>动态分配</strong>：根据模型输出和 gt 框的损失，选择损失小的去监督网络，<strong>多 head 输出 anchor-free</strong> 均使用这种方式，比如 YOLOX、YOLOF、YOLOv6、YOLOv7</li></ul><h1 id="如何解析模型输出？"><a href="#如何解析模型输出？" class="headerlink" title="如何解析模型输出？"></a>如何解析模型输出？</h1><ul><li><strong>Two-satge</strong>：由于提取了大量的候选框，不需要复杂的解码，模型输出就是预测结果，需要使用 NMS 进一步过滤结果</li><li><strong>One-stage</strong>：该系列模型往往预测了每个 grid 的<strong>置信度</strong>，从这个置信度开始解析结果。(1)使用置信度阈值选定获选预测框；(2)根据选择的置信度位置拿到对应的分类打分、位置预测；(3)将置信度 x 分类打分作为分数，使用 IOU 阈值对所有选择出来的获选预测框进行 NMS</li><li><strong>anchor-free</strong>：该系列模型往往输出所有 grid 的 headmap，即对 grid 的分类打分，从这个打分开始解析模型输出。(1)选择 heatmap 上 topK 高的打分；(2)根据选择出来的位置确定目标的中心，然后拿出对应位置框其他属性 (宽高、中心到四边距离)；(3)以 heatmap 上的打分作为分数，，使用 IOU 阈值对所有选择出来的获选预测框进行 NMS</li></ul><h1 id="目标检测的评价指标？"><a href="#目标检测的评价指标？" class="headerlink" title="目标检测的评价指标？"></a>目标检测的评价指标？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-14%2020.11.25.excalidraw.png" alt="Drawing 2023-03-14 20.11.25.excalidraw"> </li><li>目标检测常使用 mAP 评价模型性能，注意：<strong>统计某类的 TP、FP、FN 时，是针对所有图片<mark style="background: #FFB8EBA6;">目标框</mark>预测结果进行，不针对具体图片</strong></li><li>mAP 是所有类别 AP 值的平均</li><li>每个 PR 区域是某个 IOU 阈值绘制的，并且这个 IOU 阈值已经由单阈值发展到多阈值</li></ul><h1 id="学习路线"><a href="#学习路线" class="headerlink" title="学习路线"></a>学习路线</h1><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-02-09%2015.44.06.excalidraw.png" alt="Drawing 2023-02-09 15.44.06.excalidraw"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-02-09%2015.44.06.excalidraw.png&quot; alt=&quot;Drawing 2023-02-09 15.44.06.excalidraw&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文总结自己目前对 目标检测 的认识，和学习过程&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="目标检测" scheme="https://shaogui.life/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>我的语义分割学习路线</title>
    <link href="https://shaogui.life/2023/05/15/%E6%88%91%E7%9A%84%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    <id>https://shaogui.life/2023/05/15/%E6%88%91%E7%9A%84%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/</id>
    <published>2023-05-15T15:30:22.000Z</published>
    <updated>2023-05-25T14:35:55.518Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-02-14%2011.31.41.excalidraw.png" alt="Drawing 2023-02-14 11.31.41.excalidraw"></p><p>本文总结自己目前对语义分割的认识，和学习过程</p><span id="more"></span><h1 id="什么是语义分割？"><a href="#什么是语义分割？" class="headerlink" title="什么是语义分割？"></a>什么是语义分割？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2%E9%80%9A%E7%94%A8%E7%9F%A5%E8%AF%86-20230408143823.jpeg" alt=""></li><li>语义分割是针对图片像素点的分类，1个像素的类别可以是单标签，也可以是多标签</li></ul><h1 id="语义分割原理"><a href="#语义分割原理" class="headerlink" title="语义分割原理"></a>语义分割原理</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-19%2022.02.18.excalidraw.png" alt="Drawing 2023-03-19 22.02.18.excalidraw"></li><li>图片经过 CNN 提取特征后，得到的是分辨率变小的2D featrue map，通过上采样将 featrue map 的分辨率变大，然后预测每个 grid 的类别（可以是单标签预测，也可以是多标签）</li></ul><h1 id="语义分割难点"><a href="#语义分割难点" class="headerlink" title="语义分割难点"></a>语义分割难点</h1><ol><li><strong>分辨率下降</strong>：CNN提取的特征平移不变性，这对分类任务很有用，但是对分割来说，希望原图目标移动后，其特征的响应也在移动，因此分辨率下降导致最后featrue map包含位置信息少。通常使用“跳跃连接”解决，即将包含位置信息的高分辨率特征和包含语义信息的低分辨率融合解决</li><li><strong>感受野小</strong>：CNN下采样倍率一般比较小，所以随后featrue map上每个grid的感受野一般不大，这就意味着每个特征接收少部分其他像素的信息，这对大尺度的目标来说是非常不利的。通常使用“空洞卷积”解决，即在不加深网络的情况下提高感受野</li><li><strong>目标多尺度</strong>：图片存在多尺度的目标，如果仅使用一种分辨率去做最后分类，对其他分辨率效果不佳。通常使用多尺度特征融合来解决，比如PSP、ASPP模块</li><li><strong>依赖距离短</strong>：这和感受野的影响类似，但是即使感受野再大，也不能大过原图，所以像素之间的长程依赖还不够。通常使用“自注意力机制”去解决，比如构建特征的通道注意力、空间注意力去创建这种依赖</li></ol><h1 id="语义分割模型的种类"><a href="#语义分割模型的种类" class="headerlink" title="语义分割模型的种类"></a>语义分割模型的种类</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-19%2022.20.10.excalidraw.png" alt="Drawing 2023-03-19 22.20.10.excalidraw"></li><li><strong>金字塔模型</strong>：通过构建并融合多尺度特征，实现对不同尺度目标的分割，代表模型有 DeepLab 系列，PSPNet，DANet，APCNet</li><li><strong>编码器-解码器</strong>：使用 CNN 下采样提取特征，然后使用线性插值、反卷积、反池化操作实现上采样，并通过跳跃连接将高分辨率的位置信息联通到低分辨率的语义特征</li><li><strong>“自注意力”系列</strong>：通过引入“自注意力”机制，构建像素之间的远程连接，解决感受野解决不了的尺度问题</li></ul><h1 id="语义分割的上采样类型"><a href="#语义分割的上采样类型" class="headerlink" title="语义分割的上采样类型"></a>语义分割的上采样类型</h1><p>语义分割在还原分辨率时，通常使用上采样，不同的上采样在速度、精度有不同区别</p><div class="table-container"><table><thead><tr><th>方法</th><th>描述</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>线性插值</td><td>通过相邻的元素决定待插值点的值，如最近邻插值、线性插值、双3次线性插值</td><td>快速、无需学习</td><td>-</td></tr><tr><td>反池化</td><td>记录池化时的激活位置，上采样时直接将值赋值给这个位置</td><td>无需学习</td><td>需要额外存储记录激活；上采样效果不好</td></tr><tr><td>反卷积</td><td>通过反卷积上采样</td><td>可以被学习优化</td><td>增加模型计算，有网格效应</td></tr></tbody></table></div><h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-20230408153654.png" alt="机器学习常见评价指标-20230408153654"></p><ul><li>语义分割常使用 mIOU 作为统计指标，注意：<strong>统计某类 TP、FP、FN 指标时，是针对所有图片的所有<mark style="background: #BBFABBA6;">像素</mark>预测结果、而不是具体一张图片</strong></li><li>首先统计某个类别在所有图片上的累计 TP、FP、FN 像素数量、然后计算这个类别的 IOU ，再算所有类别的平均<script type="math/tex; mode=display">I o U=\frac{T P}{F P+T P+F N}</script></li></ul><h1 id="学习路线"><a href="#学习路线" class="headerlink" title="学习路线"></a>学习路线</h1><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-02-14%2011.31.41.excalidraw.png" alt="Drawing 2023-02-14 11.31.41.excalidraw"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-02-14%2011.31.41.excalidraw.png&quot; alt=&quot;Drawing 2023-02-14 11.31.41.excalidraw&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文总结自己目前对语义分割的认识，和学习过程&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="语义分割" scheme="https://shaogui.life/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>我的人体姿态估计学习路线</title>
    <link href="https://shaogui.life/2023/05/15/%E6%88%91%E7%9A%84%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    <id>https://shaogui.life/2023/05/15/%E6%88%91%E7%9A%84%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/</id>
    <published>2023-05-15T14:30:22.000Z</published>
    <updated>2023-05-21T08:18:36.857Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-13%2023.28.32.excalidraw.png" alt="Drawing 2023-04-13 23.28.32.excalidraw"></p><p>本文总结自己目前对人体姿态估计的认识，和学习过程</p><span id="more"></span><h1 id="什么是人体姿态检测？"><a href="#什么是人体姿态检测？" class="headerlink" title="什么是人体姿态检测？"></a>什么是人体姿态检测？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-20230417230640.png" alt=""></li><li><strong>在人体关节连接的所有姿势空间中搜索某个特定姿势，本质为关节点的定位</strong></li><li>点之间形成的连接必须很重要，这意味着并非所有点都可以形成一对。从一开始，HPE 的目标就是形成类似骨骼的人体表示，然后针对特定任务的应用对其进行进一步处理</li></ul><h1 id="人体姿态检测的方法-？"><a href="#人体姿态检测的方法-？" class="headerlink" title="人体姿态检测的方法 ？"></a>人体姿态检测的方法 ？</h1><ul><li><strong>从上到下 (二阶段)</strong> ：主要包含两个部分，目标检测和单人人体骨骼关键点检测，对于目标检测算法，这里不再进行描述，而对于关键点检测算法，首先需要注意的是关键点局部信息的区分性很弱，即背景中很容易会出现同样的局部区域造成混淆，所以需要考虑较大的感受野区域；其次人体不同关键点的检测的难易程度是不一样的，对于腰部、腿部这类关键点的检测要明显难于头部附近关键点的检测，所以不同的关键点可能需要区别对待；最后自上而下的人体关键点定位依赖于检测算法的提出的 Proposals，会出现检测不准和重复检测等现象。如：CPM、CPN、RMPE</li><li><strong>从下到上 (单阶段)</strong> ：主要包含两个部分，关键点检测和关键点聚类，其中关键点检测和单人的关键点检测方法上是差不多的，区别在于这里的关键点检测需要将图片中所有类别的所有关键点全部检测出来，然后对这些关键点进行聚类处理，将不同人的不同关键点连接在一块，从而聚类产生不同的个体。如：PAFs</li></ul><h1 id="人体姿态检测的评价标准PCK？"><a href="#人体姿态检测的评价标准PCK？" class="headerlink" title="人体姿态检测的评价标准PCK？"></a>人体姿态检测的评价标准PCK？</h1><ul><li><strong>预测关键点与 GT 关键点的归一化距离，如果小于某个阈值，该关键点预测正确，否则失败，PCK 等于预测正确的比例</strong>，这个值和阈值有关，有 PCK@0.2 、 PCKh@0.5 ，PCK 用于 2D 和 3D ，值越高越好</li><li>PCK 值是针对一个人的关键点来说的，也就说<strong>不存在预测结果多于 gt 的问题，或者说这个问题在计算 PCK 前已经解决</strong>，计算多人的 PCK 值是所有人 PCK 值的平均</li><li>1）<strong>计算某个关键点的的 PCK 值</strong>：即以下公式，其中 i 表示 id=i 的关键点，k 表示第 k 个阈值 $T^k$ ，p 表示第 p 个行人，$d_{pi}$ 表示第 p 个人的 id=1 的预测关键点与 GT 关键点之间的欧式距离，$d_p^{def}$ 是第 p 个人的尺度因子，$T_k \in [0:0.001:0.1]$ 表示人工设定的阈值 <script type="math/tex; mode=display">PCK_i^k=\dfrac{\sum_p\delta(\frac{d_{pi}}{d_p^{def}}\leq T_k)}{\sum_p1}</script></li><li>2）<strong>计算 PCK 值</strong>：所有人所有关键点的 PCK 值的平均是模型的 PCK 值 <script type="math/tex; mode=display">PCK^k_{mean}=\dfrac{\sum_p\sum_i\delta(\frac{d_{pi}}{d_p^{def}}\leq T_k)}{\sum_p\sum_i1}</script></li><li>不同的数据集 $d_p^{def}$ 计算方法不同，MPII 中是以头部长度（头部矩形框的左上点与右下点的欧式距离）作为归一化参考，即 <strong>PCKh</strong></li></ul><h1 id="人体姿态检测的评价标准Oks？"><a href="#人体姿态检测的评价标准Oks？" class="headerlink" title="人体姿态检测的评价标准Oks？"></a>人体姿态检测的评价标准Oks？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Pasted%20image%2020230505085804.png" alt=""></li><li>PCK 仅通过归一化的欧式距离就决定关键点的预测准确度，OKs 在此基础上，加入关键点的标注难易偏差。上图是两个关键点的 OKS 值，</li><li>以下公式是 OKs 指标，其中 p 表示第 p 个行人，$p^i$ 表示第 p 个行人的关键点 i，$d<em>{pi}$ 是第 p 个行人的关键点 i 的预测位置与 GT 位置的欧式距离，$v</em>{p^i}=1、v_{p^i}=2$ 表示关键点无遮挡且标注、有遮挡且标注，$S_p$ 表示第 p 个行人的尺度因子，其值为行人检测框 (w, h)面积平方根 $S_p=\sqrt{wh}$，$\sigma_i$ 表示类型为 i 的关键点归一化因子，<script type="math/tex; mode=display">OKS_p=\dfrac{\sum_i exp\{-d_{pi}^2/2S_p^2\sigma_i^2\}\delta(v_{pi}>0)}{\sum_i\delta(v_{pi}>0)}</script></li><li>$\sigma_i$ 这个因子是通过对所有的样本的 GT 关键点由人工标注与真实值存在的标准差， $\sigma$  越大表示此类型的关键点越难标注。coco 数据集统计出17类关键点的归一化因子：{鼻子：0.026，眼睛：0.025，耳朵：0.035，肩膀：0.079，手肘：0.072，手腕：0.062，臀部：0.107，膝盖：0.087，脚踝：0.089}</li></ul><h1 id="人体姿态检测的评价标准mAp？"><a href="#人体姿态检测的评价标准mAp？" class="headerlink" title="人体姿态检测的评价标准mAp？"></a>人体姿态检测的评价标准mAp？</h1><ul><li>OKS 在物体检测中扮演的角色与 IoU 的作用相同。它是根据预测点和由人的比例对地面实况点之间的距离计算得出的。论文常出现以下指标 AP50（OKS = 0.50 的 AP）、AP（OKS 在[0.5:0.05:0.95]10 个位置的 AP 值）</li><li><strong>单人姿态估计 AP：</strong> 计算出 groundtruth 与检测得到的关键点的相似度<strong>oks</strong>为一个标量，然后人为的给定一个阈值<strong>T</strong>，然后可以通过所有图片的<strong>oks</strong>计算<strong>AP</strong><script type="math/tex; mode=display">AP=\dfrac{\sum_p\delta(oks_p>T)}{\sum_p1}</script></li><li><strong>多人姿态估计 AP：</strong> 1）如果采用的检测方法是自顶向下，先把所有的人找出来再检测关键点，那么其<strong>AP</strong>计算方法如同<strong>单人姿态估计 AP</strong>；2）如果采用的检测方法是自底向上，先把所有的关键点找出来然后再组成人。假设一张图片中有 M 个人，预测出 N 个人，由于不知道预测出的<strong>N</strong>个人与 groundtruth 中的<strong>M</strong>个人的一一对应关系，因此需要计算 groundtruth 中每一个人与预测的<strong>N</strong>个人的<strong>oks</strong>，那么可以获得一个大小为<em>M</em>×<em>N</em>的矩阵，矩阵的每一行为 groundtruth 中的一个人与预测结果的<strong>N</strong>个人的<strong>oks</strong>，然后找出每一行中<strong>oks</strong>最大的值作为当前<strong>GT</strong>的<strong>oks</strong>。最后每一个<strong>GT</strong>行人都有一个标量<strong>oks</strong>，然后人为的给定一个阈值<strong>T</strong>，然后可以通过所有图片中的所有行人计算<strong>AP</strong><script type="math/tex; mode=display">AP=\dfrac{\sum_m\sum_p\delta(oks_p>T)}{\sum_m\sum_p1}</script></li></ul><h1 id="学习路线"><a href="#学习路线" class="headerlink" title="学习路线"></a>学习路线</h1><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-13%2023.28.32.excalidraw.png" alt="Drawing 2023-04-13 23.28.32.excalidraw"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-13%2023.28.32.excalidraw.png&quot; alt=&quot;Drawing 2023-04-13 23.28.32.excalidraw&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文总结自己目前对人体姿态估计的认识，和学习过程&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="人体姿态估计" scheme="https://shaogui.life/tags/%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>我的实例分割学习路线</title>
    <link href="https://shaogui.life/2023/05/14/%E6%88%91%E7%9A%84%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    <id>https://shaogui.life/2023/05/14/%E6%88%91%E7%9A%84%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/</id>
    <published>2023-05-14T15:55:34.000Z</published>
    <updated>2023-05-21T08:18:31.144Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2013.39.58.excalidraw.png" alt="Drawing 2023-03-22 13.39.58.excalidraw"></p><p>本文总结自己目前对实例分割 的认识，和学习过程</p><span id="more"></span><h1 id="什么是实例分割？"><a href="#什么是实例分割？" class="headerlink" title="什么是实例分割？"></a>什么是实例分割？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2010.13.11.excalidraw.png" alt="Drawing 2023-03-22 10.13.11.excalidraw"></li><li>目标检测针对的是目标，语义分割针对的是像素，而实例分割针对的是实例。所谓实例就是一个不管类别、不管是否连续的 1 个目标</li><li>上图是对一张图上 3 类 4 个实例的分割示意图，最后的输出结果是<strong>每个实例的语义分割图</strong></li></ul><h1 id="实例分割的原理？"><a href="#实例分割的原理？" class="headerlink" title="实例分割的原理？"></a>实例分割的原理？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2010.27.55.excalidraw.png" alt="Drawing 2023-03-22 10.27.55.excalidraw"></li><li><strong>Two-satge</strong>：既然实例分割是针对实例的语义分割，最直接的办法就是先找出单个实例的区域，然后对这个区域进行语义分割即可</li><li><strong>one-stage</strong>：不找实例的区域，而是针对每个 grid 生成 1 个语义分割预测，通过后处理获得实例的类别及分割结果</li></ul><h1 id="实例分割的方法？"><a href="#实例分割的方法？" class="headerlink" title="实例分割的方法？"></a>实例分割的方法？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2009.38.03.excalidraw.png" alt="Drawing 2023-03-22 09.38.03.excalidraw"></li><li><mark style="background: #FF5582A6;">two-satge</mark>：类似于目标检测的 two-satge 模型，即<strong>先检测出目标的获选框，然后对候选框进行语义分割和位置回归</strong></li><li><mark style="background: #FFB86CA6;">one-stage</mark>：类似于目标检测的 one-stage 模型，即<strong>先通过 grid 确保这个位置有目标，然后对这个位置进行位置回归 (目标检测)或者 mask 生成 (实例分割)</strong></li><li>two-stage 的方法原始直接，但是对重叠目标的 Mask 预测比较麻烦，而且速度较慢；one-stage 类似 YOLO 系列的思想，速度较快</li></ul><h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-14%2020.11.25.excalidraw.png" alt="Drawing 2023-03-14 20.11.25.excalidraw"></li><li>和目标检测一样，实例分割使用 mAP 评价模型性能，注意：<strong>统计某类的 TP、FP、FN 时，是针对所有图片的<mark style="background: #FF5582A6;">实例</mark>预测结果进行，不针对具体图片</strong></li><li>AP 是指某个类别预测情况的平均精准率，mAP 指所有类别 AP 的平均</li><li>AP 可以通过求解 PR 曲线下的面积得到，求解方式包括11个点和矩形求解</li><li>每个 PR 区域是某个 IOU 阈值绘制的，并且这个 IOU 阈值已经由单阈值发展到多阈值</li></ul><h1 id="two-stage"><a href="#two-stage" class="headerlink" title="two-stage"></a>two-stage</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2013.46.18.excalidraw.png" alt="Drawing 2023-03-22 13.46.18.excalidraw"></li><li>自上而下的实例分割方法，首先按获取候选框，然后在此基础上进行目标框回归和 Mask 生成</li><li><strong>Mask RCNN</strong>：使用 RPN 获得候选框、使用 Faster RCNN 预测目标类别、使用 FCN 生成 Mask</li><li><strong>PAN</strong>：通过在 FPN 的基础上引入 bottom-up 路径，让底层信息更快传递到高层，其思想和 Mask RCNN 一致</li></ul><h1 id="One-stage"><a href="#One-stage" class="headerlink" title="One-stage"></a>One-stage</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2013.47.15.excalidraw.png" alt="Drawing 2023-03-22 13.47.15.excalidraw"></li><li>自下而上的实例分割方法类似 YOLO 系列将每个 grid 看作 1 个目标，这类实例分割方法将每个 grid 视为一个实例，并为每个 grid 预测 1 个 Mask</li><li><strong>SOLO</strong>：输出包含 2 个分支，一个是 heatmap 分支，判定该 grid 是否包含实例，另一个分支是为该 grid 生成 Mask</li><li><strong>SOLOv2</strong>：在 SOLOv1 的基础上，将 Mask 分支解藕卷积核生成、卷积特征生成 2 个分支，监督网络学习卷积核，使得网络能动态学习实例的特征</li></ul><h1 id="学习路线"><a href="#学习路线" class="headerlink" title="学习路线"></a>学习路线</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2013.39.58.excalidraw.png" alt="Drawing 2023-03-22 13.39.58.excalidraw"></li></ul><ol><li><a href="MaskRCNN.md">MaskRCNN</a>：在 Faster RCNN 的基础上增加 FCN 分支，提出 ROIAlign 对齐 ROI 下采样</li><li><a href="PAN.md">PAN</a>：类似 MaskRCNN 过程，BackBone 使用 PAN 进行特征融合，最后融合所有尺度目标进行目标定位与分割</li><li><a href="yolact.md">yolact</a>：首先生成一批 prototype mask，然后目标分支生成一组权重，加权得到每个 grid 的分割结果</li><li><a href="SOLO.md">SOLO</a>：首先 Category 分支对 grid 进行分类，然后 Mask 分支生成每个 grid 的分割，实际使用通过解藕头构建 Mask 分支</li><li><a href="SOLOv2.md">SOLOv2</a>：将 SOLO 的 Mask 生成分支解藕为<strong>掩码核预测分支</strong>和掩码特征学习分支，分别负责生成卷积核和需要卷积的特征映射</li><li><a href="yolactplusplus.md">yolactplusplus</a>：基本和 yolact 类似，生成更多 anchor 、重新生成的 Mask scoreing 分支</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2013.39.58.excalidraw.png&quot; alt=&quot;Drawing 2023-03-22 13.39.58.excalidraw&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文总结自己目前对实例分割 的认识，和学习过程&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="实例分割" scheme="https://shaogui.life/tags/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>我的OCR学习路线</title>
    <link href="https://shaogui.life/2023/05/12/%E6%88%91%E7%9A%84OCR%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    <id>https://shaogui.life/2023/05/12/%E6%88%91%E7%9A%84OCR%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/</id>
    <published>2023-05-12T05:09:54.000Z</published>
    <updated>2023-05-21T08:18:23.254Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-11%2018.42.57.excalidraw.png" alt="Drawing 2023-04-11 18.42.57.excalidraw"></p><p>本文总结自己目前对 OCR 的认识，和学习过程</p><span id="more"></span><h1 id="什么是-OCR-？"><a href="#什么是-OCR-？" class="headerlink" title="什么是 OCR ？"></a>什么是 OCR ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/FOTS-20230409214506.png" alt=""></li><li>OCR （Optical Character Recognition，光学字符识别）指将打字、手写或印刷文本的图像电子或机械转换为机器编码文本的过程</li><li><strong>文本检测 (Text detection)</strong> ：<strong>检测文本的所在位置和范围及其布局</strong>，可以使用传统的 ROI 提取实现，也可使用目标检测去实现，如 Faster R-CNN，[[FCN]]</li><li><strong>文本识别 (Text recognition)</strong>：<strong>对文本内容进行识别</strong>，将图像中的文本信息转化为文本信息</li><li><strong>文本定位 (Text Spotting)</strong> ：分文本检测 (Text detection) 文本识别 (Text recognition)统一到一起的简称</li></ul><h1 id="OCR-的方法？"><a href="#OCR-的方法？" class="headerlink" title="OCR 的方法？"></a>OCR 的方法？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-11%2018.42.57.excalidraw.png" alt="Drawing 2023-04-11 18.42.57.excalidraw"></li><li><strong>文本检测 (Text detection)</strong> ：其实就是检测文本行<strong>实例</strong>，可以使用目标检测的方法，也可使用语义分割的方法</li><li><strong>文本识别 (Text recognition)</strong> ：这个是 OCR 的重点，主要有 3 条路线</li><li><strong>文本定位 (Text Spotting)</strong> ：分为两种，但阶段和双阶段</li></ul><h1 id="评价指标-字符评价"><a href="#评价指标-字符评价" class="headerlink" title="评价指标-字符评价"></a>评价指标-字符评价</h1><ul><li>以字符 （文字和标点符号） 为单位的统计和分析，适用于通用印刷体、手写体类非结构化数据的OCR应用评测</li><li><strong>字符召回率</strong>：预测正确的字符总数占<strong>总符号</strong>的比例</li><li><strong>字符准确率</strong>：预测正确的字符占<strong>总测试结果</strong>的比例</li><li><strong>F-socre</strong>：字符召回率和字符准确率的综合评价指标</li></ul><h1 id="评价指标-文本段评价"><a href="#评价指标-文本段评价" class="headerlink" title="评价指标-文本段评价"></a>评价指标-文本段评价</h1><ul><li>以字段为单位的统计和分析，适用于卡证类、票据类等结构化程度较高的 OCR 应用评测</li><li><strong>字段召回率</strong>：完全识别准确的字段总数占<strong>总字段</strong>的比例</li><li><strong>字段准确率</strong>：完全识别准确的字段占<strong>总测试结果</strong>的比例</li><li><strong>最小编辑距离</strong>：编辑距离是针对二个字符串（例如英文字）的差异程度的量化量测，通过替换、插入、删除，将预测结果修正为gt所需操作步骤，最小编辑距离表示最少操作步数</li><li><strong>全图编辑距离</strong>：整个文本段的编辑距离</li><li><strong>归一化编辑距离</strong>: 编辑距离除以字符串长度</li></ul><h1 id="学习路线"><a href="#学习路线" class="headerlink" title="学习路线"></a>学习路线</h1><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2014.25.52.excalidraw.png" alt="Drawing 2023-03-22 14.25.52.excalidraw"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-11%2018.42.57.excalidraw.png&quot; alt=&quot;Drawing 2023-04-11 18.42.57.excalidraw&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文总结自己目前对 OCR 的认识，和学习过程&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
  </entry>
  
  <entry>
    <title>我的图片分类习路线</title>
    <link href="https://shaogui.life/2023/05/10/%E6%88%91%E7%9A%84%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    <id>https://shaogui.life/2023/05/10/%E6%88%91%E7%9A%84%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E4%B9%A0%E8%B7%AF%E7%BA%BF/</id>
    <published>2023-05-10T15:55:34.000Z</published>
    <updated>2023-05-21T07:33:27.950Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-02-09%2023.12.59.excalidraw.png" alt="Drawing 2023-02-09 23.12.59.excalidraw"></p><p>本文总结自己目前对图片分类的认识，和学习过程</p><span id="more"></span><h1 id="什么是图片分类"><a href="#什么是图片分类" class="headerlink" title="什么是图片分类"></a>什么是图片分类</h1><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/KsBct79-DSURag2WJ5nxSJoo1Hx8ednwNpc1z3s9abE4UO2dKzlX6Lq0R-32jdqeRE4djUQ-ppUwv5uuKwxTpc19AMjmRWXQrnUPaKAvcjwQssWFFwPBK7iWrho6mzqk.png" alt=""></p><p><strong>判断图像中包含物体的类别，如果期望判别多种物体则称为多目标分类</strong></p><h1 id="图片分类的原理"><a href="#图片分类的原理" class="headerlink" title="图片分类的原理"></a>图片分类的原理</h1><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-20%2022.35.20.excalidraw.png" alt="Drawing 2023-03-20 22.35.20.excalidraw"></p><ol><li>输入一张图片，经过 CNN 提取特征后，最后输出这张图在所有类别上的概率</li><li>CNN提取特征之后通常是(BCHW)的输出，因为分类是全局结果，所以需要去掉HW，通常做法是全局池化，得到(BC),然后接Linear输出类别打分</li></ol><h1 id="CNN-网络设计原则"><a href="#CNN-网络设计原则" class="headerlink" title="CNN 网络设计原则"></a>CNN 网络设计原则</h1><p>在 CNN 架构设计上，经常修改哪些指标去提升网络性能？</p><ul><li><strong>网络的宽度 width</strong>：每层卷积的输出通道数</li><li><strong>网络的深度 depth</strong>：网络的层数</li><li><strong>网络的分辨率 resolution</strong>：输入图像的分辨率大小</li><li><strong>网络的增长率 growth</strong>：随着层数的增加，每层卷积输出通道数的增长比例</li><li><strong>网络的特征复用</strong>：如 DenseNet 可以使用更浅的网络，更少的参数，提升特征复用，达到与深度网络相当的性能</li><li><strong>高效特征融合</strong>：InceptionNet的split-transforms-merge模式，将输入分别使用不同的转换分支提取特征，然后将多个分支的结果进行合并实现特征融合</li><li><strong>上下文依赖</strong>：通过类似SENet的方式构建像素之间的上下文依赖</li></ul><h1 id="图片分类方法"><a href="#图片分类方法" class="headerlink" title="图片分类方法"></a>图片分类方法</h1><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-02-09%2023.12.59.excalidraw.png" alt="Drawing 2023-02-09 23.12.59.excalidraw"></p><ol><li><strong>基础 CNN</strong>：从深度、宽度探索 CNN 的特性</li><li><strong>残差网络</strong>：卷积神经网络半边天，使得深层网络训练成为可能</li><li><strong>轻量化网络</strong>：研究 CNN 部署到移动设备的可能</li><li><strong>注意力机制</strong>：将空间注意力，时间注意力引入到CNN</li></ol><h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-20%2022.38.00.excalidraw.png" alt="Drawing 2023-03-20 22.38.00.excalidraw"></p><ol><li>分类模型的评价指标，第一步是<strong>以图片为单位，以类别为横纵座标</strong>计算其结果的混淆矩阵，然后根据混淆矩阵求准确率、AP 值</li><li><strong>准确率</strong>：等于混淆矩阵对角线位置值之和/图片数量</li><li><strong>AP 值</strong>：每设置一次分类打分阈值，求得一个混淆矩阵，然后计算得到类别的 AP 值，遍历所有阈值，计算得到所有类别 AP 值</li></ol><h1 id="学习路线"><a href="#学习路线" class="headerlink" title="学习路线"></a>学习路线</h1><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-02-09%2023.12.59.excalidraw.png" alt="Drawing 2023-02-09 23.12.59.excalidraw"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-02-09%2023.12.59.excalidraw.png&quot; alt=&quot;Drawing 2023-02-09 23.12.59.excalidraw&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文总结自己目前对图片分类的认识，和学习过程&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="图片分类" scheme="https://shaogui.life/tags/%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>TOOD：Task-aligned One-stage Object Detection</title>
    <link href="https://shaogui.life/2023/04/26/TOOD%EF%BC%9ATask-aligned%20One-stage%20Object%20Detection/"/>
    <id>https://shaogui.life/2023/04/26/TOOD%EF%BC%9ATask-aligned%20One-stage%20Object%20Detection/</id>
    <published>2023-04-26T03:42:22.000Z</published>
    <updated>2023-05-21T08:14:56.233Z</updated>
    
    <content type="html"><![CDATA[<p>通过设计新的预测头 T-Head 和样本对齐损失 (TAL)，实现分类、定位分支的对齐，使得两个分支的最佳锚框更加接近。这样可以减少“低分类概率+准确位置预测”、“高概率预测+不太准确预测”这两种情况目标的漏检 </p><span id="more"></span><h1 id="什么是-TOOD-？"><a href="#什么是-TOOD-？" class="headerlink" title="什么是 TOOD ？"></a>什么是 TOOD ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/TOOD-20230514144236.png" alt=""></li><li><strong>TSS (Training Sample Selection)</strong>：<strong>由于分类和定位的学习机制不同，两个任务学习到的特征的空间分布可能不同，当使用两个单独的分支进行预测时，会导致一定程度的错位</strong>，如图“result”栏第一行所示，其中红色、绿色块表示分类、回归的最佳锚点，TSS 检测器识别“餐桌”对象时，分类锚点位置向上偏离真实最佳中心（甚至达到披萨的中心），目标检测锚点位置向下偏离真实最佳中心</li><li><strong>TOOD</strong>：通过设计新的预测头 T-Head 和样本对齐损失 (TAL)，实现分类、定位分支的对齐，使得两个分支的最佳锚框更加接近 (Score/IOU 从上往下看，确实调整了激活中心)。这样可以减少“低分类概率+准确位置预测”、“高概率预测+不太准确预测”这两种情况目标的漏检 (因为这两类目标可能被 NMS 过滤，TOOD 对齐后，这类目标变成“高分类概率+准确位置预测”)</li></ul><h1 id="TOOD-的网络结构？"><a href="#TOOD-的网络结构？" class="headerlink" title="TOOD 的网络结构？"></a>TOOD 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/TOOD-20230514144326.png" alt=""></li><li>T-head 首先对 FPN 特征进行预测，然后 TAL 对这两个任务给出一个一致性的度量，最后 T-head 会自动的调整分类输出和定位输出</li></ul><h1 id="TOOD-的-Task-aligned-Head-T-Head"><a href="#TOOD-的-Task-aligned-Head-T-Head" class="headerlink" title="TOOD 的 Task-aligned Head (T-Head)?"></a>TOOD 的 Task-aligned Head (T-Head)?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/TOOD-20230514144347.png" alt=""></li><li><strong>a)Paraller head</strong>：直接基于 FPN 特征输出每个 grid 的分类概率和 box 预测</li><li><strong>b) Task-algined head (T-Head)</strong>：首先使用 TAP 对 FPN 特征进行<strong>对齐</strong>，然后输出每个 grid 的分类概率和 box 预测</li><li><strong>c）Task-algined predictor (TAP)</strong>：基于 FPN 特征，如果输出对齐后的分类 heatmap，则输出 (H, W, 80)，如果输出对齐后的定位特征，则输出 (H, W, 4)，图示将两个过程融合在一张图上，实际包含两个结构</li></ul><h1 id="TOOD-的-T-Head-和-TAP"><a href="#TOOD-的-T-Head-和-TAP" class="headerlink" title="TOOD 的 T-Head 和 TAP?"></a>TOOD 的 T-Head 和 TAP?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/TOOD-20230514144347.png" alt=""></li><li>T-head 和 TAP 的目标是对齐分类、定位两个分支的锚框位置</li><li><strong>计算 M/O</strong>：按照以下公式从 FPN 特征计算得到，其中 M (H, W, 1) 用于对齐分类 headmap，O (H, W, 8) 用于对齐定位 headmap，<script type="math/tex; mode=display">\begin{gathered}M=\sigma(c o n v_{2}(\delta(c o n v_{1}(X^{i n t e r})))) \\O=conv_4(\delta(conv_3(X^{inter})))\end{gathered}</script></li><li><strong>对齐分类 headmap</strong>：基于 M 调整即可<script type="math/tex; mode=display">P^{alignn}=\sqrt{P\times M}</script></li><li><strong>对齐定位 headmap</strong>: 原始定位输出是 (H, W, 4)，结合 O (H, W, 8)输出对齐后的位置，8 是因为用于定位的 4 个输出都有 x, y 两个方向的调整。公式可知，8 的偶数位置通道用于计算 i，奇数位置通道用于计算 j  <script type="math/tex; mode=display">B^{alignn}(i,j,c)=B(i+O(i,j,2\times c),j+O(i,j,2\times c+1),c)</script></li></ul><h1 id="TOOD-的样本分配策略"><a href="#TOOD-的样本分配策略" class="headerlink" title="TOOD 的样本分配策略?"></a>TOOD 的样本分配策略?</h1><ul><li><strong>Anchor 对齐度量</strong>：根据分类的预测概率 s 和定位预测 IOU 计算以下值，该值表示预测与 gt 的相近程度，其中 $\alpha \beta$ 是自定义参数，控制两者占比<script type="math/tex; mode=display">t=s^\alpha\times u^\beta</script></li><li><strong>样本分配</strong>：对于每个 gt，我们选择 m 个具有最大 t 值的 anchor 作为正样本点，其余的为负样本</li></ul><h1 id="TOOD-的损失函数-TAL-？"><a href="#TOOD-的损失函数-TAL-？" class="headerlink" title="TOOD 的损失函数 (TAL)？"></a>TOOD 的损失函数 (TAL)？</h1><ul><li><strong>分类损失</strong>：为了显式的增加对齐的 anchor 的得分，减少不对齐的 anchor 的得分，用 t 来代替正样本 anchor 的标签。我们发现，当α和β变换导致正样本的标签变小之后，模型无法收敛，因此，因此使用了归一化的 t，这个归一化有两个性质：1）确保可以有效学习困难样本，2）保持原来的排序。最终借鉴 focal loss 的思想得到分类损失 $L_{cls}$ <script type="math/tex; mode=display">\begin{array}{c}L_{cls\_pos}=\sum\limits_{i=1}^{N_{pos}}BCE(s_i,\hat{t}_i) \\ L_{cls}=\sum_{i=1}^{N_{pa}}|\hat{t}_{i}-s_{i}|^{\gamma}BCE(s_{i},\hat{t}_{i})+\sum_{j=1}^{N_{nameg}}{s_{j}}^{\gamma}BCE(s_{j},0),\end{array}</script></li><li><strong>定位损失</strong>：使用归一化的 t 来对 GIoU loss 进行了加权<script type="math/tex; mode=display">L_{reg}=\sum_{i=1}^{N_{pos}}\hat{t}_{i}L_{GIoU}(b_{i},\bar{b}_{i})</script></li></ul><h1 id="TOOD-的-T-Head-和-TAL-的作用？"><a href="#TOOD-的-T-Head-和-TAL-的作用？" class="headerlink" title="TOOD 的 T-Head 和 TAL 的作用？"></a>TOOD 的 T-Head 和 TAL 的作用？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/TOOD-20230514144549.png" alt=""></li><li><strong>由于分类和定位的学习机制不同，两个任务学习到的特征的空间分布可能不同，当使用两个单独的分支进行预测时，会导致一定程度的错位</strong>，第一行可以看出分类、定位的最佳定位位置都是分开的，所以其 Score 和 IOU 都是偏低的</li><li>使用 T-Head 和 TAL 后，分类、定位的最佳定位位置非常接近，其 Score 和 IOU 都更高</li></ul><p>参考：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/405271228">[论文解读]51.1AP，TOOD: Task-aligned One-stage Object Detection - 知乎</a></li><li><a href="https://www.cnblogs.com/wxkang/p/15658375.html">ICCV2021 | TOOD：任务对齐的单阶段目标检测 - CV技术指南（公众号） - 博客园</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;通过设计新的预测头 T-Head 和样本对齐损失 (TAL)，实现分类、定位分支的对齐，使得两个分支的最佳锚框更加接近。这样可以减少“低分类概率+准确位置预测”、“高概率预测+不太准确预测”这两种情况目标的漏检 &lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="目标检测" scheme="https://shaogui.life/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>YOLOv7：Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</title>
    <link href="https://shaogui.life/2023/04/18/YOLOv7%EF%BC%9ATrainable%20bag-of-freebies%20sets%20new%20state-of-the-art%20for%20real-time%20object%20detectors/"/>
    <id>https://shaogui.life/2023/04/18/YOLOv7%EF%BC%9ATrainable%20bag-of-freebies%20sets%20new%20state-of-the-art%20for%20real-time%20object%20detectors/</id>
    <published>2023-04-18T03:49:49.000Z</published>
    <updated>2023-05-21T08:13:08.478Z</updated>
    
    <content type="html"><![CDATA[<p>YOLOv7通过扩展高效聚合网络(E-ELAN)、一致性的模型缩放策略、模型重参数化和动态标签分配，实现更高的精度</p><span id="more"></span><h1 id="什么是-YOLOv7"><a href="#什么是-YOLOv7" class="headerlink" title="什么是 YOLOv7?"></a>什么是 YOLOv7?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/YOLOv7-20230408142002.png" alt="YOLOv7-20230408142002"></li><li>YOLOv7 是 YOLO 系列新一代的实时目标检测算法，通过扩展高效聚合网络(E-ELAN)、一致性的模型缩放策略、模型重参数化和动态标签分配，实现更高的精度</li><li>YOLOv7仍然是一种anchor base的目标检测算法、采用多head检测、动态样本分配</li></ul><h1 id="YOLOv7的网络结构？"><a href="#YOLOv7的网络结构？" class="headerlink" title="YOLOv7的网络结构？"></a>YOLOv7的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/YOLOv7-20230408142002-1.png" alt=""></li><li>YOLOv7 由 2 个部分组成，backbone 和 head，其中 head 包含 neck 部分</li><li>输入640x640 的图片，得到 3 层不同 size 的特征，然后经过 RepVGG block 和 conv，最后输出目标的类别、置信度和边框进行</li><li>YOLOv7 采用 3 个检测 head，每个网格使用 3 种 anchor</li></ul><h1 id="YOLOv7网络结构的backbone部分？"><a href="#YOLOv7网络结构的backbone部分？" class="headerlink" title="YOLOv7网络结构的backbone部分？"></a>YOLOv7网络结构的backbone部分？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/YOLOv7-20230408142003.png" alt=""></li><li><strong>CBS：</strong> 主要是由 Conv+BN+SiLU，颜色表示卷积使用不同的 size、stride <img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/YOLOv7-20230408142004.png" alt=""></li><li><strong>ELAN：</strong> 由多个 CBS 组成，其输入输出特征大小保持不变，通道数在开始的两个 CBS 会有变化，后面的几个输入通道都是和输出通道保持一致的，经过最后一个 CBS 输出为需要的通道 <img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/YOLOv7-20230408142004-1.png" alt=""></li><li><strong>MP：</strong> 主要是分为 Maxpool 和 CBS , 其中 MP1 和 MP2 主要是通道数的比变化 <img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/YOLOv7-20230408142004-2.png" alt=""></li></ul><h1 id="YOLOv7网络结构的head部分？"><a href="#YOLOv7网络结构的head部分？" class="headerlink" title="YOLOv7网络结构的head部分？"></a>YOLOv7网络结构的head部分？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/YOLOv7-20230408142005.png" alt=""></li><li>head 其实就是一个 PAN 的结构，和之前的 YOLOv4，YOLOv5一样。首先对于 backbone 最后输出的 32 倍降采样特征图 C5，然后经过 SPPCSP ，通道数从 1024 变为 512。先按照 topdown 和 C4、C3 融合，得到 P3、P4 和 P5；再按 bottom-up 去和 P4、P5 做融合</li></ul><h1 id="YOLOv7的一致性的模型缩放策略？"><a href="#YOLOv7的一致性的模型缩放策略？" class="headerlink" title="YOLOv7的一致性的模型缩放策略？"></a>YOLOv7的一致性的模型缩放策略？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/YOLOv7-20230408142005-1.png" alt="YOLOv7-20230408142005-1"></li><li>模型缩放是一种放大或缩小己经设计好的模型，使其适合不同的计算设备的方法</li><li>模型缩放法通常使用不同的缩放因子，如<strong>分辨率</strong>（输入图像的大小）、<strong>深度</strong>（层数）、<strong>宽度</strong>（通道数）和<strong>阶段</strong>（特征金字塔的数量），从而在网络参数的数量、计算量、推理速度和精度方面达到良好的权衡</li><li>目前几乎所有的模型缩放方法都是独立分析单个缩放因子的，甚至复合缩放类别中的方法也是独立优化缩放因子的，当这些模型的深度被缩放时，将改变某些层的输入宽度。<strong>对于基于级联的模型，不能单独分析每个比例因子，必须一起考虑。以按比例放大深度为例，这样的操作将导致过渡层的输入通道和输出通道之间的比率变化，这可能会导致模型的硬件使用量减少</strong></li><li>当我们缩放计算块的深度因子时，还必须计算该块的输出通道的变化。然后，我们将对过渡层执行相同变化量的宽度因子缩放</li></ul><h1 id="YOLOv7的扩展高效聚合网络-E-ELAN"><a href="#YOLOv7的扩展高效聚合网络-E-ELAN" class="headerlink" title="YOLOv7的扩展高效聚合网络(E-ELAN)?"></a>YOLOv7的扩展高效聚合网络(E-ELAN)?</h1><ul><li><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/YOLOv7-20230408142006.png" alt="YOLOv7-20230408142006"></p></li><li><p>图 (b) 中 CSPVoVNet 的设计是 VoVNet 的一种变体。CSPVoVNet 的架构除了考虑上述基本设计问题外，还分析了梯度路径，以使不同层的权重能够学习到更多样化的特征。上述梯度分析方法使推理更快、更准确</p></li><li><p>图 (c) 中的 ELAN 考虑了以下设计策略——“如何设计一个高效的网络？”。他们得出了一个结论：通过控制最短最长的梯度路径，更深的网络可以有效地学习和收敛</p></li><li><p>E-ELAN 对基数 (Cardinality) 做了扩展 (Expand)、乱序 (Shuffle)、合并 (Merge cardinality)，能在不破坏原始梯度路径的情况下，提高网络的学习能力。使用组卷积来扩展计算块的通道和基数。将对计算层的所有计算块应用相同的组参数和通道乘数。然后，每个计算块计算出的特征图会根据设置的组参数 g 被打乱成 g 个组，然后将它们连接在一起。此时，每组特征图的通道数将与原始架构中的通道数相同。最后，添加 g 组特征图来执行合并基数</p></li></ul><h1 id="YOLOv7的模型重参数化方法？"><a href="#YOLOv7的模型重参数化方法？" class="headerlink" title="YOLOv7的模型重参数化方法？"></a>YOLOv7的模型重参数化方法？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/YOLOv7-20230408142006-1.png" alt="YOLOv7-20230408142006-1"></li><li>块级重参数化在训练期间将模块拆分为多个相同或不同的模块分支，在推理期间将多个分支模块集成为完全等效的模块</li><li>YOLOv7 发现 RepConv 中的 id 连接破坏了 ResNet 中的残差和 DenseNet中的连接，因此当具有残差或级联的卷积层被重参数化的卷积代替时，应该不使用 indentity 连接</li></ul><h1 id="YOLOv7的样本匹配方法？"><a href="#YOLOv7的样本匹配方法？" class="headerlink" title="YOLOv7的样本匹配方法？"></a>YOLOv7的样本匹配方法？</h1><ul><li>YOLOv7正负样本匹配前一部分与yolov5的一样后面加了simOTA来精确筛选</li></ul><h1 id="YOLOv7的损失函数？"><a href="#YOLOv7的损失函数？" class="headerlink" title="YOLOv7的损失函数？"></a>YOLOv7的损失函数？</h1><ul><li>YOLOv5一样的损失函数</li></ul><h1 id="什么是SPPCSPC？"><a href="#什么是SPPCSPC？" class="headerlink" title="什么是SPPCSPC？"></a>什么是SPPCSPC？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/YOLOv7-20230408142007.png" alt=""></li><li>YOLOv7中使用的SPP结构，在COCO数据集上表现优于SPPF  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SPPCSPC</span>(nn.Module):</span><br><span class="line"><span class="comment"># CSP https://github.com/WongKinYiu/CrossStagePartialNetworks</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c1, c2, n=<span class="number">1</span>, shortcut=<span class="literal">False</span>, g=<span class="number">1</span>, e=<span class="number">0.5</span>, k=(<span class="params"><span class="number">5</span>, <span class="number">9</span>, <span class="number">13</span></span>)</span>):</span><br><span class="line">    <span class="built_in">super</span>(SPPCSPC, self).__init__()</span><br><span class="line">    c_ = <span class="built_in">int</span>(<span class="number">2</span> * c2 * e)  <span class="comment"># hidden channels</span></span><br><span class="line">    self.cv1 = Conv(c1, c_, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    self.cv2 = Conv(c1, c_, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    self.cv3 = Conv(c_, c_, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">    self.cv4 = Conv(c_, c_, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    self.m = nn.ModuleListnn.MaxPool2d(kernel_size=x, stride=<span class="number">1</span>, padding=x // <span class="number">2</span>) <span class="keyword">for</span> x <span class="keyword">in</span> k])</span><br><span class="line">    self.cv5 = Conv(<span class="number">4</span> * c_, c_, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    self.cv6 = Conv(c_, c_, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">    self.cv7 = Conv(<span class="number">2</span> * c_, c2, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    x1 = self.cv4(self.cv3(self.cv1(x)))</span><br><span class="line">    y1 = self.cv6(self.cv5(torch.catx1] +m(x1) <span class="keyword">for</span> m <span class="keyword">in</span> self.m], <span class="number">1</span>)))</span><br><span class="line">    y2 = self.cv2(x)</span><br><span class="line">    <span class="keyword">return</span> self.cv7(torch.cat((y1, y2), dim=<span class="number">1</span>))</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;YOLOv7通过扩展高效聚合网络(E-ELAN)、一致性的模型缩放策略、模型重参数化和动态标签分配，实现更高的精度&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="目标检测" scheme="https://shaogui.life/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    <category term="anchor-base" scheme="https://shaogui.life/tags/anchor-base/"/>
    
    <category term="one-stage" scheme="https://shaogui.life/tags/one-stage/"/>
    
  </entry>
  
  <entry>
    <title>HSSN：Deep Hierarchical Semantic Segmentation</title>
    <link href="https://shaogui.life/2023/04/11/HSSN%EF%BC%9ADeep%20Hierarchical%20Semantic%20Segmentation/"/>
    <id>https://shaogui.life/2023/04/11/HSSN%EF%BC%9ADeep%20Hierarchical%20Semantic%20Segmentation/</id>
    <published>2023-04-11T14:54:52.000Z</published>
    <updated>2023-05-21T08:13:59.121Z</updated>
    
    <content type="html"><![CDATA[<p>HSSN抛弃传统的每个像素进行扁平分类的思想，在借鉴层次聚类的想法后，在网络输出端增加”类别树”约束，使得网络学习到的特征更加鲁棒</p><span id="more"></span><h1 id="什么是-HSSN？"><a href="#什么是-HSSN？" class="headerlink" title="什么是 HSSN？"></a>什么是 HSSN？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/HSSN-20230408143142.png" alt="HSSN-20230408143142"></li><li>人类能够在观察中识别结构化关系，使我们能够将复杂的场景分解为更简单的部分，并在多个层次上抽象视觉世界。然而，人类感知的这种分层推理能力在当前的语义分割文献中仍然在很大程度上没有得到探索。现有工作通常知道扁平化标签，并专门针对每个像素预测目标类</li><li>在本文中，我们讨论了分层语义分割（HSS），它旨在根据类层次结构对视觉观察进行结构化的像素级描述。我们设计了 HSSN，这是一个通用的 HSS 框架，解决了这项任务中的两个关键问题：i）如何有效地使现有的与层次结构无关的分段网络适应 HSS 设置，以及 ii）如何利用层次结构信息来规范 HSS 网络学习</li><li>了解决第1个问题，HSSN直接将HSS视为一个像素级多标签分类问题，因此相比于现在的分割模型只引入了极小的改动，对于第2个问题，HSSN网络首先将探索语义层次作为训练目标，这将会迫使分割结果遵从语义结构，同时，通过施加类别间的边缘约束，HSSN将会对像素映射空间进行重新构造，最终产生更好的像素表示并提升模型的效果</li></ul><h1 id="HSSN-的网络结构？"><a href="#HSSN-的网络结构？" class="headerlink" title="HSSN 的网络结构？"></a>HSSN 的网络结构？</h1><ul><li><strong>Hierarchical Semantic Segmentation Networks</strong>：这一部分确保了类别的连贯性以及一致性（推测在取类别最大值的时候会做一些额外的处理，来确保所有取到的最大值都处于同一个类别树的分支上保证类别的连贯，但是作者在正文中没给出）</li><li><strong>Hierarchy-Aware Segmentation Learning</strong>：<ul><li><strong>pixel-wise hierarchical segmentation learning strategy</strong>：像素级层次分割学习策略，确保预测能够在层次关系上保持一致<ul><li>每个像素分配的标签都应该具有层次上的一致性，因此需要遵守以下的两个原则 <img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/HSSN-20230408143142-1.png" alt="HSSN-20230408143142-1"></li><li><strong>Tree-Min Loss</strong>：如果子节点是对的，就取概率最小的节点路径（保证子节点要比父节点概率小，这条路径还没到当前的节点，也保证了之前的路径最短），如果子节点是错的，就取概率最大的节点路径（因为是错的取最大，最大最小一样的概率很小，所以就能够防止在这条路径上走下去，另外这条路径已经包括了当前的节点，或者说这条路径的最后一个节点就是当前这个节点）。相比于直接使用BCE Loss，Tree-Min Loss得出的score能确保完全符合作者设定的两个限制条件，并能够加大对不符合条件预测的惩罚</li><li><strong>Focal Tree-Min Loss</strong>：受到focal Loss的启发，作者在其中增加了调节因子，以便能够对困难的例子更好的学习</li></ul></li><li><strong>pixel-wise hierarchical representation learning strategy</strong>：像素级层次表示学习策略，确保在表示空间中能够将不同类别的表示有效地重构，从而学习到更好的表示。类似于对比学习，作者会挑选标准图、正样本、负样本三张图作为一组，通过多组计算损失，里面的参数 m 会迫使正负样本的距离超过 m <img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/HSSN-20230408143143.png" alt="HSSN-20230408143143"></li></ul></li></ul><h1 id="HSSN-如何更新预测分数的？"><a href="#HSSN-如何更新预测分数的？" class="headerlink" title="HSSN 如何更新预测分数的？"></a>HSSN 如何更新预测分数的？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/HSSN-20230408143143-1.png" alt="HSSN-20230408143143-1"></li><li>上图是计算图像某一个像素点类别时画出的树状图，图（a）中将正确的分类路径用红色标注出来，错误的用蓝色标注（实心代表 positive class，空心代表 negative class），图（b）则显示了相对应的 BCE 损失，作者将两个不合理的点单独做了标注（在数字上添加了方框），可以看到在使用 BCE Loss 时，第二层节点的损失是差不多的，图（c）则采用了作者的 $L^{tm}$ 损失（后面会提到，这个损失可以保证在计算时让每个点都符合作者给出的两个限制条件），能够成功地在不同类别上显示出区分性，保证模型训练的层次结构较为合理</li><li>假设模型直接输出是 s，更新后是 p，其中 $A_v$ 与 $C_v$ 代表 $v$ 的父集和子类集，第二行也可以更新为 $p_v=max(s_u)$ ，按照以下公式更新得到图 c 的新分数 <script type="math/tex; mode=display">\begin{cases}\quad p_v=\min\limits_{u\in\mathcal{A}_v}(s_u)&\text{if}\hat{l}_v=1,\\ 1-p_v=\min\limits_{u\in\mathcal{C}_v}(1-s_u)=1-\max\limits_{u\in\mathcal{C}_v}(s_u)&\text{if}\hat{l}_v=0,\end{cases}</script></li><li><strong>总结</strong>：在父节点 $u$ 往下辨别子节点 $v$ 时，如果子节点是对的，就取概率最小的节点路径（保证子节点要比父节点概率小，这条路径还没到当前的节点，也保证了之前的路径最短），如果子节点是错的，就取概率最大的节点路径（因为是错的取最大，最大最小一样的概率很小，所以就能够防止在这条路径上走下去，另外这条路径已经包括了当前的节点，或者说这条路径的最后一个节点就是当前这个节点）</li></ul><h1 id="HSSN-如何强制预测符合类别层次关系的？"><a href="#HSSN-如何强制预测符合类别层次关系的？" class="headerlink" title="HSSN 如何强制预测符合类别层次关系的？"></a>HSSN 如何强制预测符合类别层次关系的？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/HSSN-20230408143143.png" alt="HSSN-20230408143143"></li><li>挑选标准图、正样本、负样本三张图作为一组，通过多组计算损失，里面的参数 m 会迫使正负样本的距离超过 m，从而使得损失为0 <script type="math/tex; mode=display">\mathcal{L}^{\mathrm{TT}}(\boldsymbol{i},\boldsymbol{i}^+,\boldsymbol{i}^-)=\max\{\langle\boldsymbol{i},\boldsymbol{i}^+\rangle-\langle\boldsymbol{i},\boldsymbol{i}^-\rangle+m,0\},</script></li></ul><h1 id="HSSN-的-TML-损失函数？"><a href="#HSSN-的-TML-损失函数？" class="headerlink" title="HSSN 的 TML 损失函数？"></a>HSSN 的 TML 损失函数？</h1><ul><li><strong>Tree-Min Loss</strong>：如果子节点是对的，就取概率最小的节点路径（保证子节点要比父节点概率小，这条路径还没到当前的节点，也保证了之前的路径最短），如果子节点是错的，就取概率最大的节点路径（因为是错的取最大，最大最小一样的概率很小，所以就能够防止在这条路径上走下去，另外这条路径已经包括了当前的节点，或者说这条路径的最后一个节点就是当前这个节点）。相比于直接使用 BCE Loss，Tree-Min Loss 得出的 score 能确保完全符合作者设定的两个限制条件，并能够加大对不符合条件预测的惩罚<script type="math/tex; mode=display">\begin{aligned}\mathcal{L}^{\mathtt{TM}}(p)&=\sum\limits_{v\in\mathcal{V}}-\hat{l}_v\log(p_v)-(1-\hat{l}_v)\log(1-p_v),\\ &=\sum\limits_{v\in\mathcal{V}}-\hat{l}_v\log(\min_{u\in\mathcal{A}_v}(s_u))-\\ &(1-\hat{l}_v)\log(1-\max_{u\in\mathcal{C}_v}(s_u)).\end{aligned}</script></li><li><strong>Focal Tree-Min Loss</strong>：受到 focal Loss 的启发，作者在其中增加了调节因子，以便能够对困难的例子更好的学习<script type="math/tex; mode=display">\begin{array}{l}\mathcal{L}^{\mathrm{TM}}(p)=\sum\limits_{v\in\mathcal{P}}-\hat{l}_v(1-p_v)^\gamma\log(p_v)-(1-\hat{l}_v)(p_v)^\gamma\log(1-p_v),\\ =\sum\limits_{v\in\mathcal{V}}-\hat{l}_v(1-\operatorname*{min}_{u\in\mathcal{A}_v}(s_u))^\gamma\log(\operatorname*{min}_{u\in A_v}(s_u))-(1-\hat{l}_v)(\max_{u\in\mathcal{C}_v}(s_u))^\gamma\log(1-\max_{u\in\mathcal{C}_v}(s_u)),\end{array}</script></li></ul><h1 id="HSSN-的-TTL-损失函数？"><a href="#HSSN-的-TTL-损失函数？" class="headerlink" title="HSSN 的 TTL 损失函数？"></a>HSSN 的 TTL 损失函数？</h1><ul><li>挑选标准图、正样本、负样本三张图作为一组，通过多组计算损失，里面的参数 m 会迫使正负样本的距离超过 m，从而使得损失为0 <script type="math/tex; mode=display">\mathcal{L}^{\mathrm{TT}}(\boldsymbol{i},\boldsymbol{i}^+,\boldsymbol{i}^-)=\max\{\langle\boldsymbol{i},\boldsymbol{i}^+\rangle-\langle\boldsymbol{i},\boldsymbol{i}^-\rangle+m,0\},</script></li></ul><p>参考：<br><a href="https://zhuanlan.zhihu.com/p/551064469">Papers - Deep Hierarchical Semantic Segmentation - 知乎</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;HSSN抛弃传统的每个像素进行扁平分类的思想，在借鉴层次聚类的想法后，在网络输出端增加”类别树”约束，使得网络学习到的特征更加鲁棒&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="语义分割" scheme="https://shaogui.life/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>EAST：An Efficient and Accurate Scene Text Detector</title>
    <link href="https://shaogui.life/2023/03/25/EAST%EF%BC%9AAn%20Efficient%20and%20Accurate%20Scene%20Text%20Detector/"/>
    <id>https://shaogui.life/2023/03/25/EAST%EF%BC%9AAn%20Efficient%20and%20Accurate%20Scene%20Text%20Detector/</id>
    <published>2023-03-25T14:35:22.000Z</published>
    <updated>2023-05-25T14:41:15.046Z</updated>
    
    <content type="html"><![CDATA[<p>EAST基于FCN输出，对每个grid进行文本行预测，可实现旋转矩形框、任意四边形框的预测</p><span id="more"></span><h1 id="什么是-EAST-？"><a href="#什么是-EAST-？" class="headerlink" title="什么是 EAST ？"></a>什么是 EAST ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/EAST-20230408144126.png" alt="EAST-20230408144126"></li><li>（a）、（b）、（c）、（d）都是几种常见 stat-of-the-art 的文本检测过程，算法思想遵循之前 two-stage 的方法，一般都需要先提出候选框，过滤后对剩下的候选框要进行回归操作得出更精细的边框信息，然后再合并候选框等</li><li><strong>EAST</strong>基于 FCN 输出特征，类似 anchor-free 的目标检测模型，预测每个 grid 的代表的文本行信息，然后使用 NMS（非极大值抑制）合并预测后的信息，可实现矩形、选择矩阵和四边形的文本检测，不能实现弯曲文本的检测</li></ul><h1 id="EAST-的网络结构？"><a href="#EAST-的网络结构？" class="headerlink" title="EAST 的网络结构？"></a>EAST 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/EAST-20230408144126-1.png" alt="EAST-20230408144126-1"></li><li><strong>FCN 特征提取</strong>：通过<strong>特征提取</strong>和<strong>特征融合</strong>两个步骤，最后取 C 2 特征输入预测头</li><li><strong>预测结果的输出层</strong>：假设 C 2 特征大小为 CHW，对于 HW 的每个 grid 输出 2 个分支，第一个分支是置信度 (1)，第二个分支是框位置，框位置如果是旋转矩形，则输出 4 (xyxy)+1（angle），如果是任意的四边形, 则输出 8 (xy * 4)</li></ul><h1 id="EAST-的标签分配？"><a href="#EAST-的标签分配？" class="headerlink" title="EAST 的标签分配？"></a>EAST 的标签分配？</h1><ul><li>检测 head 没有设置 anchor，直接按映射位置确定正样本，文本行比较大，可以按照多正样本匹配</li></ul><h1 id="EAST-的标签生成？"><a href="#EAST-的标签生成？" class="headerlink" title="EAST 的标签生成？"></a>EAST 的标签生成？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/EAST-20230408144127.png" alt="EAST-20230408144127"></li><li><strong>垂直或水平矩阵框 (AABB)</strong>：只需要 4 个值就可描述；<strong>旋转矩形框 (RBOX)</strong> ： AABB 的基础上增加角度，共 5 个值描述；<strong>任意四边形（QUAD）</strong>：需要 4 个点 8 个值去描述</li></ul><h1 id="EAST-的损失函数？"><a href="#EAST-的损失函数？" class="headerlink" title="EAST 的损失函数？"></a>EAST 的损失函数？</h1><ul><li><script type="math/tex; mode=display">L=L_\mathrm{s}+\lambda_\mathfrak{g}L_\mathfrak{g}</script></li><li><strong>分割损失 $L_s$</strong>：使用 blance 的交叉熵</li><li><strong>位置损失 $L_g$</strong>：直接使用 L 1 或者 L 2 损失去回归文本区域将导致损失偏差朝更大更长, 所以使用 IOU loss 监督 AABB 或 RBOX 类型框的位置；对于 QUAD 类型的回归框，使用尺度归一化的 smooth L 1 损失</li></ul><h1 id="EAST-如何解析模型输出"><a href="#EAST-如何解析模型输出" class="headerlink" title="EAST 如何解析模型输出"></a>EAST 如何解析模型输出</h1><ul><li>模型输出包括 2 部分，1）score map：检测框的置信度，1 个参数；2）text boxes：对于检测形状为 RBOX，检测框的位置（x, y, w, h）+旋转角度 (angle)，5 个参数；对于检测形状为 QUAD，则输出任意四边形检测框的位置坐标，(x 1, y 1), (x 2, y 2), (x 3, y 3), (x 4, y 4)，8 个参数</li><li>取 topK 的 score map 对应的预测框，然后采用 Locality-Aware NMS 过滤这些预测框，得到最终结果</li></ul><p>参考：</p><ol><li><a href="https://cloud.tencent.com/developer/article/1542875">05. OCR学习路径之文本检测（下）EAST算法简介 - 腾讯云开发者社区-腾讯云</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;EAST基于FCN输出，对每个grid进行文本行预测，可实现旋转矩形框、任意四边形框的预测&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文本检测" scheme="https://shaogui.life/tags/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>SAST：A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning</title>
    <link href="https://shaogui.life/2023/03/16/SAST%EF%BC%9AA%20Single-Shot%20Arbitrarily-Shaped%20Text%20Detector%20based%20on%20Context%20Attended%20Multi-Task%20Learning/"/>
    <id>https://shaogui.life/2023/03/16/SAST%EF%BC%9AA%20Single-Shot%20Arbitrarily-Shaped%20Text%20Detector%20based%20on%20Context%20Attended%20Multi-Task%20Learning/</id>
    <published>2023-03-16T15:20:53.000Z</published>
    <updated>2023-05-21T08:12:54.131Z</updated>
    
    <content type="html"><![CDATA[<p>属于EAST的演进版本，还是类似 anchor-free 的方式预测文本行，除了输出 grid 的 score +边框预测外，还输出更多的文本实例信息，比如 grid 到实例矩形四角、中心点、四边的距离，使得SAST可以检测弯曲文本行、中间有间隔的文本行</p><span id="more"></span><h1 id="什么是-SAST-？"><a href="#什么是-SAST-？" class="headerlink" title="什么是 SAST ？"></a>什么是 SAST ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204412.png" alt=""></li><li>属于 <a href="EAST.md">EAST</a> 的演进版本，还是类似 anchor-free 的方式预测文本行，但是除了输出 grid 的 score +边框预测外，还输出更多的文本实例信息，比如 grid 到实例矩形四角、中心点、四边的距离</li><li>每个 grid 更加复杂的输出，可以让 SAST 检测更为复杂场景下的文本行，比如弯曲文本行、中间有间隔的文本行</li></ul><h1 id="SAST-的网络结构？"><a href="#SAST-的网络结构？" class="headerlink" title="SAST 的网络结构？"></a>SAST 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204443.png" alt=""></li><li><strong>Featrue Extractor</strong>：BackBone 部分，通过类似 SegNet 的过程提取特征</li><li><strong>CABs</strong>：交叉注意力模块，用于整合 BackBone 的特征</li><li><strong>TCL map (1 xHxW)</strong>: grid 属于文本中心线像素点的概率</li><li><strong>TCO map (2 xHxW)</strong>: 文本中心点偏置，grid 距其所属的文本实例矩形框中心的 xy 方向距离</li><li><strong>TVO map (8 xHxW)</strong>: 文本四顶点偏置，grid 距其所属的文本实例矩形框四顶点的 xy 方向距离</li><li><strong>TBO map (4 xHxW)</strong>: 文本边界偏置，grid 距其所属的文本实例上下边界框的 xy 方向距离</li></ul><h1 id="SAST-的-CAB-模块？"><a href="#SAST-的-CAB-模块？" class="headerlink" title="SAST 的 CAB 模块？"></a>SAST 的 CAB 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204535.png" alt=""></li><li>交叉注意力模块，用于整合 BackBone 的特征，该模块分为上下两部分，上部分构建水平方向注意力，下部分构建垂直方向注意力，整合水平方向注意力和垂直方向注意力得到<strong>全局注意力</strong></li></ul><h1 id="SAST-样本制作？"><a href="#SAST-样本制作？" class="headerlink" title="SAST 样本制作？"></a>SAST 样本制作？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204602.png" alt=""> </li><li><strong>a). TCL map (1 xHxW)</strong>：文本中心线区域，文本行上下边界收缩 20%后得到的区域，而左右边界仍保持不变</li><li><strong>b). TBO map (4 xHxW)</strong>：文本边界偏置，首先计算斜率 k 1 (v 1, v 2)与斜率 k 1 (v 4, v 3)的平均值，对于一个给定的点 P 0，可容易地计算出斜率为 (k 1+k 2)/2、过点 P 0 的直线，由此该直线与线段 (v 1, v 4)和线段 (v 2, v 3)的交点 P 1 与 P 2 很容易得出，故 P 0 的上下边界点 $P<em>{upper}$ 和 $P</em>{lower}$ 的坐标可由线段比例关系得到，整理得到 P 0 点到四边距离的 TBO 为{$P<em>0^x-P_1^x$、 $P_0^x-P</em>{lower}^x$ 、$P<em>2^y-P_0^y$、$P</em>{upper}^y-P_0^y$}</li><li><strong>c). TVO map (8 xHxW)</strong>：文本顶点偏置，文本最小矩形框按根据一定规则由文本标注信息计算得到，计算文本中心区域中某像素点到文本矩形框四顶点的直线距离（包括 x 方向和 y 方向），所以共计给每个 grid 生成 8 个 TVO 预测</li><li><strong>d). TCO map (2 xHxW)</strong>：文本中心点偏置，计算文本中心区域内某像素点到文本最小矩形框中心点的距离 (x 方向和 y 方向)</li></ul><h1 id="SAST-的损失函数？"><a href="#SAST-的损失函数？" class="headerlink" title="SAST 的损失函数？"></a>SAST 的损失函数？</h1><ul><li><script type="math/tex; mode=display">L_{total}=\lambda_1L_{tcl}+\lambda_2L_{tco}+\lambda_3L_{tvo}+\lambda_4L_{tbo},</script></li><li><strong>(1) TCL map:</strong> 使用 Minimizing the Dice loss 作为分割 loss, 用于描述两个轮廓的相似程度</li><li><strong>(2) TVO/TCO/TBO:</strong> 使用 Smooth L 1 Loss 作为几何图 geometry map 的回归 loss</li></ul><h1 id="解析-SAST-的输出-1-生成文本实例？"><a href="#解析-SAST-的输出-1-生成文本实例？" class="headerlink" title="解析 SAST 的输出 1-生成文本实例？"></a>解析 SAST 的输出 1-生成文本实例？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204913.png" alt=""></li><li><strong>a)</strong> <strong>根据 TCL 获得文本实例包含的像素点-&gt;文本行 Mask</strong>，阈值过滤将置信率低于某值的假阳性像素点剔除，得到合适的 TCL map;</li><li><strong>b) 根据 TVO+NMS 获得文本实例-&gt;文本矩形框</strong>：将经过处理的 TCL map 中每个像素点，根据 TVO 文本实例顶点偏置图，得到对应的文本矩形框四顶点坐标，并进行非最大值抑制 NMS，得到所需的文本实例矩形框及其中心点</li><li><strong>b+c=d) 根据 TCO 合并文本实例-&gt;文本行 Mask</strong> ：计算 TCL 中属于文本的像素点的所属文本实例的几何中心点，该中心点将作为低层级像素信息，当步骤 c 计算所得的几何中心点与步骤 b 所得矩形框中心点重合或相近时，该像素点将被归类给步骤 b 中矩形框对应的文本实例，通过此步骤重新合并断开的文本行</li></ul><h1 id="解析-SAST-的输出-2-生成文本边框？"><a href="#解析-SAST-的输出-2-生成文本边框？" class="headerlink" title="解析 SAST 的输出 2-生成文本边框？"></a>解析 SAST 的输出 2-生成文本边框？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204412.png" alt=""></li><li><strong>a)</strong> 前面解析得到的文本实例</li><li><strong>b)</strong> 对文本中心线采样，采样点的间距相同，则得到的采样点数目与文本线的长度有关，故称之为自适应采样</li><li><strong>c)</strong> 根据文本边界偏置图 TBO 所提供的信息，计算文本中心线的采样点上的上下边界定位点</li><li><strong>d)</strong> 将步骤 b 所得的边界定位点按照从左上角开始的顺时针方向依次进行连接，得到最终的文本边界框</li></ul><p>参考：</p><ol><li><a href="https://blog.csdn.net/qq_37546096/article/details/102909850">SAST : Single-Shot Arbitrarily-Shaped Text Detector论文阅读笔记_text center line sampling_litchi9854的博客-CSDN博客</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;属于EAST的演进版本，还是类似 anchor-free 的方式预测文本行，除了输出 grid 的 score +边框预测外，还输出更多的文本实例信息，比如 grid 到实例矩形四角、中心点、四边的距离，使得SAST可以检测弯曲文本行、中间有间隔的文本行&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文本检测" scheme="https://shaogui.life/tags/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>FOTS：Fast Oriented Text Spotting with a Unified Network</title>
    <link href="https://shaogui.life/2023/03/13/FOTS%EF%BC%9AFast%20Oriented%20Text%20Spotting%20with%20a%20Unified%20Network/"/>
    <id>https://shaogui.life/2023/03/13/FOTS%EF%BC%9AFast%20Oriented%20Text%20Spotting%20with%20a%20Unified%20Network/</id>
    <published>2023-03-13T13:42:24.000Z</published>
    <updated>2023-05-21T08:12:23.546Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304092146678.png" alt=""></p><p>第一个<strong>端到端</strong>解决文本识别的模型，相比较两阶段的文本定位方法，它的检测速度更快，基本思路是通过文本检测分支实现文本行区域的提取，然后通过 RoIRotate 模块实现文本行的“摆正”，最后使用 CRNN+CTC 的模式实现文本行的字符识别</p><span id="more"></span><h1 id="什么是-FOTS-？"><a href="#什么是-FOTS-？" class="headerlink" title="什么是 FOTS ？"></a>什么是 FOTS ？</h1><ul><li>第一个<strong>端到端</strong>解决文本识别的模型，相比较两阶段的文本定位方法，它的检测速度更快，基本思路是通过文本检测分支实现文本行区域的提取，然后通过 RoIRotate 模块实现文本行的“摆正”，最后使用 CRNN+CTC 的模式实现文本行的字符识别</li><li>RoIRotate 模块要通过仿射变换转换文本区域，所以 FOTS 只能识别文字中心在一个线上的文本行，无法处理弯曲文本行</li></ul><h1 id="FOTS-的网络结构？"><a href="#FOTS-的网络结构？" class="headerlink" title="FOTS 的网络结构？"></a>FOTS 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304092146065.png" alt=""></li><li><strong>shared convolutions</strong>：使用 Resnet 搭建，首先使用下采样，然后使用反卷积上采样，并且使用类似 SegNet 的高分辨率连接到低分辨率的连接</li><li><strong>文本检测分支</strong>：使用类似 anchor-free 的目标检测方式，将 shared convolutions 的每个 grid 视为文本行中心，然后预测其文本行的宽高</li><li><strong>RoIRotate</strong>：根据文本检测分支的输出+shared convolutions 输出，将文本行转为横向文本</li><li><strong>文字识别分支</strong>：基于 CRNN+CTC 的方式学习和识别文本行</li></ul><h1 id="FOTS-的-shared-convolutions-模块？"><a href="#FOTS-的-shared-convolutions-模块？" class="headerlink" title="FOTS 的 shared convolutions 模块？"></a>FOTS 的 shared convolutions 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/FOTS-20230409194643.png" alt=""></li><li>首先通过 ResNet 提取特征，然后通过反卷积上采样，类似 SegNet 一样中间使用残差连接，最后输出 C 2 特征</li></ul><h1 id="FOTS-的文本检测分支？"><a href="#FOTS-的文本检测分支？" class="headerlink" title="FOTS 的文本检测分支？"></a>FOTS 的文本检测分支？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-09%2020.21.26.excalidraw.png" alt="Drawing 2023-04-09 20.21.26.excalidraw"></li><li>使用类似 anchor-free 的目标检测方式，将 shared convolutions 的每个 grid 视为文本行中心，然后预测其文本行的宽高</li><li>假设 shared convolutions 输出是 $C\times H \times W$ 的特征，文本检测分支输出 3 个分支，分别表示文本行的得分、该 grid 到四边的距离和该文本行的旋转角度</li></ul><h1 id="FOTS-的-RoIRotate-模块？"><a href="#FOTS-的-RoIRotate-模块？" class="headerlink" title="FOTS 的 RoIRotate 模块？"></a>FOTS 的 RoIRotate 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/FOTS-20230409194714.png" alt=""></li><li>使用目标检测的后处理获得文本行，根据文本行的宽高及旋转角得到四个角点的位置，假设四个点是 ($x_1$, $y_1$)、($x_2$, $y_2$)、($x_3$, $y_3$)、($x_4$, $y_4$)，现在要将这个区域转到 (0,0)起点，宽高 (wh)的区域，可以通过仿射变换实现</li><li>仿射变换矩阵需要变换前后的 3 对点求得，不妨取 ($x_1$, $y_1$)-&gt;(0,0)、($x_2$, $y_2$)-&gt;（w, 0）、($x_3$, $y_3$)-&gt;(w, h)，求取方法是调用 opencv 的 getAffineTransform 函数即可，仿射矩阵变换后，文本的中心线平行 x 轴</li></ul><h1 id="FOTS-的文字识别分支？"><a href="#FOTS-的文字识别分支？" class="headerlink" title="FOTS 的文字识别分支？"></a>FOTS 的文字识别分支？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/CRNN-20230408144101.png" alt=""></li><li>文字识别是在 RoIRotate 模块输出的基础上进行的，就是得到平行文本行的基础上进行的，其过程有 4 个</li><li><strong>CNN 提取特征</strong>：使用轻量化网络 MobileNetv 3，其中输入图像的高度统一设置为 32，宽度可以为任意长度，经过 CNN 网络后，特征图的高度缩放为 1</li><li><strong>双向 LSTM（BiLSTM）对特征序列进行预测</strong>：学习序列中的每个特征向量并输出预测标签分布。这里其实相当于把特征向量的宽度视为 LSTM 中的时间维度</li><li><strong>全连接层分类</strong>：使用全连接层对每个序列进行 N+1 类别预测，获取模型的预测结果</li><li><strong>CTC</strong>：解码模型输出的预测结果，得到最终输出</li></ul><h1 id="FOTS-的损失函数？"><a href="#FOTS-的损失函数？" class="headerlink" title="FOTS 的损失函数？"></a>FOTS 的损失函数？</h1><ul><li>网络的损失分为两部分，即文本行识别损失 $L<em>{detect}$ 、文本行字符识别损失 $L</em>{recog}$，通过参数 $\lambda_{recog}$ 控制两者的权重<script type="math/tex; mode=display">L=L_{\mathbf{detect}}+\lambda_{\mathbf{recog}}L_{\mathbf{recog}}</script></li></ul><p>参考：</p><ol><li><a href="https://blog.csdn.net/sol_data12/article/details/113501530">场边文字检测——FOTS模型详解及其代码实现_ManManMan池的博客-CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/195248125">[论文笔记] FOTS - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304092146678.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;第一个&lt;strong&gt;端到端&lt;/strong&gt;解决文本识别的模型，相比较两阶段的文本定位方法，它的检测速度更快，基本思路是通过文本检测分支实现文本行区域的提取，然后通过 RoIRotate 模块实现文本行的“摆正”，最后使用 CRNN+CTC 的模式实现文本行的字符识别&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文本定位" scheme="https://shaogui.life/tags/%E6%96%87%E6%9C%AC%E5%AE%9A%E4%BD%8D/"/>
    
  </entry>
  
  <entry>
    <title>文本检测之DB和DB++</title>
    <link href="https://shaogui.life/2023/03/12/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E4%B9%8BDB%E5%92%8CDB++/"/>
    <id>https://shaogui.life/2023/03/12/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E4%B9%8BDB%E5%92%8CDB++/</id>
    <published>2023-03-12T07:03:50.000Z</published>
    <updated>2023-05-25T14:42:29.194Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091513112.png" alt=""></p><p>本文介绍两个文本检测模型： DB 及它的升级版本 DB++，主要的原理就是将“二值化”的过程做成网络可学习、可微分的一个模块，然后向网络中插入这个模块自适应去学习二值化阈值，最终预测文字区域的一个核心部分，然后再通过公式放大这个核心部分，得到目标区域</p><span id="more"></span><h1 id="什么是-DB-？"><a href="#什么是-DB-？" class="headerlink" title="什么是 DB ？"></a>什么是 DB ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091513112.png" alt=""></li><li><strong>传统意义二值化</strong>：基于分割的文本检测算法其流程如图2中的蓝色箭头所示。在传统方法中得到分割结果之后采用一个<strong>固定阈值</strong>得到二值化的分割图</li><li><strong>DB 二值化</strong>：如图2中红色箭头所示的，通过网络去预测图片每个位置处的阈值，而不是采用一个固定的值，这样就可以很好将背景与前景分离出来，但是这样的操作会给训练带来梯度不可微的情况，对此对于二值化提出了一个叫做 <strong>Differentiable Binarization 模块</strong>来解决</li></ul><h1 id="DB-的网络结构？"><a href="#DB-的网络结构？" class="headerlink" title="DB 的网络结构？"></a>DB 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091633772.png" alt=""></li><li><strong>BackBone</strong>：可以使用类似 resnet 下采样 5 次，得到 5 个层次的特征</li><li><strong>FPN 类似结构</strong>：对 C4、C3、C2 特征采样类似 FPN 的连接，输出时是 C5、 F4、F3、F2 一共 4 个层次的特征</li><li><strong>DB 模块</strong>：以 probability map 减去 threshold map (T) 差值，输入到 DB 模块进行可微的二值化学习</li></ul><h1 id="DB-的“可微的二值化模块-DB-”？"><a href="#DB-的“可微的二值化模块-DB-”？" class="headerlink" title="DB 的“可微的二值化模块 (DB)”？"></a>DB 的“可微的二值化模块 (DB)”？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091634515.png" alt=""></li><li>上图 a、b、c 分别是标准二值化与可微二值化输出、可微二值化对正样本的梯度，可微二值化对负样本的梯度，k 是放大倍数</li><li><strong>标准二值化 (SB)</strong>：通过预先设置的阈值 t 去对概率图 $P_{i,j}$ 二值化<script type="math/tex; mode=display">B_{i,j}=\begin{cases}1\quad P_{i,j}>=t\\ 0\quad otherwise\end{cases}</script></li><li><strong>可微二值化 (DB)</strong>：借鉴 sigmoid 输出输出，将 $P<em>{i,j}-T{i,j}$ 作为 sigmoid 输入，并 K 扩大输出，使得 $\hat{B}</em>{i,j}$ 趋向 0 或 1，即通过学习每个位置的阈值 $T<em>{i, j}$ 对概率图 $P</em>{i, j}$ 二值化 <script type="math/tex; mode=display">\hat{B}_{i, j}=\dfrac{1}{1+\exp^{-k (P_{i, j}-T_{i, j})}}</script></li><li><strong>正负样本梯度比较</strong>：DB 改进性能的原因可以通过梯度的反向传播来解释，可知正负样本的梯度被 k 放大 <script type="math/tex; mode=display">\begin{aligned}l_{+}=-log\frac{1}{1+e^{-}k x}\quad =>  \frac{\partial l_{+}}{\partial x}=-k f(x)e^{-k x}\\\\ l_{-}=-log(1-\frac{1}{1+e^{-}k x})\quad\quad =>  \frac{\partial l_{-}}{\partial x}=k f(x)\end{aligned}</script></li></ul><h1 id="DB-的自适应阈值？"><a href="#DB-的自适应阈值？" class="headerlink" title="DB 的自适应阈值？"></a>DB 的自适应阈值？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091635325.png" alt=""></li><li>a、b、c、d 分别是原图、probability map、无监督的 threshold map、有监督的 threshold map</li><li>c 图表明即使没有对 threshold map 监督，其结果也会表现出突出显示文本边界区域。这表明如果加入类似边界的监督，以提供更好的指导，d 图的结果证明了这一点</li></ul><h1 id="DB-的标签生成过程？"><a href="#DB-的标签生成过程？" class="headerlink" title="DB 的标签生成过程？"></a>DB 的标签生成过程？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091635472.png" alt=""></li><li><strong>probability map</strong>：使用 Vatti clipping algorithm 将 G 缩减到 Gs（蓝线内部），A 是面积，r 是 shrink ratio，设置为0.4，L 是周长 <script type="math/tex; mode=display">D=\dfrac{A(1-r^2)}{L}</script></li><li><strong>threshold map</strong>：使用生成 probability map 一样的方法，向外进行扩张，得到绿线和蓝线中间的区域，根据到红线的距离制作标签</li><li><strong>binary map</strong>：蓝色标注线以内</li></ul><p>总结：以上 3 个标签的值范围</p><div class="table-container"><table><thead><tr><th>-</th><th>蓝线以内</th><th>蓝蓝绿之间</th><th>其他</th></tr></thead><tbody><tr><td>probability map</td><td>1</td><td>0</td><td>0</td></tr><tr><td>threshold map</td><td>0.3</td><td>越靠近红线 0.7，越远离红线 0.</td><td>0.3</td></tr><tr><td>binary map</td><td>1</td><td>0</td><td>0</td></tr></tbody></table></div><p><strong><mark style="background: #FF5582A6;">从上面标签制作可知，DB 没有直接去学习文本的边缘（图红线），而是去学习比文本边缘更小的区域 (图绿线)，我觉得这点是除了”可微二值化模块”外，尤其需要关注的地方。这里说一下自己的理解</mark></strong></p><h1 id="DB-为什么不直接学习文本外轮廓，而是学习轮廓缩小的轮廓？"><a href="#DB-为什么不直接学习文本外轮廓，而是学习轮廓缩小的轮廓？" class="headerlink" title="DB 为什么不直接学习文本外轮廓，而是学习轮廓缩小的轮廓？"></a>DB 为什么不直接学习文本外轮廓，而是学习轮廓缩小的轮廓？</h1><ul><li><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091649041.png" alt="Drawing 2023-04-09 15.43.02.excalidraw"></p></li><li><p>上图是两种学习路线下的 probability map、threshold map 及他们学习的 binary map，其中红线是直接学习文本边缘（下文称直觉模式），绿线学习文本边缘小一圈的轮廓（下文称 DB 模式）</p></li><li><strong>观察 probability map</strong>，“直觉模式”比 DB 模式范围更大，这对极度弯曲的小文本是不友好的，可以想象文本在 C2特征已经辨别不出弯曲，更小的学习区域可以有更强的能力</li><li><strong>观察 threshold map</strong>，因为文本行占据了图片大部分区域，所以“直觉模式”主要优化背景到 0.7， threshold map 计算 L1 损失，相比较 DB 模式大部分优化背景到 0.3，“直觉模式”更难优化</li><li><strong>观察 binary map</strong>：除了和优化 threshold map 同样的问题外，由于“直觉模式”对文本行内、外的梯度大小一样，说明两个区域优化权重一样。而 DB 模式内部梯度比外部梯度更大，相当于增大正样本的梯度权重</li><li><strong>总结</strong>：“直觉模式”比 DB 模式更难优化，而且 DB 模式对弯曲小文本性能更好</li></ul><h1 id="DB-的损失函数？"><a href="#DB-的损失函数？" class="headerlink" title="DB 的损失函数？"></a>DB 的损失函数？</h1><ul><li>$L_s$ 是 probability map 的 loss，$L_b$ 是 binary map 的 loss，$L_t$ 是 threshold map 的 loss，$\alpha$ 和 $\beta$ 设置为1和10，$L_s$ 和 $L_b$ 使用交差熵计算损失<script type="math/tex; mode=display">L=L_s+\alpha\times L_b+\beta\times L_t</script></li><li>$S_l$ 表示使用 OHEM 进行采样，正负样本的比例为1：3, $L_t$ 使用 L 1 loss，$R_d$ 表示绿线内的区域，<script type="math/tex; mode=display">L_t=\sum_{i\in R_d}|y_i^*-x_i^*|</script></li></ul><h1 id="DB-如何解析输出的？"><a href="#DB-如何解析输出的？" class="headerlink" title="DB 如何解析输出的？"></a>DB 如何解析输出的？</h1><ul><li>在推理阶段，可以使用 binary map 或者 probability map</li><li><strong>使用 binary map</strong>：需要 probability map+threshold map 两个分支计算得到，其结果就是文本实例</li><li><strong>使用 probability map</strong>：不需要 threshold map、binary map 分支，直接按照 Vatti clipping algorithm 公式还原回去即可，即1)使用0.3的阈值进行二值化；2)将 pixel 连接成不同的文本实例；3)将文本实例进行扩张，得到最终的文本框<script type="math/tex; mode=display"> D^{'}=\dfrac{A^{'}(1-r^{'})}{L^{'}}</script></li><li>使用第二种方法，网络计算更少，论文使用第二种方法</li></ul><h1 id="DB-的网络结构？-1"><a href="#DB-的网络结构？-1" class="headerlink" title="DB++ 的网络结构？"></a>DB++ 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091649944.png" alt=""></li><li><strong>BackBone</strong>：可以使用类似 resnet 下采样 5 次，得到 5 个层次的特征</li><li><strong>ASF 模块</strong>：ASF 特征融合模块其实就是 FPN，只不过在此基础上增加Spatial Attention</li><li><strong>DB 模块</strong>：和 DB 一样，以 probability map 减去 threshold map (T) 差值，输入到 DB 模块进行可微的二值化学习</li></ul><h1 id="DB-的自适应多尺度特征融合模块-ASF-？"><a href="#DB-的自适应多尺度特征融合模块-ASF-？" class="headerlink" title="DB++的自适应多尺度特征融合模块 ASF ？"></a>DB++的自适应多尺度特征融合模块 ASF ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091650873.png" alt=""></li><li><strong>输入输出</strong>：输入是 BackBone 4 个层次的特征，输出是经过加权的特征</li><li><strong>Spatial Attention</strong>：对特征加空间注意力，使用空间（沿通道方向）进行池化，得到注意力矩阵 $1\times H\times W$</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091513112.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文介绍两个文本检测模型： DB 及它的升级版本 DB++，主要的原理就是将“二值化”的过程做成网络可学习、可微分的一个模块，然后向网络中插入这个模块自适应去学习二值化阈值，最终预测文字区域的一个核心部分，然后再通过公式放大这个核心部分，得到目标区域&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文本检测" scheme="https://shaogui.life/tags/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>CNN之结构重参数化</title>
    <link href="https://shaogui.life/2023/03/06/CNN%E4%B9%8B%E7%BB%93%E6%9E%84%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96/"/>
    <id>https://shaogui.life/2023/03/06/CNN%E4%B9%8B%E7%BB%93%E6%9E%84%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96/</id>
    <published>2023-03-06T04:40:44.000Z</published>
    <updated>2023-05-21T08:08:54.887Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是结构重参数化？"><a href="#什么是结构重参数化？" class="headerlink" title="什么是结构重参数化？"></a>什么是结构重参数化？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005953.png" alt=""></li><li>我们知道模型要变好，就必须构建得更加复杂，但是这来这带来一个坏处，就是模型部署的耗时会增长，这两者是相互矛盾的，<strong>结构从参数化</strong>就是两者都可以做到，在训练的时候，通过复杂的神经网络去训练，提升模型的性能，但是在推理的时候，我通过对模型结构的重参数化生成了一个更加精简的结构，使推理的时候速度更快</li><li><strong>RepVGG 的结构重参数化过程</strong>：上图是左边是训练时的卷积网络，右边通过对结构进行重参数化，得到一个只有 1 个分支的结构，因此可以做到训练时提升性能，推理时提升速度</li><li><strong>结构从参数化的基本原理</strong>：<strong>卷积的可加性</strong>，对于同一个输入，只要其扫描频率一致（相同的通道数、kernel size、stride、padding），其卷积可过程可以融合。如下公式 1 是一个实数乘特征图和乘卷积是等效的，公式 2卷积核 F1 与 F2 可以被融合为 1 个卷积，卷积核为 (F1+F2) </li><li><script type="math/tex; mode=display">\begin{matrix}I\otimes(pF)=p(I\otimes F) \quad&(1)\\I\otimes F^{(1)}+I\otimes F^{(2)}=I\otimes(F^{(1)}+F^{(1)})\quad&(2)\end{matrix}</script></li></ul><h1 id="ACNet-的网络结构？"><a href="#ACNet-的网络结构？" class="headerlink" title="ACNet 的网络结构？"></a>ACNet 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005292.png" alt=""></li><li>上图左边展示 3 个分支的卷积融合为一个等效卷积的过程；右边是卷积融合的过程，主要包括融合 BN (BN fusion) 和融合分支 (branch fusion)两个步骤</li><li><strong>融合 BN (BN fusion)</strong> ：所有 BN 对输入操作一样，不改变输入分辨率，所以利用卷积的线性可加性，将 BN 的过程融合进卷积</li><li><strong>融合分支 (branch fusion)</strong>：同样利用卷积的线性可加性，将多分支的卷积融合为 1个卷积</li></ul><h1 id="RepVGG-如何进行“结构重参数化”？"><a href="#RepVGG-如何进行“结构重参数化”？" class="headerlink" title="RepVGG 如何进行“结构重参数化”？"></a>RepVGG 如何进行“结构重参数化”？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005549.png" alt=""></li><li>1.<strong>卷积层参数</strong>：上图是 REP-VGG 块的结构重参数化过程。为了易于可视化，我们假设 C2 = C1 = 2，因此3×3层具有四个3×3矩阵，而1×1层的核为2×2矩阵</li><li>2.<strong>BN 层参数</strong>：(1)可知 BN 层为每个通道的数据进行规范化，每个通道需要 4 个参数 $\:\mu,\:\sigma,\:\gamma,\:\beta$，输入通道 2 个则有 8 个参数；(2) 当 $\:\mu,\:\sigma,\:\gamma,\:\beta$ 均为 0 时，规范化后的数据还是原来的值，这可以用于模拟 identity 路径</li><li>3.<strong>融合卷积与 BN 层</strong>：(1)最难理解的是虚线红框部分，由原来的 $2\times 1 \times 1\times 2$ 变为 $2\times 3 \times 3\times 2$ ，也就是单个卷积核由 $1\times 1$ 变为 $3\times 3$，这是通过在 $1\times 1$ 四周补 0 做到的，因为补 0 后得到的卷积和是不变的；(2) 类似 [[ACNet#^udwpgu|ACNet的网络结构]]的过程，$\:\mu,\:\sigma,\:\gamma,\:\beta$ 的部分参数用于重构卷积核的值，部分参数组合成卷积的偏置值 $b$，并且每个通道 1 个值 <img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005037.png" alt=""></li><li>4.<strong>利用卷积的可加性，融合多路径</strong>：对应同 size 卷积核的，可以利用卷积的可加性，将卷积融合，具体来说是卷积核矩阵对应相加，偏置值对应相加</li></ul><h1 id="DBB-的网络结构？"><a href="#DBB-的网络结构？" class="headerlink" title="DBB 的网络结构？"></a>DBB 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005802.png" alt=""></li><li>参考 [[GoogleNetv1]] 的 Inception block 的概念，结合结构重参数划的理论，设计了 DBB block</li><li>每个 DBB block 包含 4 个并行的路径，推理时融合成 1 个路径</li></ul><h1 id="DBB-的-6-种模块可以等价转为单个卷积？"><a href="#DBB-的-6-种模块可以等价转为单个卷积？" class="headerlink" title="DBB 的 6 种模块可以等价转为单个卷积？"></a>DBB 的 6 种模块可以等价转为单个卷积？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082006464.png" alt=""></li><li><ol><li>Conv-BN 合并：经典的卷积层融合 BN 层的结构</li></ol></li><li><ol><li>并行合并：参考ACNet的网络结构，卷积核可线性相加</li></ol></li><li><ol><li>串行合并：参考ACNet的网络结构，卷积核可线性相加</li></ol></li><li><ol><li>并行拼接：参考ACNet的网络结构，卷积核 kernel size 保持不变，数量是两个分支相加</li></ol></li><li><ol><li>平均池化转换：平均池化很像卷积核的过程，只不过是求和后加平均而已，直接对卷积核的值除 KxK，后面得到的卷积和就是 AVG 后的值了</li></ol></li><li><ol><li>多尺度卷积合并：参考ACNet的网络结构，同一将卷积核扩充为 KxK，再进行融合</li></ol></li></ul>]]></content>
    
    
    <summary type="html">结构重参数化的原理</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="CNN" scheme="https://shaogui.life/tags/CNN/"/>
    
    <category term="结构重参数化" scheme="https://shaogui.life/tags/%E7%BB%93%E6%9E%84%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>ConvNeXt V2：Co-designing and Scaling ConvNets with Masked Autoencoders</title>
    <link href="https://shaogui.life/2023/02/20/ConvNeXt%20V2%EF%BC%9ACo-designing%20and%20Scaling%20ConvNets%20with%20Masked%20Autoencoders/"/>
    <id>https://shaogui.life/2023/02/20/ConvNeXt%20V2%EF%BC%9ACo-designing%20and%20Scaling%20ConvNets%20with%20Masked%20Autoencoders/</id>
    <published>2023-02-20T05:27:03.000Z</published>
    <updated>2023-05-22T10:10:51.147Z</updated>
    
    <content type="html"><![CDATA[<p>ConvNeXtv2借鉴掩码自编码器（MAE），在 ConvNeXt 的基础上引入全卷积掩码自编码器 (FCMAE)，但是发现 MLP 层存在潜在的特征崩溃问题，为了解决这个问题，该研究提出添加一个全局响应归一化层（Global Response Normalization layer，GRN）来增强通道间的特征竞争</p><span id="more"></span><h1 id="什么是-ConvNeXtv2-？"><a href="#什么是-ConvNeXtv2-？" class="headerlink" title="什么是 ConvNeXtv2 ？"></a>什么是 ConvNeXtv2 ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/ConvNeXtv2-20230505210037.png" alt=""></li><li>ConvNeXtv2借鉴掩码自编码器（MAE），在 ConvNeXt 的基础上引入全卷积掩码自编码器 (FCMAE)，但是发现 MLP 层存在潜在的特征崩溃问题，为了解决这个问题，该研究提出添加一个全局响应归一化层（Global Response Normalization layer，GRN）来增强通道间的特征竞争</li><li>ConvNeXtv2 表明监督学习中重复使用监督学习中的固定架构设计可能不是最佳方法，可以使用部分自监督+监督</li></ul><h1 id="ConvNeXtv2-的全卷积掩码自编码器-FCMAE-？"><a href="#ConvNeXtv2-的全卷积掩码自编码器-FCMAE-？" class="headerlink" title="ConvNeXtv2 的全卷积掩码自编码器 (FCMAE)？"></a>ConvNeXtv2 的全卷积掩码自编码器 (FCMAE)？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/ConvNeXtv2-20230505210038.png" alt=""></li><li>参考 <a href="K31-transformer/MAE.md">MAE</a> 的原理，将 encoder 和 decoder 的 vit 部分换成卷积，然后使用稀疏卷积替换 mask token。即原始的输入信号被随机 mask，输入 Encoder，希望 Encoder + Decoder 的输出预测 mask 掉的部分</li><li><strong>Encoder</strong>：使用 ConvNeXt，在预训练期间，将卷积替换为稀疏卷积，这使得模型只能在可见数据点上操作；在微调阶段，稀疏卷积层可以转换回标准卷积，而不需要额外的处理</li><li><strong>Decoder</strong>：使用 1 个 ConvNeXt Block，总体上形成了非对称的 Encoder-Decoder 体系结构，因为 Encoder 更重，且具有分层架构</li></ul><h1 id="ConvNeXtv2-的自监督性能？"><a href="#ConvNeXtv2-的自监督性能？" class="headerlink" title="ConvNeXtv2 的自监督性能？"></a>ConvNeXtv2 的自监督性能？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/ConvNeXtv2-20230505210039.png" alt=""></li><li>将 FCMAE 与监督学习进行比较。有监督训练 100 Epochs 精度是82.7%，有监督训练 300 Epochs 精度是83.8%，FCMAE 进行 800和100个 Epoch 的预训练和微调结果是 83.7%。说明 FCMAE 预训练提供了比随机基线更好的初始化 (82.7→83.7)</li></ul><h1 id="ConvNeXtv2-的全局响应标准化-Global-Response-Normalization-GRN"><a href="#ConvNeXtv2-的全局响应标准化-Global-Response-Normalization-GRN" class="headerlink" title="ConvNeXtv2 的全局响应标准化 (Global Response Normalization, GRN)?"></a>ConvNeXtv2 的全局响应标准化 (Global Response Normalization, GRN)?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/ConvNeXtv2-20230505210040.png" alt=""></li><li>GRN 通过求实例在每个通道的权重，增强通道间的特征竞争，避免出现特征崩溃。图 1是 ConvNeXtv2 应用 GRN 后，可以看出每个通道都有响应，不像 ConvNeXtv1 某些通道已经不响应</li><li>根据实验发现，当应用 GRN 时，LayerScale 不是必要的并且可以被删除，如图 2 所示</li><li>图 3 是不同层的规范化层的特征余弦距离，余弦距离越大，特征越保持，可以看出 ConvNeXtv 2的特征一直在保持，更 Vit MAE 类似</li></ul><h1 id="全局响应标准化（GRN）的计算过程？"><a href="#全局响应标准化（GRN）的计算过程？" class="headerlink" title="全局响应标准化（GRN）的计算过程？"></a>全局响应标准化（GRN）的计算过程？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-05-05%2016.29.31.excalidraw.png" alt="Drawing 2023-05-05 16.29.31.excalidraw"></li><li><strong>规范化</strong>：传统的规范化层是求出均值 $\mu$ 与方差 $\sigma$，然后使用 $\hat X_i=(X_i-\mu)/\sigma$ 进行规范化，而 GRN 只需要求出权重 $w$，然后使用 $\hat X_i=X_i\times w$ 规范化</li><li><strong>规范化值</strong>：这里的 w 是实例在所有实例上的权重，如图对于 (B, HW, C)输入的特征，先求 (B1, C1)实例上的 L2 范数，该实例的 L2 范数在所有实例 L2 范数的比例即等于 w</li><li><strong>校准数据</strong>：不同于 BN 使用公式 $X_i=\gamma\times\hat X_i+\beta$ 校准数据，GRN 使用 $X_i=\gamma\times\hat X_i+\beta+X_i$ 校准数据，相当于引入一个残差连接</li></ul><h1 id="ConvNeXtv2-的损失函数？"><a href="#ConvNeXtv2-的损失函数？" class="headerlink" title="ConvNeXtv2 的损失函数？"></a>ConvNeXtv2 的损失函数？</h1><ul><li>遵循 MAE 的做法，使用重建目标和真值之间的 Mean Squared Error, MSE Loss，作为 Reconstruction target。损失函数仅应用于 masked patches</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;ConvNeXtv2借鉴掩码自编码器（MAE），在 ConvNeXt 的基础上引入全卷积掩码自编码器 (FCMAE)，但是发现 MLP 层存在潜在的特征崩溃问题，为了解决这个问题，该研究提出添加一个全局响应归一化层（Global Response Normalization layer，GRN）来增强通道间的特征竞争&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="图片分类" scheme="https://shaogui.life/tags/%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>YOLOv6：A Single-Stage Object Detection Framework for Industrial Applications</title>
    <link href="https://shaogui.life/2023/02/10/YOLOv6%EF%BC%9AA%20Single-Stage%20Object%20Detection%20Framework%20for%20Industrial%20Applications/"/>
    <id>https://shaogui.life/2023/02/10/YOLOv6%EF%BC%9AA%20Single-Stage%20Object%20Detection%20Framework%20for%20Industrial%20Applications/</id>
    <published>2023-02-10T03:46:04.000Z</published>
    <updated>2023-05-21T08:07:33.832Z</updated>
    
    <content type="html"><![CDATA[<p>YOLOv6基于RepVGG设计了可重参数化、更高效的骨干网络</p><span id="more"></span><h1 id="什么是-YOLOv6"><a href="#什么是-YOLOv6" class="headerlink" title="什么是 YOLOv6?"></a>什么是 YOLOv6?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/YOLOv6-20230408141952.png" alt=""></li><li>1）受到硬件感知神经网络设计思想的启发，基于 RepVGG 设计了可重参数化、更高效的骨干网络 EfficientRep、RepPAN</li><li>2）优化设计了更简洁有效 YOLOv6 的解耦头，在维持精度的同时，进一步降低了一般解耦头带来的额外延时开销</li><li>3）在训练策略上，我们采用 Anchor-free无锚范式，同时 SimOTA标签分配策略、SIOUloss边界框回归损失来进一步提高检测精度</li></ul><h1 id="YOLOv6的网络结构？"><a href="#YOLOv6的网络结构？" class="headerlink" title="YOLOv6的网络结构？"></a>YOLOv6的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/YOLOv6-20230408141952-1.png" alt=""></li><li>1）受到硬件感知神经网络设计思想的启发，基于 RepVGG 设计了可重参数化、更高效的骨干网络 EfficientRep、RepPAN</li><li>2）优化设计了更简洁有效YOLOv6的解耦头，在维持精度的同时，进一步降低了一般解耦头带来的额外延时开销</li></ul><h1 id="YOLOv6的解耦头"><a href="#YOLOv6的解耦头" class="headerlink" title="YOLOv6的解耦头?"></a>YOLOv6的解耦头?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/YOLOv6-20230408141954.png" alt=""></li><li>YOLOv5检测头是通过分类和回归分支融合共享的方式来实现的，而 YOLOX的检测头则是将分类和回归分支进行解耦，同时新增了两个额外的 3x3 的卷积层，虽然提升了检测精度，但一定程度上增加了网络延时</li><li>在 YOLOv6 中，采用了解耦头结构，并对其进行了精简设计。采用 Hybrid Channels 策略重新设计了一个更高效的解耦头结构，在维持精度的同时降低了延时，缓解了解耦头中 3x3 卷积带来的额外延时开销</li></ul><h1 id="什么是-EfficientRep？"><a href="#什么是-EfficientRep？" class="headerlink" title="什么是 EfficientRep？"></a>什么是 EfficientRep？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/YOLOv6-20230408141955.png" alt=""></li><li>基于以上 Rep 算子设计了一个高效的 Backbone。相比于 YOLOv5采用的 CSP-Backbone，该 Backbone 能够高效利用硬件（如 GPU）算力的 同时，还具有较强的表征能力</li></ul><h1 id="什么是-RepPAN？"><a href="#什么是-RepPAN？" class="headerlink" title="什么是 RepPAN？"></a>什么是 RepPAN？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/YOLOv6-20230408141955-1.png" alt=""></li><li>基于硬件感知神经网络设计思想，为 YOLOv6 设计了一个更有效的特征融合网络结构RepPAN</li><li>基于PAN拓扑方式，用RepBlock替换了YOLOv5中使用的CSP-Block，同时对整体 Neck 中的算子进行了调整，目的是在硬件上达到高效推理的同时，保持较好的多尺度特征融合能力</li></ul><h1 id="什么是SimSPPF？"><a href="#什么是SimSPPF？" class="headerlink" title="什么是SimSPPF？"></a>什么是SimSPPF？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/YOLOv6-20230408141956.png" alt=""></li><li>YOLOv6提出的模块，感觉SPPF只差了一个激活函数，简单测试了一下，单个ConvBNReLU速度要比ConvBNSiLU快18%</li></ul><h1 id="YOLOv6-的-SimOTA-标签分配策略？"><a href="#YOLOv6-的-SimOTA-标签分配策略？" class="headerlink" title="YOLOv6 的 SimOTA 标签分配策略？"></a>YOLOv6 的 SimOTA 标签分配策略？</h1><ul><li>为了获得更多高质量的正样本，YOLOv6 引入了 SimOTA 动态分配正样本，进一步提高检测精度，YOLOv5 的标签分配策略是基于 Shape 匹配，并通过跨网格匹配策略增加正样本数量，从而使得网络快速收敛，但是该方法属于静态分配方法，并不会随着网络训练的过程而调整</li><li>在YOLOX中，OT又简化为动态的top-k策略进行分配，也就是得到一个近似解的策略：动态样本匹配(SimOTA)。YOLOv6也沿用这个标签分配的方法</li></ul><p>参考:</p><ol><li><a href="https://blog.csdn.net/weixin_43694096/article/details/126354660">空间金字塔池化改进 SPP / SPPF / SimSPPF / ASPP / RFB / SPPCSPC / SPPFCSPC<em>空洞空间卷积池化金字塔</em>迪菲赫尔曼的博客-CSDN博客</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;YOLOv6基于RepVGG设计了可重参数化、更高效的骨干网络&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="目标检测" scheme="https://shaogui.life/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    <category term="anchor-free" scheme="https://shaogui.life/tags/anchor-free/"/>
    
    <category term="one-stage" scheme="https://shaogui.life/tags/one-stage/"/>
    
  </entry>
  
  <entry>
    <title>ConvNeXt：A ConvNet for the 2020s</title>
    <link href="https://shaogui.life/2023/02/03/ConvNeXt%EF%BC%9AA%20ConvNet%20for%20the%202020s/"/>
    <id>https://shaogui.life/2023/02/03/ConvNeXt%EF%BC%9AA%20ConvNet%20for%20the%202020s/</id>
    <published>2023-02-03T05:26:59.000Z</published>
    <updated>2023-05-22T10:11:14.834Z</updated>
    
    <content type="html"><![CDATA[<p>针对目前火热的 transformer，ConvNeXt 认为不是卷积固有的劣势导致的 CNN 性能比 transformer 差，而是 CNN 的设计不充分导致的</p><span id="more"></span><h1 id="什么是-ConvNeXt-？"><a href="#什么是-ConvNeXt-？" class="headerlink" title="什么是 ConvNeXt ？"></a>什么是 ConvNeXt ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/ConvNeXt-20230505210019.png" alt=""></li><li>针对目前火热的 transformer，ConvNeXt 认为不是卷积固有的劣势导致的 CNN 性能比 transformer 差，而是 CNN 的设计不充分导致的</li><li>ConvNeXt 基于 ResNet50，并根据过去十年中的新最佳实践和发现对其进行迭代改进，使得 CNN 的性能超过transformer</li></ul><h1 id="ConvNeXt-优化“宏观设计”？"><a href="#ConvNeXt-优化“宏观设计”？" class="headerlink" title="ConvNeXt 优化“宏观设计”？"></a>ConvNeXt 优化“宏观设计”？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/ConvNeXt-20230505210020.png" alt=""></li><li><strong>stage ratio</strong>：众所周知 resnet50有4个由若干个 block 堆叠出的 stage，每个 stage 的 block 数量不太相同，作者将堆叠次数按照 Swin-T (1,1,3,1)来设计，ResNet 50 由 (3,4,6,3)调整为 (3,3,9,3)</li><li><strong>patchify stem</strong>：将 stem 更改为 Patchify，即 ResNet50 开头的 7 x 7 卷积（stride=4）、3 x 3 最大池化（stride=4）-&gt;4 x 4（stride=4）</li><li>stage ratio 相当于加深原始 resnet50，带来效果最明显</li></ul><h1 id="ConvNeXt-优化“参考-ResNetXt”？"><a href="#ConvNeXt-优化“参考-ResNetXt”？" class="headerlink" title="ConvNeXt 优化“参考 ResNetXt”？"></a>ConvNeXt 优化“参考 ResNetXt”？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/ConvNeXt-20230505210021.png" alt=""></li><li>ResNetXt 对 BottleNeck 中的 3x3 卷积层采用分组卷积来减少 FLOPS。 在 ConvNext 中使用depth-wise convolution</li></ul><h1 id="ConvNeXt-优化“参考倒残差-Inverted-Bottleneck”？"><a href="#ConvNeXt-优化“参考倒残差-Inverted-Bottleneck”？" class="headerlink" title="ConvNeXt 优化“参考倒残差 Inverted Bottleneck”？"></a>ConvNeXt 优化“参考倒残差 Inverted Bottleneck”？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/ConvNeXt-20230505210022.png" alt=""></li><li>resnet 里 block 的设计是先1x1卷积降维，之后再用1x1卷积升维，形成一个瓶颈层。而 Transformer 块的一个重要设计是，它会产生一个<strong>反向瓶颈</strong>(b)，ConvNeXt 将 b 的 3 x 3 提前，</li></ul><h1 id="ConvNeXt-优化“使用大卷积核”？"><a href="#ConvNeXt-优化“使用大卷积核”？" class="headerlink" title="ConvNeXt 优化“使用大卷积核”？"></a>ConvNeXt 优化“使用大卷积核”？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/ConvNeXt-20230505210023.png" alt=""></li><li>参考 Swin-T 中使用了7x7的窗口，ConvNeXt 将 Inverted Bottleneck 的 3 x 3 卷积替换成 7 x 7，为了降低计算量把 dw conv 放到了 Inverted Bottleneck 的开头，最终结果相近</li></ul><h1 id="ConvNeXt-优化“微观设计”？"><a href="#ConvNeXt-优化“微观设计”？" class="headerlink" title="ConvNeXt 优化“微观设计”？"></a>ConvNeXt 优化“微观设计”？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/ConvNeXt-20230505210024.png" alt=""></li><li><strong>ReLU-&gt;GELU</strong>：GELU 可以被认为是 ReLU 的一个更平滑的变体，最近的 transformer 中被使用，ConvNet 使用后性能保持不变</li><li><strong>更少激活函数</strong>：Transformer 和 ResNet 块之间的一个小区别是 Transformer 的激活功能较少，根据这一思路消除将 ResNet Block 的 3 个 RELU 激活改为 1 个 GELU 激活</li><li><strong>更少 BN 层</strong>：Transformer块通常也有较少的规范化层。在这里，我们移除了两个BatchNorm层，<strong>在conv 1×1层之前只留下一个BN层</strong></li><li><strong>BN-&gt;LN</strong>：BatchNorm 是 ConvNets 中的一个重要组成部分，因为它提高了收敛性，减少了过度拟合，在原始 ResNet 中直接用 LN 代替 BN 将导致次优性能。在对网络架构和训练技术进行修改后，在这里重新使用 LN 代替 BN，性能稍好一些</li><li><strong>分离下采样层</strong>：在 ResNet 中，下采样是通过 stride=2 conv 完成的。 Transformers（以及其他卷积网络）也有一个单独的下采样模块。 作者删除了 stride=2 并在三个 conv 之前添加了一个下采样块，为了保持训练期间的稳定性在，在下采样操作之前需要进行归一化</li></ul><p>参考：</p><ol><li><a href="https://blog.csdn.net/qq_37541097/article/details/122556545">ConvNeXt网络详解_太阳花的小绿豆的博客-CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/458016349">ConvNeXt：全面超越Swin Transformer的CNN - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/511477312">使用PyTorch复现ConvNext：从Resnet到ConvNext的完整步骤详解 - 知乎</a></li><li><a href="https://blog.csdn.net/qq_41917697/article/details/122997809">CNN卷积神经网络之ConvNeXt_球场书生的博客-CSDN博客</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;针对目前火热的 transformer，ConvNeXt 认为不是卷积固有的劣势导致的 CNN 性能比 transformer 差，而是 CNN 的设计不充分导致的&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="图片分类" scheme="https://shaogui.life/tags/%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>PP-LiteSeg：A Superior Real-Time Semantic Segmentation Model</title>
    <link href="https://shaogui.life/2023/01/28/PP-LiteSeg%EF%BC%9AA%20Superior%20Real-Time%20Semantic%20Segmentation%20Model/"/>
    <id>https://shaogui.life/2023/01/28/PP-LiteSeg%EF%BC%9AA%20Superior%20Real-Time%20Semantic%20Segmentation%20Model/</id>
    <published>2023-01-28T12:42:20.000Z</published>
    <updated>2023-05-20T11:03:07.544Z</updated>
    
    <content type="html"><![CDATA[<p>为搭建一个轻量化的语义分割网络，PP-LiteSeg 设计了灵活轻便的 encoder 模块、统一的注意力模块和高效的金字塔特征融合模块</p><span id="more"></span><h1 id="什么是-PP-LiteSeg-？"><a href="#什么是-PP-LiteSeg-？" class="headerlink" title="什么是 PP-LiteSeg ？"></a>什么是 PP-LiteSeg ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PP-LiteSeg-20230408143341.png" alt="PP-LiteSeg-20230408143341"></li><li>为搭建一个轻量化的语义分割网络，PP-LiteSeg 设计了灵活轻便的 encoder 模块、统一的注意力模块和高效的金字塔特征融合模块</li></ul><h1 id="PP-LiteSeg-的网络结构？"><a href="#PP-LiteSeg-的网络结构？" class="headerlink" title="PP-LiteSeg 的网络结构？"></a>PP-LiteSeg 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PP-LiteSeg-20230408143341.png" alt="PP-LiteSeg-20230408143341"></li><li><strong>Flexible and Lightweight Decoder</strong>(FLD)：灵活轻便的 encoder 模块，用于提取图片的特征，主要特点是通道数不增反减</li><li><strong>Unified Attention Fusion Module</strong>(UAFM)：使用 2 种空间注意力和通道注意力，构建输入特征的空间之间以及通道之间的关系</li><li><strong>Simple Pyramid Pooling Module</strong>(SPPM)：简单的特征融合模块，它减少了中间通道和输出通道，消除了Shortcut，并用一个add操作替换了concat操作</li></ul><h1 id="PP-LiteSeg-的-Flexible-and-Lightweight-Decoder-FLD-？"><a href="#PP-LiteSeg-的-Flexible-and-Lightweight-Decoder-FLD-？" class="headerlink" title="PP-LiteSeg 的 Flexible and Lightweight Decoder (FLD)？"></a>PP-LiteSeg 的 Flexible and Lightweight Decoder (FLD)？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PP-LiteSeg-20230408143412.png" alt="PP-LiteSeg-20230408143412"></li><li>一般来说，<strong>编码器</strong>利用一系列分成几个阶段的层来提取层次特征。对于从 low-level 到 high-level 的特征，通道的数量逐渐增加，特征的空间尺寸逐渐减小</li><li><strong>解码器</strong>也有几个阶段，负责对 low-level 到 high-level 的特征融合和上采样特征，为了提高解码器的效率，FLD逐渐将特征的通道从high-level减少到low-level层次</li></ul><h1 id="PP-LiteSeg-的-Unified-Attention-Fusion-Module-UAFM-？"><a href="#PP-LiteSeg-的-Unified-Attention-Fusion-Module-UAFM-？" class="headerlink" title="PP-LiteSeg 的 Unified Attention Fusion Module (UAFM)？"></a>PP-LiteSeg 的 Unified Attention Fusion Module (UAFM)？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PP-LiteSeg-20230408143413.png" alt="PP-LiteSeg-20230408143413"></li><li>输入特征被表示为 $F<em>{hight}$ 和 $F</em>{low}$。$F<em>{hight}$ 是深层模块的输出，而 $F</em>{low}$ 是编码器的对应模块。请注意，它们有相同的通道数量</li><li>UAFM 首先利用双线性插值操作将 $F<em>{hight}$ 上采样到 $F</em>{low}$ 相同大小，而上采样特征记为 $F<em>{up}$。然后，注意力模块以 $F</em>{up}$ 和 $F_{low}$ 作为输入，产生权重 $\alpha$</li><li>注意，注意力模块可以是一个插件，如空间注意力模块、通道注意力模块等</li></ul><h1 id="PP-LiteSeg-的-Simple-Pyramid-Pooling-Module-SPPM-？"><a href="#PP-LiteSeg-的-Simple-Pyramid-Pooling-Module-SPPM-？" class="headerlink" title="PP-LiteSeg 的 Simple Pyramid Pooling Module (SPPM)？"></a>PP-LiteSeg 的 Simple Pyramid Pooling Module (SPPM)？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PP-LiteSeg-20230408143414.png" alt="PP-LiteSeg-20230408143414"></li><li>与原始的 金字塔池化模块（PPM） 相比，SPPM 减少了中间通道和输出通道，删除了 shortcut，并用 add 操作替换了 cat 操作。因此，SPPM 更有效，也更适合用于实时模型</li></ul><p>参考：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/540851314">PP-LiteSeg：轻量级实时语义分割模型 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/506936919">高精度轻量级图像分割SOTA模型PP-LiteSeg重磅开源！ - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;为搭建一个轻量化的语义分割网络，PP-LiteSeg 设计了灵活轻便的 encoder 模块、统一的注意力模块和高效的金字塔特征融合模块&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="语义分割" scheme="https://shaogui.life/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
    <category term="轻量" scheme="https://shaogui.life/tags/%E8%BD%BB%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>GhostNetV2：Enhance Cheap Operation with Long-Range Attention</title>
    <link href="https://shaogui.life/2023/01/13/GhostNetV2%EF%BC%9AEnhance%20Cheap%20Operation%20with%20Long-Range%20Attention/"/>
    <id>https://shaogui.life/2023/01/13/GhostNetV2%EF%BC%9AEnhance%20Cheap%20Operation%20with%20Long-Range%20Attention/</id>
    <published>2023-01-13T15:56:37.000Z</published>
    <updated>2023-05-22T10:16:15.746Z</updated>
    
    <content type="html"><![CDATA[<p>GhostNetV2在GhostNetV1的基础上，提出新的注意力机制，用于捕获长距离的空间信息，在精度和推理速度之间获得更好的平衡</p><span id="more"></span><h1 id="什么是-GhostNetv2-？"><a href="#什么是-GhostNetv2-？" class="headerlink" title="什么是 GhostNetv2 ？"></a>什么是 GhostNetv2 ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/GhostNetv2-20230408140227.png" alt=""></li><li>GhostNetv1通过将标准卷积分解为2个过程，减少了计算量，但是其特征表示能力下降，GhostNetv2则通过引入注意力来解决</li><li>如果构建特征的自注意力，需要构建 HWxHW 大小的注意力矩阵，这对轻量化模型是有害的，借鉴 CBAM 的 SAM 模块，直接构建其 HW 大小的<strong>空间注意力</strong>矩阵，同时为了轻量化，通过分解卷积去构建，最终得到 DFC 注意力构建模块</li></ul><h1 id="GhostNetv2-的-G-bootleneck"><a href="#GhostNetv2-的-G-bootleneck" class="headerlink" title="GhostNetv2 的 G-bootleneck?"></a>GhostNetv2 的 G-bootleneck?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/GhostNetv2-20230408140228.png" alt="GhostNetv2-20230408140228"></li><li>GhostNetv2修改了 G-bootleneck，输入特征时，第一个 Ghost Module 变为并行生成特征及特征注意力，注意力加权特征后再给后续模块</li></ul><h1 id="GhostNetv2-的“双解藕密集连接注意力“模块"><a href="#GhostNetv2-的“双解藕密集连接注意力“模块" class="headerlink" title="GhostNetv2 的“双解藕密集连接注意力“模块?"></a>GhostNetv2 的“双解藕密集连接注意力“模块?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/GhostNetv2-20230408140228-1.png" alt=""></li><li>GhostNetv2没有直接构建 HWxHW 大小的注意力矩阵，而是依次在水平方向、垂直方向构建注意力，获得全局注意力，其构建注意力的复杂度由 O (HWxHW)变为 $O(H^2W+HW^2)$</li><li><strong>SE Block</strong>：输入 hwc，直接使用1x1卷积，生成大小为 hw 的注意力矩阵，c 的每个通道共用这个注意力矩阵</li><li><strong>DFC</strong>：输入hwc，使用分组卷积+分解卷积，生成大小为hwc的注意力矩阵，每个通道有自己的注意力矩阵</li></ul><p><strong>参考资料</strong></p><ol><li><a href="https://blog.csdn.net/ooooocj/article/details/128994836">GhostNet v2（NeurIPS 2022 Spotlight）原理与代码解析_ghostnet源码解析_00000cj的博客-CSDN博客</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;GhostNetV2在GhostNetV1的基础上，提出新的注意力机制，用于捕获长距离的空间信息，在精度和推理速度之间获得更好的平衡&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="图片分类" scheme="https://shaogui.life/tags/%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/"/>
    
    <category term="轻量" scheme="https://shaogui.life/tags/%E8%BD%BB%E9%87%8F/"/>
    
  </entry>
  
</feed>
