<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>年轻人起来冲</title>
  
  
  <link href="https://shaogui.life/atom.xml" rel="self"/>
  
  <link href="https://shaogui.life/"/>
  <updated>2023-05-20T02:34:20.949Z</updated>
  <id>https://shaogui.life/</id>
  
  <author>
    <name>绍桂</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Segment Anything</title>
    <link href="https://shaogui.life/2023/05/20/Segment%20Anything/"/>
    <id>https://shaogui.life/2023/05/20/Segment%20Anything/</id>
    <published>2023-05-20T11:36:23.000Z</published>
    <updated>2023-05-20T02:34:20.949Z</updated>
    
    <content type="html"><![CDATA[<p>SAM通过transformer将点、框、Mask、文本等prompt和图片进行编码学习，可以实现对图片任意目标的分割</p><a id="more"></a><h1 id="什么是-SAM-？"><a href="#什么是-SAM-？" class="headerlink" title="什么是 SAM ？"></a>什么是 SAM ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAM-3.png" alt=""></li><li>a)SAM 利用“图片-分割提示”实现对图片上任意目标的分割，分割提示包括：点、框、Mask、文本</li><li>b) SAM 首先利用 prompt encoder 编码”分割提示”，利用 image encoder 编码“图片”，然后通过 Mask decoder 解析输出 Mask</li><li>c)SAM 利用数据驱动去做模型训练，模型输出结果后再输入模型训练</li></ul><h1 id="SAM-的网络结构？"><a href="#SAM-的网络结构？" class="headerlink" title="SAM 的网络结构？"></a>SAM 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAM.png" alt=""></li><li><strong>image encoder</strong>：类似 VIT 的过程，输入 image (1,3, H, W), 输出 image_embedding (1, C, H/16, W/16)，即 (1, HW/256, C)的 tokens 表示</li><li><strong>mask</strong>：mask prompt，直接和image_embedding相加即可</li><li><strong>prompt encoder</strong>：包含3种提示的编码过程，其中点、框按位置被编码为Pos embedding(1,N,C)，文本通过clip模型被编码为Pos embedding(1,M,C)</li><li><strong>mask decoder</strong>：根据image_embedding和prompt encoder输出，结合IOU tokens(1,1,C)和mask tokens(1,P,C)，解析出目标mask(1,1+P+N+M, H/16, W/16)和iou(1,1+P+N+M)</li></ul><h1 id="SAM-的-image-encoder？"><a href="#SAM-的-image-encoder？" class="headerlink" title="SAM 的 image encoder？"></a>SAM 的 image encoder？</h1><ul><li>类似 VIT 的 encoder 过程，输入 image (1,3, H, W), 输出 image_embedding (1, C, H/16, W/16)，即 (1, HW/256, C)的 tokens 表示</li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> image_encoder=ImageEncoderViT(..)</span><br><span class="line"> <span class="comment"># batched_input=&#123;List,List&#125; -&gt; torch.Size([2, 3, 1024, 1024])</span></span><br><span class="line"> input_images = torch.stack([preprocess(x[<span class="string">&quot;image&quot;</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> batched_input], dim=<span class="number">0</span>) </span><br><span class="line"><span class="comment"># torch.Size([2, 3, 1024, 1024]) -&gt; torch.Size([2, 256, 64, 64])</span></span><br><span class="line"> image_embeddings = image_encoder(input_images)</span><br></pre></td></tr></table></figure><h1 id="SAM-的-prompt-encoder"><a href="#SAM-的-prompt-encoder" class="headerlink" title="SAM 的 prompt encoder?"></a>SAM 的 prompt encoder?</h1></li><li><p>包含3种提示的编码过程，其中点、框按位置被编码为 Pos embedding (1, N, C)，文本通过 clip 模型被编码为 Pos embedding (1, M, C)，最终输出（1,N+M,C )的稀疏编码sparse_embeddings</p></li><li><strong>point&amp;box</strong>：每个点编码为1个 pos embedding，每个 box 编码为2个 pos embedding（box 被两个点定义）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"> embed_dim=<span class="number">256</span></span><br><span class="line"> num_point_embeddings: <span class="built_in">int</span> = <span class="number">4</span>  <span class="comment"># pos/neg point + 2 box corners</span></span><br><span class="line"> point_embeddings = [nn.Embedding(<span class="number">1</span>, embed_dim) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_point_embeddings)]</span><br><span class="line"> point_embeddings = nn.ModuleList(point_embeddings)</span><br><span class="line"> not_a_point_embed = nn.Embedding(<span class="number">1</span>, embed_dim)</span><br><span class="line"><span class="comment"># point prompt</span></span><br><span class="line">points = points + <span class="number">0.5</span>  <span class="comment"># Shift to center of pixel  </span></span><br><span class="line"><span class="comment"># 根据点位置points，在输入(1024,1024)的基础上生成pos embedding</span></span><br><span class="line"> point_embedding = pe_layer.forward_with_coords(points, input_image_size) <span class="comment">#torch.Size([1,3,2])+(1024,1024)-&gt;torch.Size([1,3,256])</span></span><br><span class="line"> <span class="comment"># 点有3类，-1表示非嵌入点，此时不使用pos embedding，0表示正样本点，1表示负样本点</span></span><br><span class="line">point_embedding[labels == -<span class="number">1</span>] = <span class="number">0.0</span></span><br><span class="line"> point_embedding[labels == -<span class="number">1</span>] += not_a_point_embed.weight</span><br><span class="line"> point_embedding[labels == <span class="number">0</span>] += point_embeddings[<span class="number">0</span>].weight</span><br><span class="line"> point_embedding[labels == <span class="number">1</span>] += point_embeddings[<span class="number">1</span>].weight</span><br><span class="line"><span class="comment"># box prompt</span></span><br><span class="line"> boxes = boxes + <span class="number">0.5</span>  <span class="comment"># Shift to center of pixel</span></span><br><span class="line"> coords = boxes.reshape(-<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>) <span class="comment"># 一个框肯定2个点</span></span><br><span class="line"> corner_embedding = pe_layer.forward_with_coords(coords, input_image_size)</span><br><span class="line"> corner_embedding[:, <span class="number">0</span>, :] += point_embeddings[<span class="number">2</span>].weight <span class="comment">#框第一个点</span></span><br><span class="line"> corner_embedding[:, <span class="number">1</span>, :] += point_embeddings[<span class="number">3</span>].weight <span class="comment">#框第二个点</span></span><br><span class="line"><span class="comment"># 汇总point、box编码</span></span><br><span class="line">sparse_embeddings = torch.empty((<span class="number">1</span>, <span class="number">0</span>, embed_dim))</span><br><span class="line">sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=<span class="number">1</span>)</span><br><span class="line">sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><strong>text</strong>：通过CLIP模型将文本编码到(1,M,C)</li></ul><h1 id="SAM的mask-prompt如何处理？"><a href="#SAM的mask-prompt如何处理？" class="headerlink" title="SAM的mask prompt如何处理？"></a>SAM的mask prompt如何处理？</h1><ul><li>mask利用CNN输出和image_embedding(1,C,H/16,W/16)一样大小的编码，后续直接相加</li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mask_downscaling = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, mask_in_chans // <span class="number">4</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    LayerNorm2d(mask_in_chans // <span class="number">4</span>),</span><br><span class="line">    activation(),</span><br><span class="line">    nn.Conv2d(mask_in_chans // <span class="number">4</span>, mask_in_chans, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    LayerNorm2d(mask_in_chans),</span><br><span class="line">    activation(),</span><br><span class="line">    nn.Conv2d(mask_in_chans, embed_dim, kernel_size=<span class="number">1</span>),</span><br><span class="line">)</span><br><span class="line">mask_input_size = (<span class="number">4</span> * image_embedding_size[<span class="number">0</span>], <span class="number">4</span> * image_embedding_size[<span class="number">1</span>])</span><br><span class="line">no_mask_embed = nn.Embedding(<span class="number">1</span>, embed_dim) </span><br><span class="line"><span class="keyword">if</span> masks <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    dense_embeddings = self._embed_masks(masks) <span class="comment"># 利用CNN生成mask embedding</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    dense_embeddings = self.no_mask_embed.weight.reshape(<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).expand(</span><br><span class="line">        bs, -<span class="number">1</span>, self.image_embedding_size[<span class="number">0</span>], self.image_embedding_size[<span class="number">1</span>]</span><br><span class="line">    ) <span class="comment"># 随机初始化生成mask embedding</span></span><br></pre></td></tr></table></figure><h1 id="SAM-的-mask-decoder"><a href="#SAM-的-mask-decoder" class="headerlink" title="SAM 的 mask decoder?"></a>SAM 的 mask decoder?</h1></li><li><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAM-4.png" alt=""></p></li><li><strong>输入</strong>:image_embedding(1, C, H/16, W/16)、image_embedding大小的位置编码image_pe(1, C, H/16, W/16)、稀疏提示编码sparse_prompt_embeddings(1, N, C)、密集提示编码dense_prompt_embeddings(1,C,H/16, W/16)</li><li><strong>(1)tansformer整合所有编码</strong>:将image_embedding+dense_prompt_embeddings视为transformer encoder的k,image_pe视为pos embedding,sparse_prompt_embeddings视为decoder的q，并且参考VIT的class_token，不直接使用sparse_prompt_embeddings输出作为最终结果，而是另外生成1个iou token和P个mask token作为最终结果，所以输入transformer decoder的token变为(1,1+P+N,C)，经过transformer后decoder和encoder分别输出hs(1,1+P+N,C), src(1,HW/256,C)；<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"> num_multimask_outputs=<span class="number">3</span></span><br><span class="line">transformer_dim=<span class="number">256</span></span><br><span class="line"> iou_token = nn.Embedding(<span class="number">1</span>, transformer_dim)</span><br><span class="line"> num_mask_tokens = num_multimask_outputs + <span class="number">1</span></span><br><span class="line"> mask_tokens = nn.Embedding(num_mask_tokens, transformer_dim)</span><br><span class="line"> <span class="comment"># Concatenate output tokens</span></span><br><span class="line"> output_tokens = torch.cat([iou_token.weight, mask_tokens.weight], dim=<span class="number">0</span>) <span class="comment"># torch.Size([5, 256])</span></span><br><span class="line"> output_tokens = output_tokens.unsqueeze(<span class="number">0</span>).expand(sparse_prompt_embeddings.size(<span class="number">0</span>), -<span class="number">1</span>, -<span class="number">1</span>) <span class="comment"># torch.Size([1, 5, 256])</span></span><br><span class="line"> tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=<span class="number">1</span>) <span class="comment"># torch.Size([1, 12, 256])</span></span><br><span class="line"> <span class="comment"># Expand per-image data in batch direction to be per-mask</span></span><br><span class="line"> src = torch.repeat_interleave(image_embeddings, tokens.shape[<span class="number">0</span>], dim=<span class="number">0</span>) <span class="comment"># torch.Size([1, 256, 64, 64]) -》torch.Size([1, 256, 64, 64])</span></span><br><span class="line"> src = src + dense_prompt_embeddings <span class="comment"># torch.Size([1, 256, 64, 64])+torch.Size([1, 256, 64, 64])</span></span><br><span class="line"> pos_src = torch.repeat_interleave(image_pe, tokens.shape[<span class="number">0</span>], dim=<span class="number">0</span>) <span class="comment"># torch.Size([1, 256, 64, 64])</span></span><br><span class="line"> b, c, h, w = src.shape</span><br><span class="line"> <span class="comment"># Run the transformer torch.Size([1, 256, 64, 64])，torch.Size([1, 256, 64, 64])，torch.Size([1, 12, 256])</span></span><br><span class="line"> hs, src = transformer(src, pos_src, tokens) <span class="comment"># torch.Size([1, 12, 256]) torch.Size([1, 4096, 256]) = q,k</span></span><br><span class="line"> iou_token_out = hs[:, <span class="number">0</span>, :] <span class="comment"># torch.Size([1, 256])</span></span><br><span class="line"> mask_tokens_out = hs[:, <span class="number">1</span> : (<span class="number">1</span> + num_mask_tokens), :] <span class="comment"># torch.Size([1, 4, 256])</span></span><br></pre></td></tr></table></figure></li><li><strong>(2)生成Mask预测</strong>：取hs的第1-P个token作为预测结果mask_tokens_out，src经过反卷积上采样4倍，输出upscaled_embedding(1,HW/16,C’)，mask_tokens_out经过MLP操作，将隐变量长度变为C’,即输出hyper_in(1,P,C’)，hyper_in与upscaled_embedding点乘后输出masks(1,P,HW/16)，表示p个mask<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">self.output_upscaling = nn.Sequential(</span><br><span class="line">    nn.ConvTranspose2d(transformer_dim, transformer_dim // <span class="number">4</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    LayerNorm2d(transformer_dim // <span class="number">4</span>),</span><br><span class="line">    activation(),</span><br><span class="line">    nn.ConvTranspose2d(transformer_dim // <span class="number">4</span>, transformer_dim // <span class="number">8</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    activation(),</span><br><span class="line">)</span><br><span class="line">self.output_hypernetworks_mlps = nn.ModuleList(</span><br><span class="line">    [MLP(transformer_dim, transformer_dim, transformer_dim // <span class="number">8</span>, <span class="number">3</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_mask_tokens)])  </span><br><span class="line"><span class="comment"># Upscale mask embeddings and predict masks using the mask tokens</span></span><br><span class="line">src = src.transpose(<span class="number">1</span>, <span class="number">2</span>).view(b, c, h, w) <span class="comment"># torch.Size([1, 256, 64, 64])</span></span><br><span class="line">upscaled_embedding = self.output_upscaling(src) <span class="comment"># torch.Size([1, 256, 64, 64]) -》torch.Size([1, 32, 256, 256])</span></span><br><span class="line">hyper_in_list: List[torch.Tensor] = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_mask_tokens):</span><br><span class="line">    hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :])) <span class="comment"># torch.Size([1, 32])x4</span></span><br><span class="line">hyper_in = torch.stack(hyper_in_list, dim=<span class="number">1</span>) <span class="comment"># torch.Size([1, 4, 32])</span></span><br><span class="line">b, c, h, w = upscaled_embedding.shape <span class="comment"># torch.Size([1, 32, 256, 256])</span></span><br><span class="line"><span class="comment"># 运算符@表示矩阵的点乘</span></span><br><span class="line">masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -<span class="number">1</span>, h, w) <span class="comment"># torch.Size([1, 4, 32]) @ torch.Size([1, 32, 256, 256]) -&gt; torch.Size([1, 4, 256, 256])</span></span><br></pre></td></tr></table></figure></li><li><p><strong>(3)生成IOU预测</strong>：取hs的第1个token作为预测结果iou_token_out，然后使用MLP将隐变量长度变为P，表示P各mask的iou预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">iou_prediction_head = MLP(transformer_dim, iou_head_hidden_dim, num_mask_tokens, iou_head_depth)</span><br><span class="line"><span class="comment"># Generate mask quality predictions</span></span><br><span class="line">iou_pred = iou_prediction_head(iou_token_out) <span class="comment"># torch.Size([1,256]) -&gt; torch.Size([1, 4])  </span></span><br></pre></td></tr></table></figure><h1 id="SAM-如何直接分割所有目标？"><a href="#SAM-如何直接分割所有目标？" class="headerlink" title="SAM 如何直接分割所有目标？"></a>SAM 如何直接分割所有目标？</h1></li><li><p>以原图所有cell作为point prompt输入，输出Mask和iou后，通过iou阈值过滤mask,得到所有目标的mask</p></li></ul><p>参考：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/621040230">模型方法—-真的分割任何东西(Segment Anything) - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;SAM通过transformer将点、框、Mask、文本等prompt和图片进行编码学习，可以实现对图片任意目标的分割&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
    <category term="分割" scheme="https://shaogui.life/tags/%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>我的实例分割学习路线</title>
    <link href="https://shaogui.life/2023/05/13/%E6%88%91%E7%9A%84%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    <id>https://shaogui.life/2023/05/13/%E6%88%91%E7%9A%84%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/</id>
    <published>2023-05-13T15:55:34.000Z</published>
    <updated>2023-05-20T05:34:56.582Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2013.39.58.excalidraw.svg" alt="Drawing 2023-03-22 13.39.58.excalidraw"></p><p>本文总结自己目前对实例分割 的认识，和学习过程</p><a id="more"></a><h1 id="什么是实例分割？"><a href="#什么是实例分割？" class="headerlink" title="什么是实例分割？"></a>什么是实例分割？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2010.13.11.excalidraw.svg" alt="Drawing 2023-03-22 10.13.11.excalidraw"></li><li>目标检测针对的是目标，语义分割针对的是像素，而实例分割针对的是实例。所谓实例就是一个不管类别、不管是否连续的 1 个目标</li><li>上图是对一张图上 3 类 4 个实例的分割示意图，最后的输出结果是<strong>每个实例的语义分割图</strong></li></ul><h1 id="实例分割的原理？"><a href="#实例分割的原理？" class="headerlink" title="实例分割的原理？"></a>实例分割的原理？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2010.27.55.excalidraw.png" alt="Drawing 2023-03-22 10.27.55.excalidraw"></li><li><strong>Two-satge</strong>：既然实例分割是针对实例的语义分割，最直接的办法就是先找出单个实例的区域，然后对这个区域进行语义分割即可</li><li><strong>one-stage</strong>：不找实例的区域，而是针对每个 grid 生成 1 个语义分割预测，通过后处理获得实例的类别及分割结果</li></ul><h1 id="实例分割的方法？"><a href="#实例分割的方法？" class="headerlink" title="实例分割的方法？"></a>实例分割的方法？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2009.38.03.excalidraw.svg" alt="Drawing 2023-03-22 09.38.03.excalidraw"></li><li><mark style="background: #FF5582A6;">two-satge</mark>：类似于目标检测的 two-satge 模型，即<strong>先检测出目标的获选框，然后对候选框进行语义分割和位置回归</strong></li><li><mark style="background: #FFB86CA6;">one-stage</mark>：类似于目标检测的 one-stage 模型，即<strong>先通过 grid 确保这个位置有目标，然后对这个位置进行位置回归 (目标检测)或者 mask 生成 (实例分割)</strong></li><li>two-stage 的方法原始直接，但是对重叠目标的 Mask 预测比较麻烦，而且速度较慢；one-stage 类似 YOLO 系列的思想，速度较快</li></ul><h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-14%2020.11.25.excalidraw.svg" alt="Drawing 2023-03-14 20.11.25.excalidraw"></li><li>和目标检测一样，实例分割使用 mAP 评价模型性能，注意：<strong>统计某类的 TP、FP、FN 时，是针对所有图片的<mark style="background: #FF5582A6;">实例</mark>预测结果进行，不针对具体图片</strong></li><li>AP 是指某个类别预测情况的平均精准率，mAP 指所有类别 AP 的平均</li><li>AP 可以通过求解 PR 曲线下的面积得到，求解方式包括11个点和矩形求解</li><li>每个 PR 区域是某个 IOU 阈值绘制的，并且这个 IOU 阈值已经由单阈值发展到多阈值</li></ul><h1 id="two-stage"><a href="#two-stage" class="headerlink" title="two-stage"></a>two-stage</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2013.46.18.excalidraw.svg" alt="Drawing 2023-03-22 13.46.18.excalidraw"></li><li>自上而下的实例分割方法，首先按获取候选框，然后在此基础上进行目标框回归和 Mask 生成</li><li><strong>Mask RCNN</strong>：使用 RPN 获得候选框、使用 Faster RCNN 预测目标类别、使用 FCN 生成 Mask</li><li><strong>PAN</strong>：通过在 FPN 的基础上引入 bottom-up 路径，让底层信息更快传递到高层，其思想和 Mask RCNN 一致</li></ul><h1 id="One-stage"><a href="#One-stage" class="headerlink" title="One-stage"></a>One-stage</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2013.47.15.excalidraw.svg" alt="Drawing 2023-03-22 13.47.15.excalidraw"></li><li>自下而上的实例分割方法类似 YOLO 系列将每个 grid 看作 1 个目标，这类实例分割方法将每个 grid 视为一个实例，并为每个 grid 预测 1 个 Mask</li><li><strong>SOLO</strong>：输出包含 2 个分支，一个是 heatmap 分支，判定该 grid 是否包含实例，另一个分支是为该 grid 生成 Mask</li><li><strong>SOLOv2</strong>：在 SOLOv1 的基础上，将 Mask 分支解藕卷积核生成、卷积特征生成 2 个分支，监督网络学习卷积核，使得网络能动态学习实例的特征</li></ul><h1 id="学习路线"><a href="#学习路线" class="headerlink" title="学习路线"></a>学习路线</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2013.39.58.excalidraw.svg" alt="Drawing 2023-03-22 13.39.58.excalidraw"></li></ul><ol><li><a href="MaskRCNN.md">MaskRCNN</a>：在 Faster RCNN 的基础上增加 FCN 分支，提出 ROIAlign 对齐 ROI 下采样</li><li><a href="PAN.md">PAN</a>：类似 MaskRCNN 过程，BackBone 使用 PAN 进行特征融合，最后融合所有尺度目标进行目标定位与分割</li><li><a href="yolact.md">yolact</a>：首先生成一批 prototype mask，然后目标分支生成一组权重，加权得到每个 grid 的分割结果</li><li><a href="SOLO.md">SOLO</a>：首先 Category 分支对 grid 进行分类，然后 Mask 分支生成每个 grid 的分割，实际使用通过解藕头构建 Mask 分支</li><li><a href="SOLOv2.md">SOLOv2</a>：将 SOLO 的 Mask 生成分支解藕为<strong>掩码核预测分支</strong>和掩码特征学习分支，分别负责生成卷积核和需要卷积的特征映射</li><li><a href="yolactplusplus.md">yolactplusplus</a>：基本和 yolact 类似，生成更多 anchor 、重新生成的 Mask scoreing 分支</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2013.39.58.excalidraw.svg&quot; alt=&quot;Drawing 2023-03-22 13.39.58.excalidraw&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文总结自己目前对实例分割 的认识，和学习过程&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="实例分割" scheme="https://shaogui.life/tags/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>我的OCR学习路线</title>
    <link href="https://shaogui.life/2023/05/10/%E6%88%91%E7%9A%84OCR%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    <id>https://shaogui.life/2023/05/10/%E6%88%91%E7%9A%84OCR%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/</id>
    <published>2023-05-10T05:09:54.000Z</published>
    <updated>2023-05-20T05:11:29.738Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-11%2018.42.57.excalidraw.svg" alt="Drawing 2023-04-11 18.42.57.excalidraw"></p><p>本文总结自己目前对 OCR 的认识，和学习过程</p><a id="more"></a><h1 id="什么是-OCR-？"><a href="#什么是-OCR-？" class="headerlink" title="什么是 OCR ？"></a>什么是 OCR ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/FOTS-20230409214506.png" alt=""></li><li>OCR （Optical Character Recognition，光学字符识别）指将打字、手写或印刷文本的图像电子或机械转换为机器编码文本的过程</li><li><strong>文本检测 (Text detection)</strong> ：<strong>检测文本的所在位置和范围及其布局</strong>，可以使用传统的 ROI 提取实现，也可使用目标检测去实现，如 Faster R-CNN，[[FCN]]</li><li><strong>文本识别 (Text recognition)</strong>：<strong>对文本内容进行识别</strong>，将图像中的文本信息转化为文本信息</li><li><strong>文本定位 (Text Spotting)</strong> ：分文本检测 (Text detection) 文本识别 (Text recognition)统一到一起的简称</li></ul><h1 id="OCR-的方法？"><a href="#OCR-的方法？" class="headerlink" title="OCR 的方法？"></a>OCR 的方法？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-11%2018.42.57.excalidraw.svg" alt="Drawing 2023-04-11 18.42.57.excalidraw"></li><li><strong>文本检测 (Text detection)</strong> ：其实就是检测文本行<strong>实例</strong>，可以使用目标检测的方法，也可使用语义分割的方法</li><li><strong>文本识别 (Text recognition)</strong> ：这个是 OCR 的重点，主要有 3 条路线</li><li><strong>文本定位 (Text Spotting)</strong> ：分为两种，但阶段和双阶段</li></ul><h1 id="评价指标-字符评价"><a href="#评价指标-字符评价" class="headerlink" title="评价指标-字符评价"></a>评价指标-字符评价</h1><ul><li>以字符 （文字和标点符号） 为单位的统计和分析，适用于通用印刷体、手写体类非结构化数据的OCR应用评测</li><li><strong>字符召回率</strong>：预测正确的字符总数占<strong>总符号</strong>的比例</li><li><strong>字符准确率</strong>：预测正确的字符占<strong>总测试结果</strong>的比例</li><li><strong>F-socre</strong>：字符召回率和字符准确率的综合评价指标</li></ul><h1 id="评价指标-文本段评价"><a href="#评价指标-文本段评价" class="headerlink" title="评价指标-文本段评价"></a>评价指标-文本段评价</h1><ul><li>以字段为单位的统计和分析，适用于卡证类、票据类等结构化程度较高的 OCR 应用评测</li><li><strong>字段召回率</strong>：完全识别准确的字段总数占<strong>总字段</strong>的比例</li><li><strong>字段准确率</strong>：完全识别准确的字段占<strong>总测试结果</strong>的比例</li><li><strong>最小编辑距离</strong>：编辑距离是针对二个字符串（例如英文字）的差异程度的量化量测，通过替换、插入、删除，将预测结果修正为gt所需操作步骤，最小编辑距离表示最少操作步数</li><li><strong>全图编辑距离</strong>：整个文本段的编辑距离</li><li><strong>归一化编辑距离</strong>: 编辑距离除以字符串长度</li></ul><h1 id="学习路线"><a href="#学习路线" class="headerlink" title="学习路线"></a>学习路线</h1><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-22%2014.25.52.excalidraw.svg" alt="Drawing 2023-03-22 14.25.52.excalidraw"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-11%2018.42.57.excalidraw.svg&quot; alt=&quot;Drawing 2023-04-11 18.42.57.excalidraw&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文总结自己目前对 OCR 的认识，和学习过程&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
  </entry>
  
  <entry>
    <title>EAST：An Efficient and Accurate Scene Text Detector</title>
    <link href="https://shaogui.life/2023/04/11/EAST%EF%BC%9AAn%20Efficient%20and%20Accurate%20Scene%20Text%20Detector/"/>
    <id>https://shaogui.life/2023/04/11/EAST%EF%BC%9AAn%20Efficient%20and%20Accurate%20Scene%20Text%20Detector/</id>
    <published>2023-04-11T14:35:22.000Z</published>
    <updated>2023-05-20T04:09:46.170Z</updated>
    
    <content type="html"><![CDATA[<p>EAST基于FCN输出，对每个grid进行文本行预测，可实现旋转矩形框、任意四边形框的预测</p><a id="more"></a><h1 id="什么是-EAST-？"><a href="#什么是-EAST-？" class="headerlink" title="什么是 EAST ？"></a>什么是 EAST ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/EAST-20230408144126.png" alt="EAST-20230408144126"></li><li>（a）、（b）、（c）、（d）都是几种常见 stat-of-the-art 的文本检测过程，算法思想遵循之前 two-stage 的方法，一般都需要先提出候选框，过滤后对剩下的候选框要进行回归操作得出更精细的边框信息，然后再合并候选框等</li><li><strong>EAST</strong>基于 FCN 输出特征，类似 anchor-free 的目标检测模型，预测每个 grid 的代表的文本行信息，然后使用 NMS（非极大值抑制）合并预测后的信息，可实现矩形、选择矩阵和四边形的文本检测，不能实现弯曲文本的检测</li></ul><h1 id="EAST-的网络结构？"><a href="#EAST-的网络结构？" class="headerlink" title="EAST 的网络结构？"></a>EAST 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/EAST-20230408144126-1.png" alt="EAST-20230408144126-1"></li><li><strong>FCN 特征提取</strong>：通过<strong>特征提取</strong>和<strong>特征融合</strong>两个步骤，最后取 C 2 特征输入预测头</li><li><strong>预测结果的输出层</strong>：假设 C 2 特征大小为 CHW，对于 HW 的每个 grid 输出 2 个分支，第一个分支是置信度 (1)，第二个分支是框位置，框位置如果是旋转矩形，则输出 4 (xyxy)+1（angle），如果是任意的四边形, 则输出 8 (xy * 4)</li></ul><h1 id="EAST-的标签分配？"><a href="#EAST-的标签分配？" class="headerlink" title="EAST 的标签分配？"></a>EAST 的标签分配？</h1><ul><li>检测 head 没有设置 anchor，直接按映射位置确定正样本，文本行比较大，可以按照多正样本匹配</li></ul><h1 id="EAST-的标签生成？"><a href="#EAST-的标签生成？" class="headerlink" title="EAST 的标签生成？"></a>EAST 的标签生成？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/EAST-20230408144127.png" alt="EAST-20230408144127"></li><li><strong>垂直或水平矩阵框 (AABB)</strong>：只需要 4 个值就可描述；<strong>旋转矩形框 (RBOX)</strong> ： AABB 的基础上增加角度，共 5 个值描述；<strong>任意四边形（QUAD）</strong>：需要 4 个点 8 个值去描述</li></ul><h1 id="EAST-的损失函数？"><a href="#EAST-的损失函数？" class="headerlink" title="EAST 的损失函数？"></a>EAST 的损失函数？</h1><ul><li><script type="math/tex; mode=display">L=L_\mathrm{s}+\lambda_\mathfrak{g}L_\mathfrak{g}</script></li><li><strong>分割损失 $L_s$</strong>：使用 blance 的交叉熵</li><li><strong>位置损失 $L_g$</strong>：直接使用 L 1 或者 L 2 损失去回归文本区域将导致损失偏差朝更大更长, 所以使用 IOU loss 监督 AABB 或 RBOX 类型框的位置；对于 QUAD 类型的回归框，使用尺度归一化的 smooth L 1 损失</li></ul><h1 id="EAST-如何解析模型输出"><a href="#EAST-如何解析模型输出" class="headerlink" title="EAST 如何解析模型输出"></a>EAST 如何解析模型输出</h1><ul><li>模型输出包括 2 部分，1）score map：检测框的置信度，1 个参数；2）text boxes：对于检测形状为 RBOX，检测框的位置（x, y, w, h）+旋转角度 (angle)，5 个参数；对于检测形状为 QUAD，则输出任意四边形检测框的位置坐标，(x 1, y 1), (x 2, y 2), (x 3, y 3), (x 4, y 4)，8 个参数</li><li>取 topK 的 score map 对应的预测框，然后采用 Locality-Aware NMS 过滤这些预测框，得到最终结果</li></ul><p>参考：</p><ol><li><a href="https://cloud.tencent.com/developer/article/1542875">05. OCR学习路径之文本检测（下）EAST算法简介 - 腾讯云开发者社区-腾讯云</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;EAST基于FCN输出，对每个grid进行文本行预测，可实现旋转矩形框、任意四边形框的预测&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文本检测" scheme="https://shaogui.life/tags/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>SAST：A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning</title>
    <link href="https://shaogui.life/2023/04/10/SAST%EF%BC%9AA%20Single-Shot%20Arbitrarily-Shaped%20Text%20Detector%20based%20on%20Context%20Attended%20Multi-Task%20Learning/"/>
    <id>https://shaogui.life/2023/04/10/SAST%EF%BC%9AA%20Single-Shot%20Arbitrarily-Shaped%20Text%20Detector%20based%20on%20Context%20Attended%20Multi-Task%20Learning/</id>
    <published>2023-04-10T15:20:53.000Z</published>
    <updated>2023-04-10T15:30:27.798Z</updated>
    
    <content type="html"><![CDATA[<p>属于EAST的演进版本，还是类似 anchor-free 的方式预测文本行，除了输出 grid 的 score +边框预测外，还输出更多的文本实例信息，比如 grid 到实例矩形四角、中心点、四边的距离，使得SAST可以检测弯曲文本行、中间有间隔的文本行</p><a id="more"></a><h1 id="什么是-SAST-？"><a href="#什么是-SAST-？" class="headerlink" title="什么是 SAST ？"></a>什么是 SAST ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204412.png" alt=""></li><li>属于 <a href="EAST.md">EAST</a> 的演进版本，还是类似 anchor-free 的方式预测文本行，但是除了输出 grid 的 score +边框预测外，还输出更多的文本实例信息，比如 grid 到实例矩形四角、中心点、四边的距离</li><li>每个 grid 更加复杂的输出，可以让 SAST 检测更为复杂场景下的文本行，比如弯曲文本行、中间有间隔的文本行</li></ul><h1 id="SAST-的网络结构？"><a href="#SAST-的网络结构？" class="headerlink" title="SAST 的网络结构？"></a>SAST 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204443.png" alt=""></li><li><strong>Featrue Extractor</strong>：BackBone 部分，通过类似 SegNet 的过程提取特征</li><li><strong>CABs</strong>：交叉注意力模块，用于整合 BackBone 的特征</li><li><strong>TCL map (1 xHxW)</strong>: grid 属于文本中心线像素点的概率</li><li><strong>TCO map (2 xHxW)</strong>: 文本中心点偏置，grid 距其所属的文本实例矩形框中心的 xy 方向距离</li><li><strong>TVO map (8 xHxW)</strong>: 文本四顶点偏置，grid 距其所属的文本实例矩形框四顶点的 xy 方向距离</li><li><strong>TBO map (4 xHxW)</strong>: 文本边界偏置，grid 距其所属的文本实例上下边界框的 xy 方向距离</li></ul><h1 id="SAST-的-CAB-模块？"><a href="#SAST-的-CAB-模块？" class="headerlink" title="SAST 的 CAB 模块？"></a>SAST 的 CAB 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204535.png" alt=""></li><li>交叉注意力模块，用于整合 BackBone 的特征，该模块分为上下两部分，上部分构建水平方向注意力，下部分构建垂直方向注意力，整合水平方向注意力和垂直方向注意力得到<strong>全局注意力</strong></li></ul><h1 id="SAST-样本制作？"><a href="#SAST-样本制作？" class="headerlink" title="SAST 样本制作？"></a>SAST 样本制作？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204602.png" alt=""> </li><li><strong>a). TCL map (1 xHxW)</strong>：文本中心线区域，文本行上下边界收缩 20%后得到的区域，而左右边界仍保持不变</li><li><strong>b). TBO map (4 xHxW)</strong>：文本边界偏置，首先计算斜率 k 1 (v 1, v 2)与斜率 k 1 (v 4, v 3)的平均值，对于一个给定的点 P 0，可容易地计算出斜率为 (k 1+k 2)/2、过点 P 0 的直线，由此该直线与线段 (v 1, v 4)和线段 (v 2, v 3)的交点 P 1 与 P 2 很容易得出，故 P 0 的上下边界点 $P<em>{upper}$ 和 $P</em>{lower}$ 的坐标可由线段比例关系得到，整理得到 P 0 点到四边距离的 TBO 为{$P<em>0^x-P_1^x$、 $P_0^x-P</em>{lower}^x$ 、$P<em>2^y-P_0^y$、$P</em>{upper}^y-P_0^y$}</li><li><strong>c). TVO map (8 xHxW)</strong>：文本顶点偏置，文本最小矩形框按根据一定规则由文本标注信息计算得到，计算文本中心区域中某像素点到文本矩形框四顶点的直线距离（包括 x 方向和 y 方向），所以共计给每个 grid 生成 8 个 TVO 预测</li><li><strong>d). TCO map (2 xHxW)</strong>：文本中心点偏置，计算文本中心区域内某像素点到文本最小矩形框中心点的距离 (x 方向和 y 方向)</li></ul><h1 id="SAST-的损失函数？"><a href="#SAST-的损失函数？" class="headerlink" title="SAST 的损失函数？"></a>SAST 的损失函数？</h1><ul><li><script type="math/tex; mode=display">L_{total}=\lambda_1L_{tcl}+\lambda_2L_{tco}+\lambda_3L_{tvo}+\lambda_4L_{tbo},</script></li><li><strong>(1) TCL map:</strong> 使用 Minimizing the Dice loss 作为分割 loss, 用于描述两个轮廓的相似程度</li><li><strong>(2) TVO/TCO/TBO:</strong> 使用 Smooth L 1 Loss 作为几何图 geometry map 的回归 loss</li></ul><h1 id="解析-SAST-的输出-1-生成文本实例？"><a href="#解析-SAST-的输出-1-生成文本实例？" class="headerlink" title="解析 SAST 的输出 1-生成文本实例？"></a>解析 SAST 的输出 1-生成文本实例？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204913.png" alt=""></li><li><strong>a)</strong> <strong>根据 TCL 获得文本实例包含的像素点-&gt;文本行 Mask</strong>，阈值过滤将置信率低于某值的假阳性像素点剔除，得到合适的 TCL map;</li><li><strong>b) 根据 TVO+NMS 获得文本实例-&gt;文本矩形框</strong>：将经过处理的 TCL map 中每个像素点，根据 TVO 文本实例顶点偏置图，得到对应的文本矩形框四顶点坐标，并进行非最大值抑制 NMS，得到所需的文本实例矩形框及其中心点</li><li><strong>b+c=d) 根据 TCO 合并文本实例-&gt;文本行 Mask</strong> ：计算 TCL 中属于文本的像素点的所属文本实例的几何中心点，该中心点将作为低层级像素信息，当步骤 c 计算所得的几何中心点与步骤 b 所得矩形框中心点重合或相近时，该像素点将被归类给步骤 b 中矩形框对应的文本实例，通过此步骤重新合并断开的文本行</li></ul><h1 id="解析-SAST-的输出-2-生成文本边框？"><a href="#解析-SAST-的输出-2-生成文本边框？" class="headerlink" title="解析 SAST 的输出 2-生成文本边框？"></a>解析 SAST 的输出 2-生成文本边框？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SAST-20230410204412.png" alt=""></li><li><strong>a)</strong> 前面解析得到的文本实例</li><li><strong>b)</strong> 对文本中心线采样，采样点的间距相同，则得到的采样点数目与文本线的长度有关，故称之为自适应采样</li><li><strong>c)</strong> 根据文本边界偏置图 TBO 所提供的信息，计算文本中心线的采样点上的上下边界定位点</li><li><strong>d)</strong> 将步骤 b 所得的边界定位点按照从左上角开始的顺时针方向依次进行连接，得到最终的文本边界框</li></ul><p>参考：</p><ol><li><a href="https://blog.csdn.net/qq_37546096/article/details/102909850">SAST : Single-Shot Arbitrarily-Shaped Text Detector论文阅读笔记_text center line sampling_litchi9854的博客-CSDN博客</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;属于EAST的演进版本，还是类似 anchor-free 的方式预测文本行，除了输出 grid 的 score +边框预测外，还输出更多的文本实例信息，比如 grid 到实例矩形四角、中心点、四边的距离，使得SAST可以检测弯曲文本行、中间有间隔的文本行&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文本检测" scheme="https://shaogui.life/tags/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>FOTS：Fast Oriented Text Spotting with a Unified Network</title>
    <link href="https://shaogui.life/2023/04/09/FOTS%EF%BC%9AFast%20Oriented%20Text%20Spotting%20with%20a%20Unified%20Network/"/>
    <id>https://shaogui.life/2023/04/09/FOTS%EF%BC%9AFast%20Oriented%20Text%20Spotting%20with%20a%20Unified%20Network/</id>
    <published>2023-04-09T13:42:24.000Z</published>
    <updated>2023-04-09T14:28:39.081Z</updated>
    
    <content type="html"><![CDATA[<p>第一个<strong>端到端</strong>解决文本识别的模型，相比较两阶段的文本定位方法，它的检测速度更快，基本思路是通过文本检测分支实现文本行区域的提取，然后通过 RoIRotate 模块实现文本行的“摆正”，最后使用 CRNN+CTC 的模式实现文本行的字符识别</p><a id="more"></a><h1 id="什么是-FOTS-？"><a href="#什么是-FOTS-？" class="headerlink" title="什么是 FOTS ？"></a>什么是 FOTS ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304092146678.png" alt=""></li><li>第一个<strong>端到端</strong>解决文本识别的模型，相比较两阶段的文本定位方法，它的检测速度更快，基本思路是通过文本检测分支实现文本行区域的提取，然后通过 RoIRotate 模块实现文本行的“摆正”，最后使用 CRNN+CTC 的模式实现文本行的字符识别</li><li>RoIRotate 模块要通过仿射变换转换文本区域，所以 FOTS 只能识别文字中心在一个线上的文本行，无法处理弯曲文本行</li></ul><h1 id="FOTS-的网络结构？"><a href="#FOTS-的网络结构？" class="headerlink" title="FOTS 的网络结构？"></a>FOTS 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304092146065.png" alt=""></li><li><strong>shared convolutions</strong>：使用 Resnet 搭建，首先使用下采样，然后使用反卷积上采样，并且使用类似 SegNet 的高分辨率连接到低分辨率的连接</li><li><strong>文本检测分支</strong>：使用类似 anchor-free 的目标检测方式，将 shared convolutions 的每个 grid 视为文本行中心，然后预测其文本行的宽高</li><li><strong>RoIRotate</strong>：根据文本检测分支的输出+shared convolutions 输出，将文本行转为横向文本</li><li><strong>文字识别分支</strong>：基于 CRNN+CTC 的方式学习和识别文本行</li></ul><h1 id="FOTS-的-shared-convolutions-模块？"><a href="#FOTS-的-shared-convolutions-模块？" class="headerlink" title="FOTS 的 shared convolutions 模块？"></a>FOTS 的 shared convolutions 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/FOTS-20230409194643.png" alt=""></li><li>首先通过 ResNet 提取特征，然后通过反卷积上采样，类似 SegNet 一样中间使用残差连接，最后输出 C 2 特征</li></ul><h1 id="FOTS-的文本检测分支？"><a href="#FOTS-的文本检测分支？" class="headerlink" title="FOTS 的文本检测分支？"></a>FOTS 的文本检测分支？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-09%2020.21.26.excalidraw.svg" alt="Drawing 2023-04-09 20.21.26.excalidraw"></li><li>使用类似 anchor-free 的目标检测方式，将 shared convolutions 的每个 grid 视为文本行中心，然后预测其文本行的宽高</li><li>假设 shared convolutions 输出是 $C\times H \times W$ 的特征，文本检测分支输出 3 个分支，分别表示文本行的得分、该 grid 到四边的距离和该文本行的旋转角度</li></ul><h1 id="FOTS-的-RoIRotate-模块？"><a href="#FOTS-的-RoIRotate-模块？" class="headerlink" title="FOTS 的 RoIRotate 模块？"></a>FOTS 的 RoIRotate 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/FOTS-20230409194714.png" alt=""></li><li>使用目标检测的后处理获得文本行，根据文本行的宽高及旋转角得到四个角点的位置，假设四个点是 ($x_1$, $y_1$)、($x_2$, $y_2$)、($x_3$, $y_3$)、($x_4$, $y_4$)，现在要将这个区域转到 (0,0)起点，宽高 (wh)的区域，可以通过仿射变换实现</li><li>仿射变换矩阵需要变换前后的 3 对点求得，不妨取 ($x_1$, $y_1$)-&gt;(0,0)、($x_2$, $y_2$)-&gt;（w, 0）、($x_3$, $y_3$)-&gt;(w, h)，求取方法是调用 opencv 的 getAffineTransform 函数即可，仿射矩阵变换后，文本的中心线平行 x 轴</li></ul><h1 id="FOTS-的文字识别分支？"><a href="#FOTS-的文字识别分支？" class="headerlink" title="FOTS 的文字识别分支？"></a>FOTS 的文字识别分支？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/CRNN-20230408144101.png" alt=""></li><li>文字识别是在 RoIRotate 模块输出的基础上进行的，就是得到平行文本行的基础上进行的，其过程有 4 个</li><li><strong>CNN 提取特征</strong>：使用轻量化网络 MobileNetv 3，其中输入图像的高度统一设置为 32，宽度可以为任意长度，经过 CNN 网络后，特征图的高度缩放为 1</li><li><strong>双向 LSTM（BiLSTM）对特征序列进行预测</strong>：学习序列中的每个特征向量并输出预测标签分布。这里其实相当于把特征向量的宽度视为 LSTM 中的时间维度</li><li><strong>全连接层分类</strong>：使用全连接层对每个序列进行 N+1 类别预测，获取模型的预测结果</li><li><strong>CTC</strong>：解码模型输出的预测结果，得到最终输出</li></ul><h1 id="FOTS-的损失函数？"><a href="#FOTS-的损失函数？" class="headerlink" title="FOTS 的损失函数？"></a>FOTS 的损失函数？</h1><ul><li>网络的损失分为两部分，即文本行识别损失 $L<em>{detect}$ 、文本行字符识别损失 $L</em>{recog}$，通过参数 $\lambda_{recog}$ 控制两者的权重<script type="math/tex; mode=display">L=L_{\mathbf{detect}}+\lambda_{\mathbf{recog}}L_{\mathbf{recog}}</script></li></ul><p>参考：</p><ol><li><a href="https://blog.csdn.net/sol_data12/article/details/113501530">场边文字检测——FOTS模型详解及其代码实现_ManManMan池的博客-CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/195248125">[论文笔记] FOTS - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;第一个&lt;strong&gt;端到端&lt;/strong&gt;解决文本识别的模型，相比较两阶段的文本定位方法，它的检测速度更快，基本思路是通过文本检测分支实现文本行区域的提取，然后通过 RoIRotate 模块实现文本行的“摆正”，最后使用 CRNN+CTC 的模式实现文本行的字符识别&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文本定位" scheme="https://shaogui.life/tags/%E6%96%87%E6%9C%AC%E5%AE%9A%E4%BD%8D/"/>
    
  </entry>
  
  <entry>
    <title>文本检测之DB和DB++</title>
    <link href="https://shaogui.life/2023/04/08/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E4%B9%8BDB%E5%92%8CDB++/"/>
    <id>https://shaogui.life/2023/04/08/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E4%B9%8BDB%E5%92%8CDB++/</id>
    <published>2023-04-08T07:03:50.000Z</published>
    <updated>2023-05-20T04:05:32.477Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091513112.png" alt=""></p><p>本文介绍两个文本检测模型： DB 及它的升级版本 DB++，主要的原理就是将“二值化”的过程做成网络可学习、可微分的一个模块，然后向网络中插入这个模块自适应去学习二值化阈值，最终预测文字区域的一个核心部分，然后再通过公式放大这个核心部分，得到目标区域</p><a id="more"></a><h1 id="什么是-DB-？"><a href="#什么是-DB-？" class="headerlink" title="什么是 DB ？"></a>什么是 DB ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091513112.png" alt=""></li><li><strong>传统意义二值化</strong>：基于分割的文本检测算法其流程如图2中的蓝色箭头所示。在传统方法中得到分割结果之后采用一个<strong>固定阈值</strong>得到二值化的分割图</li><li><strong>DB 二值化</strong>：如图2中红色箭头所示的，通过网络去预测图片每个位置处的阈值，而不是采用一个固定的值，这样就可以很好将背景与前景分离出来，但是这样的操作会给训练带来梯度不可微的情况，对此对于二值化提出了一个叫做 <strong>Differentiable Binarization 模块</strong>来解决</li></ul><h1 id="DB-的网络结构？"><a href="#DB-的网络结构？" class="headerlink" title="DB 的网络结构？"></a>DB 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091633772.png" alt=""></li><li><strong>BackBone</strong>：可以使用类似 resnet 下采样 5 次，得到 5 个层次的特征</li><li><strong>FPN 类似结构</strong>：对 C4、C3、C2 特征采样类似 FPN 的连接，输出时是 C5、 F4、F3、F2 一共 4 个层次的特征</li><li><strong>DB 模块</strong>：以 probability map 减去 threshold map (T) 差值，输入到 DB 模块进行可微的二值化学习</li></ul><h1 id="DB-的“可微的二值化模块-DB-”？"><a href="#DB-的“可微的二值化模块-DB-”？" class="headerlink" title="DB 的“可微的二值化模块 (DB)”？"></a>DB 的“可微的二值化模块 (DB)”？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091634515.png" alt=""></li><li>上图 a、b、c 分别是标准二值化与可微二值化输出、可微二值化对正样本的梯度，可微二值化对负样本的梯度，k 是放大倍数</li><li><strong>标准二值化 (SB)</strong>：通过预先设置的阈值 t 去对概率图 $P_{i,j}$ 二值化<script type="math/tex; mode=display">B_{i,j}=\begin{cases}1\quad P_{i,j}>=t\\ 0\quad otherwise\end{cases}</script></li><li><strong>可微二值化 (DB)</strong>：借鉴 sigmoid 输出输出，将 $P<em>{i,j}-T{i,j}$ 作为 sigmoid 输入，并 K 扩大输出，使得 $\hat{B}</em>{i,j}$ 趋向 0 或 1，即通过学习每个位置的阈值 $T<em>{i, j}$ 对概率图 $P</em>{i, j}$ 二值化 <script type="math/tex; mode=display">\hat{B}_{i, j}=\dfrac{1}{1+\exp^{-k (P_{i, j}-T_{i, j})}}</script></li><li><strong>正负样本梯度比较</strong>：DB 改进性能的原因可以通过梯度的反向传播来解释，可知正负样本的梯度被 k 放大 <script type="math/tex; mode=display">\begin{aligned}l_{+}=-log\frac{1}{1+e^{-}k x}\quad =>  \frac{\partial l_{+}}{\partial x}=-k f(x)e^{-k x}\\\\ l_{-}=-log(1-\frac{1}{1+e^{-}k x})\quad\quad =>  \frac{\partial l_{-}}{\partial x}=k f(x)\end{aligned}</script></li></ul><h1 id="DB-的自适应阈值？"><a href="#DB-的自适应阈值？" class="headerlink" title="DB 的自适应阈值？"></a>DB 的自适应阈值？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091635325.png" alt=""></li><li>a、b、c、d 分别是原图、probability map、无监督的 threshold map、有监督的 threshold map</li><li>c 图表明即使没有对 threshold map 监督，其结果也会表现出突出显示文本边界区域。这表明如果加入类似边界的监督，以提供更好的指导，d 图的结果证明了这一点</li></ul><h1 id="DB-的标签生成过程？"><a href="#DB-的标签生成过程？" class="headerlink" title="DB 的标签生成过程？"></a>DB 的标签生成过程？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091635472.png" alt=""></li><li><strong>probability map</strong>：使用 Vatti clipping algorithm 将 G 缩减到 Gs（蓝线内部），A 是面积，r 是 shrink ratio，设置为0.4，L 是周长 <script type="math/tex">D=\dfrac{A(1-r^2)}{L}</script></li><li><strong>threshold map</strong>：使用生成 probability map 一样的方法，向外进行扩张，得到绿线和蓝线中间的区域，根据到红线的距离制作标签</li><li><strong>binary map</strong>：蓝色标注线以内</li></ul><p>总结：以上 3 个标签的值范围</p><div class="table-container"><table><thead><tr><th>-</th><th>蓝线以内</th><th>蓝蓝绿之间</th><th>其他</th></tr></thead><tbody><tr><td>probability map</td><td>1</td><td>0</td><td>0</td></tr><tr><td>threshold map</td><td>0.3</td><td>越靠近红线 0.7，越远离红线 0.</td><td>0.3</td></tr><tr><td>binary map</td><td>1</td><td>0</td><td>0</td></tr></tbody></table></div><p><strong><mark style="background: #FF5582A6;">从上面标签制作可知，DB 没有直接去学习文本的边缘（图红线），而是去学习比文本边缘更小的区域 (图绿线)，我觉得这点是除了”可微二值化模块”外，尤其需要关注的地方。这里说一下自己的理解</mark></strong></p><h1 id="DB-为什么不直接学习文本外轮廓，而是学习轮廓缩小的轮廓？"><a href="#DB-为什么不直接学习文本外轮廓，而是学习轮廓缩小的轮廓？" class="headerlink" title="DB 为什么不直接学习文本外轮廓，而是学习轮廓缩小的轮廓？"></a>DB 为什么不直接学习文本外轮廓，而是学习轮廓缩小的轮廓？</h1><ul><li><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091649041.png" alt="Drawing 2023-04-09 15.43.02.excalidraw"></p></li><li><p>上图是两种学习路线下的 probability map、threshold map 及他们学习的 binary map，其中红线是直接学习文本边缘（下文称直觉模式），绿线学习文本边缘小一圈的轮廓（下文称 DB 模式）</p></li><li><strong>观察 probability map</strong>，“直觉模式”比 DB 模式范围更大，这对极度弯曲的小文本是不友好的，可以想象文本在 C2特征已经辨别不出弯曲，更小的学习区域可以有更强的能力</li><li><strong>观察 threshold map</strong>，因为文本行占据了图片大部分区域，所以“直觉模式”主要优化背景到 0.7， threshold map 计算 L1 损失，相比较 DB 模式大部分优化背景到 0.3，“直觉模式”更难优化</li><li><strong>观察 binary map</strong>：除了和优化 threshold map 同样的问题外，由于“直觉模式”对文本行内、外的梯度大小一样，说明两个区域优化权重一样。而 DB 模式内部梯度比外部梯度更大，相当于增大正样本的梯度权重</li><li><strong>总结</strong>：“直觉模式”比 DB 模式更难优化，而且 DB 模式对弯曲小文本性能更好</li></ul><h1 id="DB-的损失函数？"><a href="#DB-的损失函数？" class="headerlink" title="DB 的损失函数？"></a>DB 的损失函数？</h1><ul><li>$L_s$ 是 probability map 的 loss，$L_b$ 是 binary map 的 loss，$L_t$ 是 threshold map 的 loss，$\alpha$ 和 $\beta$ 设置为1和10，$L_s$ 和 $L_b$ 使用交差熵计算损失<script type="math/tex; mode=display">L=L_s+\alpha\times L_b+\beta\times L_t</script></li><li>$S_l$ 表示使用 OHEM 进行采样，正负样本的比例为1：3, $L_t$ 使用 L 1 loss，$R_d$ 表示绿线内的区域，<script type="math/tex; mode=display">L_t=\sum_{i\in R_d}|y_i^*-x_i^*|</script></li></ul><h1 id="DB-如何解析输出的？"><a href="#DB-如何解析输出的？" class="headerlink" title="DB 如何解析输出的？"></a>DB 如何解析输出的？</h1><ul><li>在推理阶段，可以使用 binary map 或者 probability map</li><li><strong>使用 binary map</strong>：需要 probability map+threshold map 两个分支计算得到，其结果就是文本实例</li><li><strong>使用 probability map</strong>：不需要 threshold map、binary map 分支，直接按照 Vatti clipping algorithm 公式还原回去即可，即1)使用0.3的阈值进行二值化；2)将 pixel 连接成不同的文本实例；3)将文本实例进行扩张，得到最终的文本框<script type="math/tex; mode=display"> D^{'}=\dfrac{A^{'}(1-r^{'})}{L^{'}}</script></li><li>使用第二种方法，网络计算更少，论文使用第二种方法</li></ul><h1 id="DB-的网络结构？-1"><a href="#DB-的网络结构？-1" class="headerlink" title="DB++ 的网络结构？"></a>DB++ 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091649944.png" alt=""></li><li><strong>BackBone</strong>：可以使用类似 resnet 下采样 5 次，得到 5 个层次的特征</li><li><strong>ASF 模块</strong>：ASF 特征融合模块其实就是 FPN，只不过在此基础上增加Spatial Attention</li><li><strong>DB 模块</strong>：和 DB 一样，以 probability map 减去 threshold map (T) 差值，输入到 DB 模块进行可微的二值化学习</li></ul><h1 id="DB-的自适应多尺度特征融合模块-ASF-？"><a href="#DB-的自适应多尺度特征融合模块-ASF-？" class="headerlink" title="DB++的自适应多尺度特征融合模块 ASF ？"></a>DB++的自适应多尺度特征融合模块 ASF ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091650873.png" alt=""></li><li><strong>输入输出</strong>：输入是 BackBone 4 个层次的特征，输出是经过加权的特征</li><li><strong>Spatial Attention</strong>：对特征加空间注意力，使用空间（沿通道方向）进行池化，得到注意力矩阵 $1\times H\times W$</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304091513112.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文介绍两个文本检测模型： DB 及它的升级版本 DB++，主要的原理就是将“二值化”的过程做成网络可学习、可微分的一个模块，然后向网络中插入这个模块自适应去学习二值化阈值，最终预测文字区域的一个核心部分，然后再通过公式放大这个核心部分，得到目标区域&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文本检测" scheme="https://shaogui.life/tags/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>CNN之结构重参数化</title>
    <link href="https://shaogui.life/2023/04/07/CNN%E4%B9%8B%E7%BB%93%E6%9E%84%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96/"/>
    <id>https://shaogui.life/2023/04/07/CNN%E4%B9%8B%E7%BB%93%E6%9E%84%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96/</id>
    <published>2023-04-07T04:40:44.000Z</published>
    <updated>2023-05-20T04:05:40.772Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是结构重参数化？"><a href="#什么是结构重参数化？" class="headerlink" title="什么是结构重参数化？"></a>什么是结构重参数化？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005953.png" alt=""></li><li>我们知道模型要变好，就必须构建得更加复杂，但是这来这带来一个坏处，就是模型部署的耗时会增长，这两者是相互矛盾的，<strong>结构从参数化</strong>就是两者都可以做到，在训练的时候，通过复杂的神经网络去训练，提升模型的性能，但是在推理的时候，我通过对模型结构的重参数化生成了一个更加精简的结构，使推理的时候速度更快</li><li><strong>RepVGG 的结构重参数化过程</strong>：上图是左边是训练时的卷积网络，右边通过对结构进行重参数化，得到一个只有 1 个分支的结构，因此可以做到训练时提升性能，推理时提升速度</li><li><strong>结构从参数化的基本原理</strong>：<strong>卷积的可加性</strong>，对于同一个输入，只要其扫描频率一致（相同的通道数、kernel size、stride、padding），其卷积可过程可以融合。如下公式 1 是一个实数乘特征图和乘卷积是等效的，公式 2卷积核 F1 与 F2 可以被融合为 1 个卷积，卷积核为 (F1+F2) </li><li><script type="math/tex; mode=display">\begin{matrix}I\otimes(pF)=p(I\otimes F) \quad&(1)\\I\otimes F^{(1)}+I\otimes F^{(2)}=I\otimes(F^{(1)}+F^{(1)})\quad&(2)\end{matrix}</script></li></ul><h1 id="ACNet-的网络结构？"><a href="#ACNet-的网络结构？" class="headerlink" title="ACNet 的网络结构？"></a>ACNet 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005292.png" alt=""></li><li>上图左边展示 3 个分支的卷积融合为一个等效卷积的过程；右边是卷积融合的过程，主要包括融合 BN (BN fusion) 和融合分支 (branch fusion)两个步骤</li><li><strong>融合 BN (BN fusion)</strong> ：所有 BN 对输入操作一样，不改变输入分辨率，所以利用卷积的线性可加性，将 BN 的过程融合进卷积</li><li><strong>融合分支 (branch fusion)</strong>：同样利用卷积的线性可加性，将多分支的卷积融合为 1个卷积</li></ul><h1 id="RepVGG-如何进行“结构重参数化”？"><a href="#RepVGG-如何进行“结构重参数化”？" class="headerlink" title="RepVGG 如何进行“结构重参数化”？"></a>RepVGG 如何进行“结构重参数化”？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005549.png" alt=""></li><li>1.<strong>卷积层参数</strong>：上图是 REP-VGG 块的结构重参数化过程。为了易于可视化，我们假设 C2 = C1 = 2，因此3×3层具有四个3×3矩阵，而1×1层的核为2×2矩阵</li><li>2.<strong>BN 层参数</strong>：(1)可知 BN 层为每个通道的数据进行规范化，每个通道需要 4 个参数 $\:\mu,\:\sigma,\:\gamma,\:\beta$，输入通道 2 个则有 8 个参数；(2) 当 $\:\mu,\:\sigma,\:\gamma,\:\beta$ 均为 0 时，规范化后的数据还是原来的值，这可以用于模拟 identity 路径</li><li>3.<strong>融合卷积与 BN 层</strong>：(1)最难理解的是虚线红框部分，由原来的 $2\times 1 \times 1\times 2$ 变为 $2\times 3 \times 3\times 2$ ，也就是单个卷积核由 $1\times 1$ 变为 $3\times 3$，这是通过在 $1\times 1$ 四周补 0 做到的，因为补 0 后得到的卷积和是不变的；(2) 类似 [[ACNet#^udwpgu|ACNet的网络结构]]的过程，$\:\mu,\:\sigma,\:\gamma,\:\beta$ 的部分参数用于重构卷积核的值，部分参数组合成卷积的偏置值 $b$，并且每个通道 1 个值 <img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005037.png" alt=""></li><li>4.<strong>利用卷积的可加性，融合多路径</strong>：对应同 size 卷积核的，可以利用卷积的可加性，将卷积融合，具体来说是卷积核矩阵对应相加，偏置值对应相加</li></ul><h1 id="DBB-的网络结构？"><a href="#DBB-的网络结构？" class="headerlink" title="DBB 的网络结构？"></a>DBB 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082005802.png" alt=""></li><li>参考 [[GoogleNetv1]] 的 Inception block 的概念，结合结构重参数划的理论，设计了 DBB block</li><li>每个 DBB block 包含 4 个并行的路径，推理时融合成 1 个路径</li></ul><h1 id="DBB-的-6-种模块可以等价转为单个卷积？"><a href="#DBB-的-6-种模块可以等价转为单个卷积？" class="headerlink" title="DBB 的 6 种模块可以等价转为单个卷积？"></a>DBB 的 6 种模块可以等价转为单个卷积？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/202304082006464.png" alt=""></li><li><ol><li>Conv-BN 合并：经典的卷积层融合 BN 层的结构</li></ol></li><li><ol><li>并行合并：参考ACNet的网络结构，卷积核可线性相加</li></ol></li><li><ol><li>串行合并：参考ACNet的网络结构，卷积核可线性相加</li></ol></li><li><ol><li>并行拼接：参考ACNet的网络结构，卷积核 kernel size 保持不变，数量是两个分支相加</li></ol></li><li><ol><li>平均池化转换：平均池化很像卷积核的过程，只不过是求和后加平均而已，直接对卷积核的值除 KxK，后面得到的卷积和就是 AVG 后的值了</li></ol></li><li><ol><li>多尺度卷积合并：参考ACNet的网络结构，同一将卷积核扩充为 KxK，再进行融合</li></ol></li></ul>]]></content>
    
    
    <summary type="html">结构重参数化的原理</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="CNN" scheme="https://shaogui.life/tags/CNN/"/>
    
    <category term="结构重参数化" scheme="https://shaogui.life/tags/%E7%BB%93%E6%9E%84%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>人工神经网络(ANN)的理解</title>
    <link href="https://shaogui.life/2023/03/01/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(ANN)%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://shaogui.life/2023/03/01/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(ANN)%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2023-03-01T02:23:01.000Z</published>
    <updated>2023-05-20T04:32:47.236Z</updated>
    
    <content type="html"><![CDATA[<p>本文用于解释最基础的神经网络-感知机的原理，它是后续 CNN、RNN 的基础，CNN 只是在感知机的概念加入“局部连接”的思想，每个局部执行的还是加权和；RNN 只是给每个隐藏层加一个隐状态</p><p>本文按照：感知机-&gt;多层感知机-&gt;全连接层-&gt;人工神经网络的步骤去理解 Linear 层</p><a id="more"></a><h1 id="什么是感知机-Perceptron-？"><a href="#什么是感知机-Perceptron-？" class="headerlink" title="什么是感知机 (Perceptron)？"></a>什么是感知机 (Perceptron)？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-26%2010.55.05.excalidraw.png" alt="Drawing 2023-03-26 10.55.05.excalidraw"></li><li><strong>一种用于单类别分类监督学习的算法</strong>，输入一组特征向量，然后通过一组等长权重计算线性加权和，最后通过判别函数输出结果。如果使用以下判别函数，则是一个二元分类器 <script type="math/tex; mode=display">f(x)=\operatorname{sign}(w * x+b)=\left\{\begin{array}{ll} +1 & x \geq 0 \\ -1 & x<0 \end{array}\right.</script></li><li>感知机模拟的是人的神经细胞，单个神经细胞有 2 种状态，激活与不激活，当信号总量超过某个阈值时，该神经细胞处于激活状态，否则不激活，激活时向其他连接的神经元传递信息</li></ul><p>一个感知机包含等于输入数据长度的权重 w 和 1 个偏置 b（因为输出是 1），所谓感知，就是利用权重对所有输入进行加权和操作。接下来同时使用多个感知机去感知输入，就构成“单层感知机 (SLP)”</p><h1 id="什么是单层感知机-SLP"><a href="#什么是单层感知机-SLP" class="headerlink" title="什么是单层感知机 (SLP)?"></a>什么是单层感知机 (SLP)?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-26%2010.55.05.excalidraw.png" alt="Drawing 2023-03-26 10.55.05.excalidraw"></li><li>所谓单层感知机，就是<strong>由感知机组成的一层网络，没有多层感知机的 hidden layer，每个感知机连接都连接所有输入</strong>，注意：感知机本身可以理解为只有 1 个节点的单层感知机</li></ul><p>除了在单层使用多个感知机外，还可以使用多层感知机，每层感知机的输入是前一层前一层感知机的输出，由此出现“多层感知机 (MLP)”</p><h1 id="什么是多层感知机-MLP"><a href="#什么是多层感知机-MLP" class="headerlink" title="什么是多层感知机 (MLP)?"></a>什么是多层感知机 (MLP)?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-26%2010.55.05.excalidraw.png" alt="Drawing 2023-03-26 10.55.05.excalidraw"></li><li><strong>将多个单层感知机堆叠起来，其中每个感知机机与所有输入或者所有感知机输出连接，输入层与输出层之间的所有层是隐藏层</strong></li><li><p>MLP 至少由 3 层感知机组成，即<strong>输入层、隐藏层、输出层</strong></p></li><li><p>感知机、单层感知机、多层感知机的区别？</p><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(ANN" alt="人工神经网络(ANN)-20230408150310">-20230408150310.png)</li><li><strong>感知机 (Perceptron)</strong>：左图是一个 2 输入的感知机模型，经过参数加权累加和后，其值在一个<strong>平面</strong>上</li><li><strong>单层感知机 (SLP)</strong>：中图是一个 2 输入、包含 3 个输出节点的单层感知机，经过参数加权累加和后输出，输出是超平面</li><li><strong>多层感知机 (MLP)</strong>：多个单层感知机组成，至少包括 1 层隐藏层，输出结果是多个多个<strong>超平面</strong>的组合</li><li><strong>感知机无论叠加多少层，只要没使用<mark style="background: #FF5582A6;">非线性激活函数</mark>，其结果都是线性可分的，无法处理非线性问题</strong></li></ul></li></ul><p>但是使用任意个、任意层感知机都是线性函数 $Wx+b$ 的线性叠加，其处理的问题都是线性可分的，但是对于线性不可分问题无法解决</p><h1 id="什么是线性可分？"><a href="#什么是线性可分？" class="headerlink" title="什么是线性可分？"></a>什么是线性可分？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-26%2014.23.58.excalidraw.svg" alt="Drawing 2023-03-26 14.23.58.excalidraw"></li><li><strong>线性可分</strong>：计算所有样本的凸包，如果凸包内值包含 1 个类别，就是线性可分，本质就是数据可以被有限个超平面区分，在 2D 数据上就是被有限个直线做划分</li><li><strong>线性不可分</strong>：计算所有样本的凸包，如果凸包内值包含 2 个类别以上，则是非线性可分</li></ul><p>日常构建网络时，都是使用 <code>torch.Linear</code> 去构建全连接层，它和感知机是什么关系呢，实际上它是确定输入维度、输出维度的单层感知机</p><h1 id="什么是全连接层-Fully-Connected-Layer"><a href="#什么是全连接层-Fully-Connected-Layer" class="headerlink" title="什么是全连接层 (Fully Connected Layer)?"></a>什么是全连接层 (Fully Connected Layer)?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-03-26%2015.29.48.excalidraw.svg" alt="Drawing 2023-03-26 15.29.48.excalidraw"></li><li>其实<strong>就是单层的感知机</strong>，任意一个感知机都与所有输入连接形成，定义时需要指定输入数据维度和感知机数量，其中感知机的数量等于输出的维度</li><li><strong>参数量</strong>：每个感知机都与输入一一相连，加上偏置，共用参数 $\mathrm{Param}<em>{\text {linear }}=C</em>{\text {in }} \times C<em>{\text {out }}+C</em>{\text {out }}$</li><li><strong>计算量</strong>：全连接层的<strong>每个输出</strong>需要经过 $C<em>{in}$ 的乘法和加法运算，没考虑 bias 需要减1，$F L O P</em>{\text {linear }}=(2 \times C<em>{\text {in }}) \times C</em>{\text {out }} \times Batchsize$</li><li>Pytorch 可以使用接口快速定义该层<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=torch.randn([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">fc = torch.nn.Linear(in_features = <span class="number">2</span>, out_features = <span class="number">3</span>)</span><br><span class="line">y=fc(x) // y=[<span class="number">1</span>,<span class="number">3</span>]</span><br></pre></td></tr></table></figure>单层感知机加上激活函数，就可以构建人工神经网络 (ANN) </li></ul><h1 id="什么是人工神经网络-ANN-？"><a href="#什么是人工神经网络-ANN-？" class="headerlink" title="什么是人工神经网络 (ANN) ？"></a>什么是人工神经网络 (ANN) ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/MenuqjAChEyAwrQdOWJoWi4FHpKQ551d1d2kcUoW7jknzv9F_i4-MHztszxy1H4fPSoqPKdjNO51C3mZKtlppVQljk-iYKTbFmNPxm19CYwxSlf0o6oMtwmI4Uq1ma2p.gif" alt=""></li><li>ANN 其实就是在多层感知器的基础上引入”非线性”激活函数，使得 ANN 可以模拟函数逼近器的作用，用于学习数据特征</li><li>缺点：1）随着图像大小的增加，可训练参数增多；2）会丢失图像的空间特征；3）处理序列数据无法记忆历史信息</li><li>Pytorch 可以堆叠多层的”FC+relu”层实现<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=torch.randn([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">fc = torch.nn.Linear(in_features = <span class="number">2</span>, out_features = <span class="number">3</span>)</span><br><span class="line">y=fc(x) // y=[<span class="number">1</span>,<span class="number">3</span>]</span><br><span class="line">y=torch.nn.functional.relu(y)</span><br></pre></td></tr></table></figure></li></ul><p>参考：</p><ol><li><a href="https://luweikxy.gitbook.io/machine-learning-notes/artificial-neural-network#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%9C%E7%94%A8">ANN人工神经网络 - machine-learning-notes</a></li><li><a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multilayer perceptron - Wikipedia</a></li><li><a href="https://www.cnblogs.com/lgdblog/p/6858832.html">线性可分 与线性不可分 - Amazing_Man - 博客园</a></li><li><a href="https://www.zhihu.com/question/593941226">transformer为什么有利于并行计算？ - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文用于解释最基础的神经网络-感知机的原理，它是后续 CNN、RNN 的基础，CNN 只是在感知机的概念加入“局部连接”的思想，每个局部执行的还是加权和；RNN 只是给每个隐藏层加一个隐状态&lt;/p&gt;
&lt;p&gt;本文按照：感知机-&amp;gt;多层感知机-&amp;gt;全连接层-&amp;gt;人工神经网络的步骤去理解 Linear 层&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="ANN" scheme="https://shaogui.life/tags/ANN/"/>
    
    <category term="Linear" scheme="https://shaogui.life/tags/Linear/"/>
    
  </entry>
  
  <entry>
    <title>CILP：Learning Transferable Visual Models From Natural Language Supervision</title>
    <link href="https://shaogui.life/2022/12/20/CILP%EF%BC%9ALearning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/"/>
    <id>https://shaogui.life/2022/12/20/CILP%EF%BC%9ALearning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/</id>
    <published>2022-12-20T11:33:45.000Z</published>
    <updated>2023-05-20T03:04:08.703Z</updated>
    
    <content type="html"><![CDATA[<p>CLIP 通过<strong>文本-图像对</strong>实现对模型预训练，上图是是 3 个文本-图像对，每个文本-图像对由一段文本+一张图片表示，文本描述了这个图片的内容（对象类别），CLIP 模型收集<strong>4亿（400 million）</strong>个**文本-图像对</p><a id="more"></a><h1 id="什么是-CILP-？"><a href="#什么是-CILP-？" class="headerlink" title="什么是 CILP ？"></a>什么是 CILP ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/CILP-20230510223614.png" alt=""></li><li><strong>Contrastive Language-Image Pre-training (CLIP)</strong> 是一个联通文本和图像的模型，比如输入图片和提示文本，模型输出图片类别</li><li>CLIP 通过<strong>文本-图像对</strong>实现对模型预训练，上图是是 3 个文本-图像对，每个文本-图像对由一段文本+一张图片表示，文本描述了这个图片的内容（对象类别），CLIP 模型收集<strong>4亿（400 million）</strong>个**文本-图像对</li></ul><h1 id="CILP-的网络结构？"><a href="#CILP-的网络结构？" class="headerlink" title="CILP 的网络结构？"></a>CILP 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/CILP-20230510215531.png" alt=""></li><li>CLIP 模型包含两部分，即<strong>文本编码器 (Text Encoder)</strong> 和<strong>图像编辑器 (Image Encoder)</strong>，Text Encoder 选择的是 Text Transformer 模型；Image Encoder 选择了两种模型，一是基于 CNN 的 ResNet（对比了不同层数的 ResNet），二是基于 Transformer 的 ViT</li><li><strong>编码器作用</strong>：假设一次输入 N 个文本对，N 个文本首先经过<strong>文本编码器 (Text Encoder)</strong> ，输出 [$T_1,T_2,T_3,…,T_N$]，每个文本的输出是长度为 $d_t$ 的向量，对应的 N 个图片经过<strong>图像编辑器 (Image Encoder)</strong>，输出[$I_1,I_2,I_3,…,I_N$]，每张图片输出也是长度为 $d_t$ 的向量</li><li><strong>自监督训练</strong>：得到[$T_1,T_2,T_3,…,T_N$]和[$I_1,I_2,I_3,…,I_N$]两两组合构成一个矩阵，其中 $T_i$ 与 $I_i$ 匹配，否则不匹配，将匹配的文本-图片对标记为正样本，共计 N 个，不匹配的文本-图像对标记为负样本，共计 N^2-N 个。通过正负样本可训练Text Encoder和Image Encoder</li><li><strong>损失函数</strong>：对于每个文本-图片对的输出，其都是长度为 $d_i$ 的向量，计算损失时通过余弦相似度计算损失即可，对于匹配的文本-图片对，其损失越小越好，对于不匹配的文本图片对，其损失越大越好，即 <script type="math/tex; mode=display">min(\sum_{i=1}^N\sum_{j=1}^N(I_i\cdot T_j)_{(i\neq j)}-\sum_{i=1}^N(I_i\cdot T_i))</script></li></ul><h1 id="文本编码器和图像编码器为什么只输出一维特征"><a href="#文本编码器和图像编码器为什么只输出一维特征" class="headerlink" title="文本编码器和图像编码器为什么只输出一维特征?"></a>文本编码器和图像编码器为什么只输出一维特征?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/CILP-20230510222400.png" alt=""></li><li><strong>编码器输出</strong>：对于每个文本-图片对，正常 N 个长度为 S 的文本输入 transformer，其输出是 (N, S, di)，同理图片输出应该是 (N, S’, dt)，如果只输出 (N, di)或 (N, dt)，那就是类似 Vit 的情况，增加一个 class token 汇总所有 tokens 的信息，或者平均所有的 S 作为输出；</li><li><strong>输出映射</strong>：即使文本编码器输出 (N, di)，图片编码器输出 (N, dt)，最后一维还是长度不一样的，此时分别学习一个 W_i（di, de）、W_t（dt, de）的嵌入，与前面两个输出点乘都得到长度为 (n, de)的输出，然后才能计算模型输出的余弦相似度损失</li><li>输出映射相当于 transformer 的 decoder 的输入，可以认为是文本或图片的 quies 查询向量，找出有用特征计算损失</li></ul><h1 id="CILP-的-zero-shot-分类？"><a href="#CILP-的-zero-shot-分类？" class="headerlink" title="CILP 的 zero-shot 分类？"></a>CILP 的 zero-shot 分类？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/CILP-20230510222948.png" alt=""></li><li><strong>生成类别特征</strong>：根据所迁移的数据集将所有类别转换为文本。这里以 Imagenet 有1000类为例，我们得到了1000个文本：<code>A photo of &#123;label&#125;</code>。我们将这1000个文本全部输入 Text Encoder 中，得到1000个编码后的向量 $T_i(i=1,2,…,N)(N=1000)$ , 这被视作文本特征</li><li><strong>生成图片特征</strong>：将需要分类的图像（单张图像）输入 Image Encoder 中，得到这张图像编码后的向量 $I_1$</li><li><strong>计算余弦相似度</strong>：将 $I_1$ 与得到的1000个文本特征分别计算余弦相似度。找出1000个相似度中最大的那一个（上图中对应的为 ($T_3$），那么评定要分类的图片与第三个文本标签（dog）最匹配，即可将其分类为狗</li></ul><p>参考：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/521151393">详解CLIP (一) | 打通文本-图像预训练实现ImageNet的zero-shot分类，比肩全监督训练的ResNet50/101 - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;CLIP 通过&lt;strong&gt;文本-图像对&lt;/strong&gt;实现对模型预训练，上图是是 3 个文本-图像对，每个文本-图像对由一段文本+一张图片表示，文本描述了这个图片的内容（对象类别），CLIP 模型收集&lt;strong&gt;4亿（400 million）&lt;/strong&gt;个**文本-图像对&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
    <category term="zero-shot" scheme="https://shaogui.life/tags/zero-shot/"/>
    
    <category term="分类" scheme="https://shaogui.life/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>MAE：Masked Autoencoders Are Scalable Vision Learners</title>
    <link href="https://shaogui.life/2022/11/13/MAE%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners/"/>
    <id>https://shaogui.life/2022/11/13/MAE%EF%BC%9AMasked%20Autoencoders%20Are%20Scalable%20Vision%20Learners/</id>
    <published>2022-11-13T03:43:03.000Z</published>
    <updated>2023-05-20T02:57:27.245Z</updated>
    
    <content type="html"><![CDATA[<p>在深度学习模型中，通常会通过 BackBone 提取特征，这些 BackBone 通常使用大量的人工标注数据训练得到，但是人工标注成本很高，能不能在没有标注数据的情况下，训练得到一个预训练模型呢？MAE 通过自监督方法训练得到 BackBone 模型，MAE 开发了一种非对称编码器-解码器结构，其中的编码器仅对可见的 patch 子集进行操作，而轻量级解码器则从潜在表示和 mask token 重建原始图像。对输入图像的高比例（例如 75%）进行 mask 会产生一项困难且有意义的自监督任务</p><a id="more"></a><h1 id="什么是掩码自编码器-MAE-？"><a href="#什么是掩码自编码器-MAE-？" class="headerlink" title="什么是掩码自编码器 (MAE) ？"></a>什么是掩码自编码器 (MAE) ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Pasted%20image%2020230506134447.png" alt=""></li><li>在深度学习模型中，通常会通过 BackBone 提取特征，这些 BackBone 通常使用大量的人工标注数据训练得到，但是人工标注成本很高，能不能在没有标注数据的情况下，训练得到一个预训练模型呢？</li><li>MAE 通过自监督方法训练得到 BackBone 模型，MAE 开发了一种非对称编码器-解码器结构，其中的编码器仅对可见的 patch 子集进行操作，而轻量级解码器则从潜在表示和 mask token 重建原始图像。对输入图像的高比例（例如 75%）进行 mask 会产生一项困难且有意义的自监督任务</li></ul><h1 id="MAE-的网络结构？"><a href="#MAE-的网络结构？" class="headerlink" title="MAE 的网络结构？"></a>MAE 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Pasted%20image%2020230506134447.png" alt=""></li><li>首先将input image切分为patches，执行mask操作，然后只把可见的patches送入encoder中，再将encoder的输出（latent representations）以及mask tokens作为轻量级decoder的输入，decoder重构整张image</li><li><strong>encoder</strong>：一个标准的 ViT 结构，只把可见的 patches 送入 encoder 中，编码器输出 token 后，结合 mask tokens 组成完整的全套 token (encoder 和 decoder 处理的 token 数量一致)</li><li><strong>decoder</strong>：也是一个 vit 结构，目的是执行图像重建任务，将 mask tokens 和 encoder 的输出作为输入，加上位置编码。decoder 的最后一层是 linear projection，输出通道数量和一个 patch 内的 pixel 数量相同（方便重构），然后再 reshape，重构 image</li><li>MAE 中 decoder 的设计并不重要，因为预训练结束之后，只保留 encoder，decoder 只需要完成预训练时的图像重构任务，encoder 用于下游任务</li></ul><h1 id="MAE-的不同-Mask-ratio-大小对预训练、微调的影响？"><a href="#MAE-的不同-Mask-ratio-大小对预训练、微调的影响？" class="headerlink" title="MAE 的不同 Mask ratio 大小对预训练、微调的影响？"></a>MAE 的不同 Mask ratio 大小对预训练、微调的影响？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/MAE-20230507121754.png" alt=""></li></ul><h1 id="MAE-使用-Mask-token-对预训练、微调的影响？"><a href="#MAE-使用-Mask-token-对预训练、微调的影响？" class="headerlink" title="MAE 使用 Mask token 对预训练、微调的影响？"></a>MAE 使用 Mask token 对预训练、微调的影响？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Pasted%20image%2020230508092748.png" alt=""></li><li>实验表明，encoder 如果接收 mask tokens，performance 甚至会降低14%，因此 encoder 只接收 visible tokens，既能提升性能，又能降低计算量加速训练</li></ul><h1 id="MAE-不同深度、宽度的decoder-对-encoder-输出泛化性的影响？"><a href="#MAE-不同深度、宽度的decoder-对-encoder-输出泛化性的影响？" class="headerlink" title="MAE 不同深度、宽度的decoder 对 encoder 输出泛化性的影响？"></a>MAE 不同深度、宽度的decoder 对 encoder 输出泛化性的影响？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Pasted%20image%2020230508092124.png" alt=""></li><li><strong>深度</strong>：MAE decoder 的深度对 linear probing 影响非常大，可以从65.5提升到73.5，这是由于 MAE 预训练使用像素级重构任务，而 linear probing classifier 执行的是分类任务，两者之间有一个明显的 gap，表明 encoder 输出特征不够抽象。实验显示 8 blocks 的 decoder 使得 encoder 的 <strong>latent representation</strong> 的语义信息最抽象，更适合分类任务</li><li><strong>宽度</strong>：MAE decoder 的宽度也有类似结论，实验结果显示 512 的宽度性能最佳</li></ul><h1 id="MAE-使用不同重建策略对预训练、微调的影响？"><a href="#MAE-使用不同重建策略对预训练、微调的影响？" class="headerlink" title="MAE 使用不同重建策略对预训练、微调的影响？"></a>MAE 使用不同重建策略对预训练、微调的影响？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/MAE-20230507121352.png" alt=""></li></ul><h1 id="MAE-使用不同的目标重建策略？"><a href="#MAE-使用不同的目标重建策略？" class="headerlink" title="MAE 使用不同的目标重建策略？"></a>MAE 使用不同的目标重建策略？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Pasted%20image%2020230508092946.png" alt=""></li><li>MAE 最终目标构建策略使用的是像素级别的重建（pixel），作者还和 BEiT 那种预测 token 的方式以及 PCA 的方式</li><li>可以发现，预测归一化像素值的方式最强，其次，BEiT 那种预测 token 的玩法也不差</li></ul><p>参考：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/528720386">​一文看尽MAE最新进展！恺明的MAE已经提出大半年，目前发展如何？ - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/446761025">MAE(Masked Autoencoders) - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/439554945">别再无聊地吹捧了，一起来动手实现 MAE(Masked Autoencoders Are Scalable Vision Learners) 玩玩吧！ - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;在深度学习模型中，通常会通过 BackBone 提取特征，这些 BackBone 通常使用大量的人工标注数据训练得到，但是人工标注成本很高，能不能在没有标注数据的情况下，训练得到一个预训练模型呢？MAE 通过自监督方法训练得到 BackBone 模型，MAE 开发了一种非对称编码器-解码器结构，其中的编码器仅对可见的 patch 子集进行操作，而轻量级解码器则从潜在表示和 mask token 重建原始图像。对输入图像的高比例（例如 75%）进行 mask 会产生一项困难且有意义的自监督任务&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
    <category term="自监督" scheme="https://shaogui.life/tags/%E8%87%AA%E7%9B%91%E7%9D%A3/"/>
    
  </entry>
  
  <entry>
    <title>SVTR：Scene Text Recognition with a Single Visual Model</title>
    <link href="https://shaogui.life/2022/10/25/SVTR%EF%BC%9AScene%20Text%20Recognition%20with%20a%20Single%20Visual%20Model/"/>
    <id>https://shaogui.life/2022/10/25/SVTR%EF%BC%9AScene%20Text%20Recognition%20with%20a%20Single%20Visual%20Model/</id>
    <published>2022-10-25T04:43:09.000Z</published>
    <updated>2023-05-20T03:35:46.487Z</updated>
    
    <content type="html"><![CDATA[<p>传统的文本识别算法是CNN+RNN，但是由于LSTM的效率较低，很多移动设备对LSTM的加速效果并不好，所以在实际的应用场景中也存在诸多限制，SVTR利用swin transformer替代构建局部和全局混合块，提取多尺度的特征，使得不需要RNN去构建序列依赖也能实现更好的效果</p><a id="more"></a><h1 id="什么是-SVTR-？"><a href="#什么是-SVTR-？" class="headerlink" title="什么是 SVTR ？"></a>什么是 SVTR ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SVTR-20230410205058.png" alt=""></li><li>a-d 代表 4 类文本识别模型，分别是传统 CRNN+CTC、CNN/多头自注意力模型+多头注意力、视觉语言模型、SVTR</li><li>传统的文本识别算法是CNN+RNN，但是由于LSTM的效率较低，很多移动设备对LSTM的加速效果并不好，所以在实际的应用场景中也存在诸多限制，SVTR利用swin transformer替代构建局部和全局混合块，提取多尺度的特征，使得不需要RNN去构建序列依赖也能实现更好的效果</li></ul><h1 id="SVTR-的网络结构？"><a href="#SVTR-的网络结构？" class="headerlink" title="SVTR 的网络结构？"></a>SVTR 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SVTR-20230410205119.png" alt=""></li><li><strong>输入</strong>：输入是已经检测好的文本区域</li><li><strong>Patch Embedding</strong>：对输入图像序列化，看图输出是（H/4, W/4），所以取输入的 4 x 4 区域作为一个 token，但是这里又做个改变，通过两层 CBR 取重叠的 3 x 3 卷积对其进行两次下采样，也可得到输出</li><li><strong>Mixing Block</strong>：对输入的 tokens 加自注意力机制，输入输出均是 $hw\times d_{i-1}$</li><li><strong>mergin</strong>：将 tokens 转为卷积输入的 4 D 形式，并使用卷积对其进行下采样，输入是 $hw\times d<em>{i-1}$，输出是 $\frac {h} {2}\times w\times d</em>{i-1}$</li><li><strong>Combing</strong>：使用”自适应全局平均池化”将高度 H 变为1，然后对每个 tokens 的隐变量变换 (D2-&gt;D3)</li></ul><h1 id="SVTR-的-Patch-Embedding？"><a href="#SVTR-的-Patch-Embedding？" class="headerlink" title="SVTR 的 Patch Embedding？"></a>SVTR 的 Patch Embedding？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SVTR-20230410205206.png" alt=""></li><li>已知 swin 是直接使用一个步长为4的4×4卷积进行无重叠的 patch embedding，而svtr则是使用两个步长为2的3×3卷积进行有重叠的patch embedding（延续的CNN的作风，感受野更大，提取局部信息的表达能力也会比swin的patch embedding要好）</li></ul><h1 id="SVTR-的-Mixing-Block？"><a href="#SVTR-的-Mixing-Block？" class="headerlink" title="SVTR 的 Mixing Block？"></a>SVTR 的 Mixing Block？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SVTR-20230410205244.png" alt=""></li><li>上图 (a)、（b）分别表示 Global Mixing、Local Mixing</li><li><strong>Global Mixing</strong>：类似 transformer ，在全部 tokens 上构建自注意力，首先特征图经过线性变换投影到三个空间，然后 q 矩阵和 k 矩阵的转置进行矩阵乘法、softmax 操作得到 attention 矩阵，最后和 v 矩阵进行矩阵乘法得到输出</li><li><strong>Local Mixing</strong>：类似 swin-transformer，在一个固定窗口建立自注意力，这样可以降低计算成本同时获取更多局部注意力。注意：swin 是通过 reshape 这种类似的方式来进行滑窗，并将不同的窗口累加到通道维度上，而 SVTR 则是直接使用值为0的 mask 操作。SVTR 这种做法和 swin 相比，计算复杂度还是比较高</li></ul><h1 id="SVTR-和-swin-transformer-的区别？"><a href="#SVTR-和-swin-transformer-的区别？" class="headerlink" title="SVTR 和 swin-transformer 的区别？"></a>SVTR 和 swin-transformer 的区别？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SVTR-20230410205649.png" alt=""></li><li>svtr 是一个三级逐步下采样的网络（和 swin transformer 一样，下采样三次），和 CNN 架构一样，由 block + 下采样模块组成</li><li>其 block 模块和普通的 swin 中的 block 模块一致，都是 self-attention + mlp。不同的是，SVTR 中 self-attention 的方式和 swin 的滑动窗口有一定的差异</li></ul><p>参考：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/522545062">SVTR论文学习 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/540738873">《SVTR: Scene Text Recognition with a Single Visual Model》解读 - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;传统的文本识别算法是CNN+RNN，但是由于LSTM的效率较低，很多移动设备对LSTM的加速效果并不好，所以在实际的应用场景中也存在诸多限制，SVTR利用swin transformer替代构建局部和全局混合块，提取多尺度的特征，使得不需要RNN去构建序列依赖也能实现更好的效果&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="OCR" scheme="https://shaogui.life/tags/OCR/"/>
    
    <category term="文字识别" scheme="https://shaogui.life/tags/%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>Swin Transformer：Hierarchical Vision Transformer using Shifted Windows</title>
    <link href="https://shaogui.life/2022/10/11/Swin%20Transformer%EF%BC%9AHierarchical%20Vision%20Transformer%20using%20Shifted%20Windows/"/>
    <id>https://shaogui.life/2022/10/11/Swin%20Transformer%EF%BC%9AHierarchical%20Vision%20Transformer%20using%20Shifted%20Windows/</id>
    <published>2022-10-11T15:50:11.000Z</published>
    <updated>2023-05-20T02:53:52.208Z</updated>
    
    <content type="html"><![CDATA[<p>为解决原始transformer在全局上构建注意力的成本巨大问题，Swin Transformer引入WIndows的概念，在每个Windows内构建全局注意力，使得成本由平方变为线性。同时借鉴CNN的层次特征，设计多层次的transformer block，提取图像的多尺度特征</p><a id="more"></a><h1 id="什么是-swin-trasformer"><a href="#什么是-swin-trasformer" class="headerlink" title="什么是 swin-trasformer?"></a>什么是 swin-trasformer?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/swin-transformer-20230408152156.png" alt="swin-transformer-20230408152156"></li><li>原始的 vit 构建全局注意力，在 patch 数量多的情况下，其构建成本很高，因此 swin-trasformer 引入<strong>locality 思想</strong>，对无重合的 <strong>window 区域内进行 self-attention 计算</strong>，并且为了不同 windows 的交流，设计了<strong>滑窗操作</strong></li><li>同时，借鉴 CNN 的层次化的思想构建层次的 trasformer</li><li>这种层级式的结构不仅非常灵活，可以提供各个尺度的特征信息，它的<strong>计算复杂度是随着图像大小而线性增长</strong>，而不是平方级增长</li></ul><h1 id="swin-trasformer-的结构？"><a href="#swin-trasformer-的结构？" class="headerlink" title="swin-trasformer 的结构？"></a>swin-trasformer 的结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/swin-transformer-20230408152157.png" alt="swin-transformer-20230408152157"></li><li>整个模型采取层次化的设计，一共包含4个 Stage，每个 stage 都会<strong>缩小输入特征图的分辨率</strong>，像 CNN 一样逐层扩大感受野</li><li><strong>Patch Embedding</strong>：将图片切成一个个Patch，并嵌入到Embedding</li><li><strong>Linear Embedding</strong>：将输入 (B, S, 48)转为 (B, S, C)</li><li><strong>Patch Merging</strong>：在每个 Stage 一开始<strong>降低图片分辨率</strong>，输出隐变量长度还是 2C，采用的方法是类似 YOLOv5 的输入，间隔采样 H, W，使得各缩小2倍，此时通道维度会变成原先的4倍，再通过一个全连接层再调整通道维度为 2C</li><li><strong>Swin Transformer Block</strong>：使用 Transformer 的 encoder 部分构建”windows”内所有 patch 的注意力</li></ul><h1 id="swin-trasformer-的-Patch-Merging-模块？"><a href="#swin-trasformer-的-Patch-Merging-模块？" class="headerlink" title="swin-trasformer 的 Patch Merging 模块？"></a>swin-trasformer 的 Patch Merging 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/swin-transformer-20230408152158.png" alt=""></li><li>采用的方法是类似 YOLOv5 的输入，间隔采样 H, W，使得各缩小2倍，此时通道维度会变成原先的4倍，再通过一个全连接层再调整通道维度为 2C</li></ul><h1 id="swin-trasformer-的-Swin-Transformer-Block？"><a href="#swin-trasformer-的-Swin-Transformer-Block？" class="headerlink" title="swin-trasformer 的 Swin Transformer Block？"></a>swin-trasformer 的 Swin Transformer Block？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/swin-transformer-20230408152158-1.png" alt="swin-transformer-20230408152158-1"></li><li>Swin Transformer Block 包含 2 部分，即窗口多头自注意层（window multi-head self-attention, W-MSA）和移位窗口多头自注意层（shifted-window multi-head self-attention, SW-MSA）</li><li><strong>窗口多头自注意层（W-MSA）</strong>：传统的 Transformer 都是基于全局来计算注意力的，因此计算复杂度十分高。而 Swin Transformer 则将注意力的计算限制在每个窗口内，进而减少了计算量<script type="math/tex; mode=display">\begin{array}{l}\hat{\mathbf{z}}^{l}=\mathrm{W-MSA}(\mathrm{LN}(\mathbf{z}^{l-1}))+\mathbf{z}^{l-1}\\ \mathbf{z}^{l}=\mathrm{MLP}(\mathrm{LN}(\mathbf{\hat{z}}^{l}))+\mathbf{\hat{z}}^{l}\end{array}</script></li><li><strong>移位窗口多头自注意层（SW-MSA）</strong>：为了保证<strong>不重叠窗口之间有联系</strong>，采用了<strong>shifted window self-attention</strong>的方式<strong>重新计算一遍窗口偏移之后的自注意力</strong><script type="math/tex; mode=display">\begin{array}{c}\hat{\mathbf{z}}^{l+1}=\mathrm{SW-MSA}(\mathrm{LN}(\mathbf{z}^{l}))+\mathbf{z}^{l}\\ \mathbf{z}^{l+1}=\mathrm{MLP}(\mathrm{LN}(\mathbf{\hat{z}}^{l+1}))+\mathbf{\hat{z}}^{l+1}\end{array}</script></li></ul><h1 id="swin-trasformer-的-W-MSA-模块"><a href="#swin-trasformer-的-W-MSA-模块" class="headerlink" title="swin-trasformer 的 W-MSA 模块?"></a>swin-trasformer 的 W-MSA 模块?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/swin-transformer-20230408152159.png" alt=""></li><li>左侧普通的 Multi-head Self-Attention（MSA）模块，计算 feature map 中的每个像素（或称作 token，patch）所有注意力。右侧 Windows Multi-head Self-Attention（W-MSA）模块，首先将 feature map 按照 MxM（M=2）大小划分成一个个 Windows，然后单独对每个 Windows 内部进行 Self-Attention</li></ul><h1 id="swin-trasformer-的-W-MSA-模块的“相对位置编码”？"><a href="#swin-trasformer-的-W-MSA-模块的“相对位置编码”？" class="headerlink" title="swin-trasformer 的 W-MSA 模块的“相对位置编码”？"></a>swin-trasformer 的 W-MSA 模块的“相对位置编码”？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/swin-transformer-20230408152200.png" alt=""></li><li><p>WindowAttention 与传统的 Attention 主要区别是在原始计算 Attention 的公式中的 Q, K 时<strong>添加一个可学习的相对位置参数 B</strong></p><script type="math/tex; mode=display">Attention(Q,K,V)=SoftMax(QK^T/\sqrt{d}+B)V</script></li><li><p>假设 window_size = 2*2即每个窗口有4个 token ，在计算 self-attention 时，每个 token 都要与所有的 token 计算 QK 值，如图2所示，当位置1的 token 计算 self-attention 时，要计算位置1与位置 (1,2,3,4)的 QK 值，即以位置1的 token 为中心点，中心点位置坐标 (0,0)，其他位置计算与当前位置坐标的偏移量</p></li></ul><h1 id="swin-trasformer-的-SW-MSA-模块？"><a href="#swin-trasformer-的-SW-MSA-模块？" class="headerlink" title="swin-trasformer 的 SW-MSA 模块？"></a>swin-trasformer 的 SW-MSA 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-06%2020.23.58.excalidraw.svg" alt="Drawing 2023-04-06 20.23.58.excalidraw"></li><li>上图首先绘制了 W-MSA 在 2 x 2 的 patch 上构建注意力示意图，然后通过 Shifted Windows 操作，在新的 2 x 2  patch 构建注意力</li><li><strong>Shifted Windows</strong>：W-MSA 在每个色块内构建全局注意力，如[1,2,3,4]，[5,6,7,8]，首先 patch 往左上角移动 M/2 个单位，然后通过下移、右移，得到新的 patch 矩阵</li><li><strong>SW-MSA</strong>：Shifted Windows 完成后，根据原始 patch 是否相邻构建注意力，比如对于第一个窗的[4,7,10,13]在原始 patch 矩阵上相邻，所以构建无 Mask 的 4 x 4 的注意力矩阵，而[8,3,14,9]只能在[8,3]、[14,9]之间构建注意力，得到有 Mask 的4 x 4 的注意力矩阵</li><li>通过移位重新构建注意力，可以让原始不同 Windows 之间得到交流，比如 W-MSA 上[4,7]没有构建注意力，而 SW-MSA 构建了它们之间的注意力，这类似 CNN，网络变深，感受野不断增强</li></ul><h1 id="swin-trasformer-的-SW-MSA-模块的-Mask-生成？"><a href="#swin-trasformer-的-SW-MSA-模块的-Mask-生成？" class="headerlink" title="swin-trasformer 的 SW-MSA 模块的 Mask 生成？"></a>swin-trasformer 的 SW-MSA 模块的 Mask 生成？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-06%2021.43.11.excalidraw.svg" alt="Drawing 2023-04-06 21.43.11.excalidraw"></li><li>经过 SW-MSA 模块时，每个 Windows 内不完全是构建全局注意力，这就需要使用 Mask 去掉那些不需要的位置，总体上 Shifted Windows 得到的 Windows 分为 4 种，每种 Mask 矩阵对应如上</li><li><strong>如何使用 Mask 呢？</strong>即在得到 QK^T 的指之后，将其乘上 Mask，对哪些无需计算注意力的位置赋予无穷小，使得 softmax 后趋向0</li></ul><h1 id="swin-trasformer-与-vit-的区别？"><a href="#swin-trasformer-与-vit-的区别？" class="headerlink" title="swin-trasformer 与 vit 的区别？"></a>swin-trasformer 与 vit 的区别？</h1><ul><li><strong>patch</strong>：swin-trasformer 的大小是 4 x 4，vit 是 16 x 16，不过 swin-trasformer 是指一个窗口内的</li><li><strong>emdedding</strong>：swin-trasformer 可选加，因为在计算 Attention 的时候做了一个<strong>相对位置编码</strong></li><li><strong>cls_token</strong>：swin-trasformer 直接拿所有 token 的平均，作为 cls_token，而不是像 vit 使用单独的位置</li></ul><p>参考资料：</p><ol><li><a href="https://blog.csdn.net/zhe470719/article/details/123395256">【深度学习】论文阅读：（ICCV-2021））Swin Transformer_swint模块,让swin-transformer 的使用变得和cnn一样方便快捷_sky_柘的博客-CSDN博客</a></li><li><a href="https://blog.csdn.net/qq_37541097/article/details/121119988">Swin-Transformer网络结构详解_swin transformer_太阳花的小绿豆的博客-CSDN博客</a></li><li><a href="https://github.com/MaoQiankun97/swin_transformer/tree/main">GitHub - MaoQiankun97/swin_transformer: SwinTransformer pytorch实现</a></li><li><a href="https://zhuanlan.zhihu.com/p/542675669">Swin Transformer中的mask机制 - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;为解决原始transformer在全局上构建注意力的成本巨大问题，Swin Transformer引入WIndows的概念，在每个Windows内构建全局注意力，使得成本由平方变为线性。同时借鉴CNN的层次特征，设计多层次的transformer block，提取图像的多尺度特征&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>PRTR：Pose Recognition with Cascade Transformers</title>
    <link href="https://shaogui.life/2022/09/08/PRTR%EF%BC%9APose%20Recognition%20with%20Cascade%20Transformers/"/>
    <id>https://shaogui.life/2022/09/08/PRTR%EF%BC%9APose%20Recognition%20with%20Cascade%20Transformers/</id>
    <published>2022-09-08T03:33:37.000Z</published>
    <updated>2023-05-20T02:48:02.166Z</updated>
    
    <content type="html"><![CDATA[<p>PRTR 是针对2D Pose Estimation 提出了<strong>基于 cascade transformer 结构的人体姿态估计网络</strong>，该网络首先使用 CNN 提取特征，然后使用 transformer 学习注意力，并最终使用类似 transformer-decoder 的方式逐个输出特征点位置</p><a id="more"></a><h1 id="什么是-PRTR？"><a href="#什么是-PRTR？" class="headerlink" title="什么是 PRTR？"></a>什么是 PRTR？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PRTR-20230408154304.png" alt=""></li><li>PRTR 是针对2D Pose Estimation 提出了<strong>基于 cascade transformer 结构的人体姿态估计网络</strong>，该网络首先使用 CNN 提取特征，然后使用 transformer 学习注意力，并最终使用类似 transformer-decoder 的方式逐个输出特征点位置</li><li>PRTR 是一个两阶段的模型，第一阶段使用 DETR 找到 human 位置，第二阶段对每个人预测关键点</li></ul><h1 id="PRTR-的模型结构？"><a href="#PRTR-的模型结构？" class="headerlink" title="PRTR 的模型结构？"></a>PRTR 的模型结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PRTR-20230408154305.png" alt=""></li><li><strong>Person Detection Transformer</strong>：基于 DETR 的检测方法，用一个 CNN Backbone 提取 RGB feature，然后通过 encoder 编码上下文关系，decoder 预测 bbox，得到 bbox 后，对 original image 进行 crop</li><li><strong>Keypoint Detection Transformer</strong>：得到 crop 后的 image 和对应的 positional encoding 之后，送进 encoder 学习</li></ul><h1 id="PRTR-的”Keypoint-Detection-Transformer”部分如何训练、推理？"><a href="#PRTR-的”Keypoint-Detection-Transformer”部分如何训练、推理？" class="headerlink" title="PRTR 的”Keypoint Detection Transformer”部分如何训练、推理？"></a>PRTR 的”Keypoint Detection Transformer”部分如何训练、推理？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PRTR-20230408154307.png" alt=""></li><li><strong>训练</strong>：初始化<strong>可学习</strong>的 queries，然后训练 transformer，最后输出 N 个序列的预测集合，接着使用<strong>匈牙利匹配算法</strong>计算损失，更新网络</li><li><strong>推理</strong>：根据输入图片及学习到的 queries，得到图片所有关键点的位置</li></ul><h1 id="PRTR-的-end2end-的模型结构？"><a href="#PRTR-的-end2end-的模型结构？" class="headerlink" title="PRTR 的 end2end 的模型结构？"></a>PRTR 的 end2end 的模型结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PRTR-20230408154308.png" alt=""></li><li>PRTR 根据自己的思想设计的另一个端到端的模型，该模型也是 2 阶段的过程，和原始模型区别在于其融合了多尺度特征</li></ul><h1 id="PRTR-的-queries-与关键点的关系？"><a href="#PRTR-的-queries-与关键点的关系？" class="headerlink" title="PRTR 的 queries 与关键点的关系？"></a>PRTR 的 queries 与关键点的关系？</h1><ul><li><strong>queries 与关键点位置</strong>：Keypoint Detection Transformer 的 decoder 使用 100 个 queries，最后输出也是 100 个关键点的位置输出，将这些预测的关键点位置按类别可视化后，通过分析 queries 与类别的关系，可知 queries 输出的关键点一定程度反应其真实位置<img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PRTR-20230408154308-1.png" alt=""></li><li><strong>queries 与关键点类别</strong>：Keypoint Detection Transformer 的 decoder 使用 100 个 queries，最后输出也是 100 个关键点的类别输出，通过分析 queries 与类别的关系，可知特定的 queries 倾向输出特定的关键点<img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/PRTR-20230408154309.png" alt=""></li></ul><p>参考：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/368067142">【CVPR 2021】PRTR：基于transformer的2D Human Pose Estimation - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;PRTR 是针对2D Pose Estimation 提出了&lt;strong&gt;基于 cascade transformer 结构的人体姿态估计网络&lt;/strong&gt;，该网络首先使用 CNN 提取特征，然后使用 transformer 学习注意力，并最终使用类似 transformer-decoder 的方式逐个输出特征点位置&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>SOTR：Segmenting Objects with Transformers</title>
    <link href="https://shaogui.life/2022/08/10/SOTR%EF%BC%9ASegmenting%20Objects%20with%20Transformers/"/>
    <id>https://shaogui.life/2022/08/10/SOTR%EF%BC%9ASegmenting%20Objects%20with%20Transformers/</id>
    <published>2022-08-10T14:16:48.000Z</published>
    <updated>2023-05-20T02:42:58.585Z</updated>
    
    <content type="html"><![CDATA[<p>SOTR 利用 transformer 将实例分割任务简化为 2 个过程，一是通过 transformer 预测每个实例的类别，二是通过多级上采样模块动态生成分割掩码</p><a id="more"></a><h1 id="什么是-SOTR？"><a href="#什么是-SOTR？" class="headerlink" title="什么是 SOTR？"></a>什么是 SOTR？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SOTR-20230408154925.png" alt=""></li><li>SOTR 利用 transformer 将实例分割任务简化为 2 个过程，一是通过 transformer 预测每个实例的类别，二是通过多级上采样模块动态生成分割掩码</li><li>图片经过 FPN 的特征，对其进行序列化后得到 NxN 的序列，经过 transformer 后输出 NxN 个序列的结果，将原图 gt 实例某个序列上，计算损失并更新网络</li></ul><h1 id="SOTR-的模型结构？"><a href="#SOTR-的模型结构？" class="headerlink" title="SOTR 的模型结构？"></a>SOTR 的模型结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SOTR-20230408154925.png" alt=""></li><li><strong>BackBone</strong>：使用 FPN 生成 P2-P6 的多尺度特征</li><li><strong>Transformer</strong>：P2-P6 特征添加添加 Positional Embedding 后，输入 Transformer 进行学习，得到每张图的预测集合</li><li><strong>Multi-Level Upsampling Module</strong>：取出 BackBone 的 P 2、P 3、P 4 和 Transformer 的 P5，一起上采样到 P 2 分辨率进行合并，然后输出</li></ul><h1 id="SOTR-的-Twin-attention-模块？"><a href="#SOTR-的-Twin-attention-模块？" class="headerlink" title="SOTR 的 Twin attention 模块？"></a>SOTR 的 Twin attention 模块？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SOTR-20230408154921.png" alt=""></li><li>为了降低原始 transformer block 的计算成本，SOTR 将生成注意力的过程转为 2 次生成注意力的过程，第一次是生成行注意力，第二次生成列注意力。使得针对 HxW 个 token 生成序列时，复杂度由 O (HWxHW)变为 O (H^2 W+HW^2)</li><li>FFN 部分由 Linear 变为卷积实现</li></ul><h1 id="SOTR-的-Multi-Level-updampling-module？"><a href="#SOTR-的-Multi-Level-updampling-module？" class="headerlink" title="SOTR 的 Multi-Level updampling module？"></a>SOTR 的 Multi-Level updampling module？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/SOTR-20230408154923.png" alt=""></li><li><strong>原始特征</strong>：取出 BackBone 的 P 2、P 3、P 4 和 Transformer 的 P5，一起上采样到 P 2 分辨率进行合并</li><li><strong>动态卷积核</strong>：动态卷积核由 transformer 生成</li></ul><h1 id="SOTR-的样本匹配？"><a href="#SOTR-的样本匹配？" class="headerlink" title="SOTR 的样本匹配？"></a>SOTR 的样本匹配？</h1><ul><li>图片经过 FPN 的特征，对其进行序列化后得到 NxN 的序列，经过 transformer 后输出 NxN 个序列的结果，将原图 gt 实例某个序列上，计算损失并更新网络</li><li>直接使用位置映射样本，是因为 SOTR 只使用 transformer 的 encoder 部分，transformer 不会打乱序列的顺序，只会使用自注意力机制更新自己的隐向量</li></ul><p>参考：</p><ol><li><a href="https://blog.csdn.net/m0_61899108/article/details/121598645">【论文笔记】SOTR: Segmenting Objects with Transformers_m0_61899108的博客-CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/424036708">【分割 Transformer】SOTR: Segmenting Objects with Transformers - 知乎</a></li><li><a href="[SOTR:Segmenting Objects with Transformers [ICCV 2021] | Tianliang](https://www.starlg.cn/2022/05/19/SOTR/">SOTR:Segmenting Objects with Transformers [ICCV 2021] | Tianliang (starlg.cn)</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;SOTR 利用 transformer 将实例分割任务简化为 2 个过程，一是通过 transformer 预测每个实例的类别，二是通过多级上采样模块动态生成分割掩码&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>DETR：End-to-End Object Detection with Transformers</title>
    <link href="https://shaogui.life/2022/07/02/DETR%EF%BC%9AEnd-to-End%20Object%20Detection%20with%20Transformers/"/>
    <id>https://shaogui.life/2022/07/02/DETR%EF%BC%9AEnd-to-End%20Object%20Detection%20with%20Transformers/</id>
    <published>2022-07-01T23:22:34.000Z</published>
    <updated>2023-05-20T02:34:00.141Z</updated>
    
    <content type="html"><![CDATA[<p><strong>将 transformers 运用到了 object detection 领域，取代了现在的模型需要手工设计的工作（非极大值抑制和 anchor generation）</strong>，并且取得了不错的结果。在 object detection 上 DETR 准确率和运行时间上和 Faster RCNN 相当；将模型应用到全景分割任务上，DETR 表现甚至还超过了其他的 baseline</p><a id="more"></a><h1 id="什么是-DETR？"><a href="#什么是-DETR？" class="headerlink" title="什么是 DETR？"></a>什么是 DETR？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/DETR-20230408152356.png" alt="DETR-20230408152356"></li><li><strong>将 transformers 运用到了 object detection 领域，取代了现在的模型需要手工设计的工作（非极大值抑制和 anchor generation）</strong>，并且取得了不错的结果。在 object detection 上 DETR 准确率和运行时间上和 Faster RCNN 相当；将模型应用到全景分割任务上，DETR 表现甚至还超过了其他的 baseline</li><li>图片经过 CNN 学习后，输出 (B, C, H, W)特征，HxW=S 视为 token 的个数得到序列 (B, S, C)，并使用 transformer 学习，然后产生 N (N 可以不等于 S，一般是 100) 个 grid 预测，分别预测其类别与 box，然后使用双边匹配算法（匈牙利算法）匹配预测结果与 gt，计算损失，训练网络</li></ul><h1 id="DETR-的模型结构？"><a href="#DETR-的模型结构？" class="headerlink" title="DETR 的模型结构？"></a>DETR 的模型结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/DETR-20230408152356-1.png" alt="DETR-20230408152356-1"></li><li><strong>CNN</strong>：图像经过 CNN 学习后，得到 $(2048,H/32,W/32)$ 的输出，然后和 position encoding 相加，输入 transformer-encoder</li><li><strong>transformer-encoder</strong>：将 $(H/32,W/32)$ 作为 token 数量，然后使用 transfor 进行学习，输出 $(B,H/32 \times W/32,C)$</li><li><strong>transformer-decoder</strong>：包括两部分输入，来自 transformer-encoder 的输出 $(B,H/32 \times W/32,C)$ 和 N 个 object queries，object queries 是训练时随机初始化，训练完成后直接得到该值。在两个输入下的 transformer-decoder 输出 N 个预测结果</li><li><strong>FFN</strong>：前向计算网络，用于产生所有 token 的类别及 box 预测</li></ul><h1 id="DETR-的-transformer-部分的结构？"><a href="#DETR-的-transformer-部分的结构？" class="headerlink" title="DETR 的 transformer 部分的结构？"></a>DETR 的 transformer 部分的结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/DETR-20230408152357.png" alt="DETR-20230408152357"></li><li>不同于原始的 transformer ，DETR 在以下方面对其进行修改</li><li>1）<strong>positional embeding</strong>： DETR 的只作用于 encoder 的 Q 和 encoder-decoder K，原始 transformer 作用于所有的 Q、K、V</li><li>2）<strong>object queries</strong>：DETR 的 object queries 一次性全部输入 decoder，而原始 transformer 是通过 shifted right 一个一个地移动</li></ul><h1 id="DETR-的-transformer-decoder-上-object-queries-的作用？"><a href="#DETR-的-transformer-decoder-上-object-queries-的作用？" class="headerlink" title="DETR 的 transformer decoder 上 object queries 的作用？"></a>DETR 的 transformer decoder 上 object queries 的作用？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/DETR-20230408152357.png" alt="DETR-20230408152357"></li><li>Object queries 是 N 个 learnable embedding，训练刚开始时可以随机初始化，比如 transformer-encoder 输出是 (B, N’, C)，则 Object queries 生成后得到大小为 (B, N, C)数，相当于用 Object queries 去<strong>查询</strong> transformer-encoder 输出的目标类别和 box，N 一般取 100</li><li>训练时随机初始化 Object queries，训练过程中学习这个 embedding，训练完成后，embedding 确定下来，后续推理直接使用</li></ul><h1 id="DETR-如何进行样本分配？"><a href="#DETR-如何进行样本分配？" class="headerlink" title="DETR 如何进行样本分配？"></a>DETR 如何进行样本分配？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Drawing%202023-04-02%2022.59.03.excalidraw.svg" alt="Drawing 2023-04-02 22.59.03.excalidraw"></li><li>DETR 的 transformer decoder 输出 N 个目标的预测集合，下一步是如何将这 N 个预测结果分配到 gt 目标上去，以便计算损失，驱使网络学习</li><li><strong>假设真实目标有 m 个，DETR 认为样本是预测结果 (N) 和 GT（m+1） 的二分图匹配问题，分配的约束条件是最小化损失</strong>，之所以是 m+1，是因为 N 个预测结果大部分都是背景，所以增加一个背景目标，用于映射不需要计算损失的预测结果。比如上图有 5 个预测，图片上有 3 个目标，其中 2 个目标映射到背景上，其余目标映射到具体 gt 上</li><li>计算 gt 目标 $y<em>i$ 与预测集合 $\hat y</em>{\sigma(i)}$ 的损失，对于那些要学习的预测，获得其对应的预测是目标类别的概率，然后计算框损失和概率损失。这也就是说不仅框要近，类别也要基本一致，是最好的<script type="math/tex; mode=display"> \begin{aligned}& \hat{\sigma}=\underset{\sigma\in\mathfrak{G}_N}{\operatorname{arg}\operatorname*{min}}\sum_i^N\mathcal{L}_{\operatorname{match}}(y_i,\hat{y}_{\sigma(i)})\\\\ & \mathcal{L}_{\mathrm{match}}\left(\mathrm{y}_\mathrm{i},\hat{\mathrm{y}}_{\sigma(\mathrm{i})}\right)=-1_{\left\{\mathrm{c}_\mathrm{i}\neq\mathcal{D}\right\}}\hat{\mathrm{p}}_{\sigma(\mathrm{i})}\left(\mathrm{c}_\mathrm{i}\right)+1_{\left\{\mathrm{c}_\mathrm{i}\neq\mathcal{D}\right\}}\mathcal{L}_{\mathrm{box}}\left(\mathrm{b}_\mathrm{i},\hat{\mathrm{b}}_{\sigma(\mathrm{i})}\right)\end{aligned}</script></li></ul><h1 id="DETR-的损失函数？"><a href="#DETR-的损失函数？" class="headerlink" title="DETR 的损失函数？"></a>DETR 的损失函数？</h1><ul><li>样本匹配完成之后，使用以下公式计算损失，其中 $\mathcal{L}<em>{\mathrm{Humerian}}$ 为总损失，可以看到他计算了 N 个输出的类别损失和匹配 gt 输出的 box 损失，其中 box 损失 ${\mathcal{L}}</em>{\mathrm{box}}\left(\mathrm{b}<em>{\mathrm{i}},{\hat{\mathrm{b}}</em>{\hat{\sigma}}}\left(\mathrm{i}\right)\right)$ 使用 GIOU loss+L1 loss 计算框损失，其中 IOU 损失对于 Scale 不敏感，L1 损失对于 Scale 敏感<script type="math/tex; mode=display">\begin{aligned}& \mathcal{L}_{\mathrm{Humerian}}\left(\mathbf{y},\hat{\mathbf{y}}\right)=\sum\limits_{i=1}^{N}\left[-\log\hat{\mathbf{p}}_{\bar{\mathbf{\sigma}}\left(i\right)}\left(\mathbf{c}_{i}\right)+\mathbf{1}_{\left\{\mathbf{c}_{i}\neq\varnothing\right\}}\mathcal{L}_{\mathrm{box}}\left(\mathbf{b}_{i},\hat{\mathbf{b}_{\bar{\sigma}}}\left(\mathbf{i}\right)\right)\right]\\\\ & {\mathcal{L}}_{\mathrm{box}}\left(\mathrm{b}_{\mathrm{i}},{\hat{\mathrm{b}}_{\hat{\sigma}}}\left(\mathrm{i}\right)\right)=\lambda_{\mathrm{ion}}{\mathcal{L}}_{\mathrm{ion}}\left(\mathrm{b}_{\mathrm{i}},{\hat{\mathrm{b}}_{\sigma\left(\mathrm{i}\right)}}\right)+\lambda_{\mathrm{L1}}\left\|\mathrm{b}_{\mathrm{i}}-{\hat{\mathrm{b}}_{\sigma\left(\mathrm{i}\right)}}\right\|_{1}\end{aligned}</script></li></ul><h1 id="DETR-为什么可以不使用-NMS？"><a href="#DETR-为什么可以不使用-NMS？" class="headerlink" title="DETR 为什么可以不使用 NMS？"></a>DETR 为什么可以不使用 NMS？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/DETR-20230408152434.png" alt="DETR-20230408152434"></li><li>上图是每个编码器之后的 AP、AP 50 性能，可以看出当编码层大于 3 后，使用 NMS 和不使用 NMS 的效果是相近的，所以去掉NMS</li></ul><h1 id="DETR-位置编码-positional-encodings-的作用？"><a href="#DETR-位置编码-positional-encodings-的作用？" class="headerlink" title="DETR 位置编码 (positional encodings)的作用？"></a>DETR 位置编码 (positional encodings)的作用？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/DETR-20230408152435.png" alt="DETR-20230408152435"></li><li><strong>生成方式</strong>：DETR的 position encoding 采用原始 Transformer 论文的固定 position encoding，即对于每个 HW 维向量，用不同频率的 sin 函数对高 (H)这个维度生成 d/2 维的 position encoding，用不同频率 cos 函数对宽 (W)这个维度生成 d/2维的 position encoding，然后将两个的 position encoding concat 成 d维的 position encoding</li><li><strong>作用</strong>：看上表可知，对比不增加 positional encodings 的模型，增加了的模型效果更好，还有一种是增加可学习的 position encoding，其效果和 2D 嵌入差不多，因为 2D 嵌入成本更低，所以使用该方法。增加 position encoding 对目标检测这类位置敏感的模型提升较大</li></ul><h1 id="DETR-如何应用到全景分割？"><a href="#DETR-如何应用到全景分割？" class="headerlink" title="DETR 如何应用到全景分割？"></a>DETR 如何应用到全景分割？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/DETR-20230408152435-1.png" alt="DETR-20230408152435-1"></li><li>图片经过 CNN、transformer 后，得到 (N, M, H/32, W/32)的输出，通过 Restnet featrues 模块预测每张图的 Mask 结果，一张图预测一个 Mask，即得到一张图的全景分割</li><li>Restnet featrues 模块主要功能：还原分辨率及降低通道数M</li></ul><h1 id="什么是二分图？"><a href="#什么是二分图？" class="headerlink" title="什么是二分图？"></a>什么是二分图？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/DETR-20230408152436.png" alt="DETR-20230408152436"></li><li>设 G=(V, E)是一个无向图，如果顶点 VV 可分割为两个互不相交的子集 (A, B)，并且图中的每条边（i，j）所关联的两个顶点 i 和 j 分别属于这两个不同的顶点集 (i∈A, j∈B)，则称图 G 为一个二分图。简而言之，就是<strong>顶点集V可分割为两个互不相交的子集，并且图中每条边依附的两个顶点都分属于这两个互不相交的子集，两个子集内的顶点不相邻</strong></li><li>上图只有 1 满足二分图，其余都不是二分图</li></ul><h1 id="什么是二分图匹配？"><a href="#什么是二分图匹配？" class="headerlink" title="什么是二分图匹配？"></a>什么是二分图匹配？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/DETR-20230408152436-1.png" alt="DETR-20230408152436-1"></li><li><strong>匹配</strong>：在图论中，一个“匹配”就是一个“边”的集合，其中任意两条边都没有公共顶点</li><li><strong>最大匹配</strong>：一个图的匹配集合中，所含匹配边数最多的匹配，或者说覆盖的点最多，称为这个图的最大匹配</li><li><strong>完美匹配</strong>：当一个图的匹配覆盖了所有的点，那么它就是一个完美匹配</li><li>上图中的“一个匹配”中就仅是一个普通的匹配，而且很容易发现一条增广路径：<strong>2-B-5-C</strong>，然后就得到了最后的“最大匹配”的结果</li></ul><h1 id="什么是匈牙利算法（KM-Algorithm）？"><a href="#什么是匈牙利算法（KM-Algorithm）？" class="headerlink" title="什么是匈牙利算法（KM Algorithm）？"></a>什么是匈牙利算法（KM Algorithm）？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/DETR-20230408152437.png" alt="DETR-20230408152437"></li><li>二分图匹配常用的算法，是在二分图中寻找增广路，并修改边的匹配情况，如果没有增广路了，那么这张图就达到最大匹配了</li><li><strong>增广路</strong>：若 P 是图 G 中一条连通两个未匹配顶点的路径，并且属于 M 的边和不属于 M 的边 (即已匹配和待匹配的边)在 P 上交替出现，则称 P 为相对于 M 的一条增广路径，上图右边就是增广路径</li></ul><p>参考：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/267156624">用Transformer做object detection：DETR - 知乎</a></li><li><a href="https://blog.csdn.net/baidu_36913330/article/details/120495817">深度学习之目标检测（十一）—DETR详解_木卯_THU的博客-CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/402091571">二分图与最大匹配 - 知乎</a></li><li><a href="https://www.zhihu.com/question/21401775">ACM中二分图匹配主要可以解决哪些问题？ - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;将 transformers 运用到了 object detection 领域，取代了现在的模型需要手工设计的工作（非极大值抑制和 anchor generation）&lt;/strong&gt;，并且取得了不错的结果。在 object detection 上 DETR 准确率和运行时间上和 Faster RCNN 相当；将模型应用到全景分割任务上，DETR 表现甚至还超过了其他的 baseline&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>vit：An Image is Worth 16x16 Words：Transformers for Image Recognition at Scale</title>
    <link href="https://shaogui.life/2022/06/23/vit%EF%BC%9AAn%20Image%20is%20Worth%2016x16%20Words%EF%BC%9ATransformers%20for%20Image%20Recognition%20at%20Scale/"/>
    <id>https://shaogui.life/2022/06/23/vit%EF%BC%9AAn%20Image%20is%20Worth%2016x16%20Words%EF%BC%9ATransformers%20for%20Image%20Recognition%20at%20Scale/</id>
    <published>2022-06-23T13:48:16.000Z</published>
    <updated>2023-05-20T04:04:14.050Z</updated>
    
    <content type="html"><![CDATA[<p>一直以来都是使用卷积来处理图像数据，即使后面提出空洞卷积、特征金字塔来、注意力机制缓解其感受野受限，但是还是使用卷积计算去提取图像特征，<strong>受 NLP 中 transformer 的影响，VIT 将图像拆分为块 (patch)，并将图像数据学习转为图像块序列的学习问题</strong>，这里图像块 (patches) 的处理方式同 NLP 的标记 (tokens) </p><a id="more"></a><h1 id="什么是-vit-vision-in-transformer"><a href="#什么是-vit-vision-in-transformer" class="headerlink" title="什么是 vit (vision in transformer)?"></a>什么是 vit (vision in transformer)?</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/vit-20230408150924.png" alt=""></li><li>一直以来都是使用卷积来处理图像数据，即使后面提出空洞卷积、特征金字塔来、注意力机制缓解其感受野受限，但是还是使用卷积计算去提取图像特征</li><li><strong>受 NLP 中 transformer 的影响，VIT 将图像拆分为块 (patch)，并将图像数据学习转为图像块序列的学习问题</strong>，这里图像块 (patches) 的处理方式同 NLP 的标记 (tokens) </li><li>注意：Transformers 缺乏 CNN 固有的一些归纳偏置 (inductive biases) —— 如平移等效性和局部性 (translation equivariance and locality)，因此在数据量不足时，训练不能很好地泛化</li></ul><h1 id="Vit-如何将-2D-图像转为-transformer-输入？"><a href="#Vit-如何将-2D-图像转为-transformer-输入？" class="headerlink" title="Vit 如何将 2D 图像转为 transformer 输入？"></a>Vit 如何将 2D 图像转为 transformer 输入？</h1><ul><li>tansformer 是 3D 输入的，即 (B, S, L)，分别表示 batch size、sequence size、sequence length，计算机视觉的输入一般是 (B, C, H, W)，从 (B, C, H, W)-&gt; (B, S, L)的过程中，可以认为 B 不变，C 表示 L，关键是如何将 （H，W）转为 S？</li><li>对于 2D 的图像，将其拆分成多个 PxP 的互不重叠区域，则共有 $S=HW/(P\times P)$ 个区域，则 $(B, C, H, W)$ 可被拆分为 $(B, C\times P\times P, HW/(P\times P))$ ，即 $(B, C’, S)$ ，这就可以被 tansformer 接收</li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmbedLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, args</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EmbedLayer, self).__init__()</span><br><span class="line">        self.args = args</span><br><span class="line">        <span class="comment"># 卷积获得图片的所有tokens,每个大小是patch_size</span></span><br><span class="line">        self.conv1 = nn.Conv2d(args.n_channels, args.embed_dim, kernel_size=args.patch_size, stride=args.patch_size) </span><br><span class="line">        self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, args.embed_dim), requires_grad=<span class="literal">True</span>)  <span class="comment"># Cls Token</span></span><br><span class="line">        self.pos_embedding = nn.Parameter(torch.zeros(<span class="number">1</span>, (args.img_size // args.patch_size) ** <span class="number">2</span> + <span class="number">1</span>, args.embed_dim), requires_grad=<span class="literal">True</span>)  <span class="comment"># Positional Embedding，这里是可学习的pos_embedding</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)  <span class="comment"># B C IH IW -&gt; B E IH/P IW/P</span></span><br><span class="line">        x = x.reshape([x.shape[<span class="number">0</span>], self.args.embed_dim, -<span class="number">1</span>])  <span class="comment"># B E IH/P IW/P -&gt; B E S</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># B E S -&gt; B S E</span></span><br><span class="line">        x = torch.cat((torch.repeat_interleave(self.cls_token, x.shape[<span class="number">0</span>], <span class="number">0</span>), x), dim=<span class="number">1</span>) <span class="comment"># 加上cls_token</span></span><br><span class="line">        x = x + self.pos_embedding <span class="comment"># 加上pos_embedding</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h1 id="Vit-的结构？"><a href="#Vit-的结构？" class="headerlink" title="Vit 的结构？"></a>Vit 的结构？</h1></li><li><p><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/vit-20230408150924.png" alt=""></p></li><li><strong>输入</strong>：输入图片 (B, C, H, W)，经过 Patch 对图像分块，得到 $(B, HW/(P\times P), C\times P\times P)$，将其类比为 (B, N, L)，然后增加 Position Embeddings 和 cls_token ，最终 Encoder 输入变为：(B, N+1, L)</li><li><strong>Encoder</strong>：对输入 (B, N+1, L)，使用 transformer 学习 N+1个 patch 的全局注意力，输出是(B, N+1, L)</li><li><strong>Decoder</strong>：从 (B, N+1, L)取出 cls_token，得到 (B, 1, L)，然后使用 Linear 编码该特征，输出 (B, L’)，L’为类别数量，最后使用 softmax 进行分类</li></ul><h1 id="Vit-为什么要加-Cls-token-节点？"><a href="#Vit-为什么要加-Cls-token-节点？" class="headerlink" title="Vit 为什么要加 Cls_token 节点？"></a>Vit 为什么要加 Cls_token 节点？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Vit-20230408150954.png" alt=""></li><li>经过 VIT 的编码器后，得到 N+1 个 Patch 的隐向量，即 (B，N+1，L)，论文使用 2 种方法去获得最后的分类结果</li><li><strong>方法 1</strong>：从训练开始就没有增加cls<em>token，然后对N个L取平均，得到一张图的向量表示，即(B，N，L)-&gt;(GAP)-&gt;$(B，1，\sum</em>{i=1}^N(L_i)/N )=(B,\hat L)$，然后对$(B,\hat L)$进行分类</li><li><strong>方法2</strong>：增加cls_token，然后只取cls_token进行分类</li><li>方法 1 是所有隐向量的线性组合，表达能力弱，方法 2 是网络学习的一部分，更有效，成本更低</li></ul><h1 id="Vit-为什么要使用-Position-Embedding？"><a href="#Vit-为什么要使用-Position-Embedding？" class="headerlink" title="Vit 为什么要使用 Position Embedding？"></a>Vit 为什么要使用 Position Embedding？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Vit-20230408150954-1.png" alt=""></li><li>不同于 CNN，Transformer 需要位置嵌入来编码 patch tokens 的位置信息，这主要是由于 自注意力 的 扰动不变性 (Permutation-invariant)，即打乱 Sequence 中 tokens 的顺序并不会改变结果，若不给模型提供图像块的位置信息，那么模型就需要通过图像块的语义来学习拼图，这就额外增加了学习成本。表现为如果不加位置编码，可能出现全部序列都预测出来，但是位置不正确</li><li>论文比较了4种嵌入编码的方式，效果差不多，因为分类任务对位置信息不敏感，如果是其他的transformer处理的任务，如果不加，效果很差</li></ul><h1 id="Vit-上不同网络位置的-transoformer-block-在注意力上有什么差异？"><a href="#Vit-上不同网络位置的-transoformer-block-在注意力上有什么差异？" class="headerlink" title="Vit 上不同网络位置的 transoformer block 在注意力上有什么差异？"></a>Vit 上不同网络位置的 transoformer block 在注意力上有什么差异？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Vit-20230408150955.png" alt=""></li><li>Mean Attention Distance主要体现自注意力机制的影响范围，比如认定注意力&gt;0.5的两个位置形成注意力，然后计算满足这个阈值的位置平均距离，这就类比于 CNN 的感受野。结果表明：<strong>前面层的 “感受野” 虽然差异很大，但总体相比后面层 “感受野” 较小；而模型后半部分 “感受野” 基本覆盖全局，和 CNN 比较类似，说明 ViT 也最后学习到了类似的范式</strong></li></ul><h1 id="Vit-与-CNN-的区别？"><a href="#Vit-与-CNN-的区别？" class="headerlink" title="Vit 与 CNN 的区别？"></a>Vit 与 CNN 的区别？</h1><ul><li><strong>归纳偏置 (Inductive bias)</strong>：Vision Transformer 的图像特定归纳偏置比 CNN 少得多。在 CNN 中，局部性、二维邻域结构和平移等效性存在于整个模型的每一层中。而在 ViT 中，只有 MLP 层是局部和平移等变的，因为自注意力层都是全局的。二维邻域结构的使用非常谨慎：在模型开始时通过将图像切分成块，并在微调时调整不同分辨率图像的位置嵌入 (如下所述)。此外，初始化时的位置嵌入不携带有关图像块的 2D 位置的信息，图像块之间的所有空间关系都必须从头开始学习</li><li><strong>混合架构 (Hybrid Architecture)</strong>：作为原始图像块的替代方案，输入序列可由 CNN 的特征图构成。在这种混合模型中，图像块嵌入投影被用在 经 CNN 特征提取的块 而非 原始输入图像块。作为一种特殊情况，块的空间尺寸可以为 ，这意味着输入序列是通过 简单地将特征图的空间维度展平并投影到 Transformer 维度 获得的。然后，如上所述添加了分类输入嵌入和位置嵌入，再将三者组成的整体馈入 Transformer 编码器。简单来说，就是先用 CNN 提取图像特征，然后由 CNN 提取的特征图构成图像块嵌入。由于 CNN 已经将图像降采样了</li></ul><p>参考：</p><ol><li><a href="https://blog.csdn.net/qq_39478403/article/details/118704747">https://blog.csdn.net/qq_39478403/article/details/118704747</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;一直以来都是使用卷积来处理图像数据，即使后面提出空洞卷积、特征金字塔来、注意力机制缓解其感受野受限，但是还是使用卷积计算去提取图像特征，&lt;strong&gt;受 NLP 中 transformer 的影响，VIT 将图像拆分为块 (patch)，并将图像数据学习转为图像块序列的学习问题&lt;/strong&gt;，这里图像块 (patches) 的处理方式同 NLP 的标记 (tokens) &lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>SETR：Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers</title>
    <link href="https://shaogui.life/2022/06/20/SETR%EF%BC%9ARethinking%20Semantic%20Segmentation%20from%20a%20Sequence-to-Sequence%20Perspective%20with%20Transformers/"/>
    <id>https://shaogui.life/2022/06/20/SETR%EF%BC%9ARethinking%20Semantic%20Segmentation%20from%20a%20Sequence-to-Sequence%20Perspective%20with%20Transformers/</id>
    <published>2022-06-20T00:42:18.000Z</published>
    <updated>2023-05-20T02:37:07.364Z</updated>
    
    <content type="html"><![CDATA[<p>一直以来，分割都是在 FCN 的基础上搭建 Encoder-Decoder 进行的，基于 CNN 的缺点，虽然有设计方法取增大感受野、引入注意力机制，但还是没有背离这个规则。<strong>SETR 以 transformer 替代 CNN 的 Encoder 部分</strong>，将 2D 图片问题转为序列注意力构建问题，能在保持分辨率不变的情况下进行特征学习，最后使用 CNN 类似的金字塔结构还原分辨率</p><a id="more"></a><h1 id="什么是-SETR？"><a href="#什么是-SETR？" class="headerlink" title="什么是 SETR？"></a>什么是 SETR？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Pasted%20image%2020230401145724.png" alt=""></li><li>一直以来，分割都是在 FCN 的基础上搭建 Encoder-Decoder 进行的，基于 CNN 的缺点，虽然有设计方法取增大感受野、引入注意力机制，但还是没有背离这个规则</li><li><strong>SETR 以 transformer 替代 CNN 的 Encoder 部分</strong>，将 2D 图片问题转为序列注意力构建问题，能在保持分辨率不变的情况下进行特征学习，最后使用 CNN 类似的金字塔结构还原分辨率</li></ul><h1 id="SETR-的模型结构？"><a href="#SETR-的模型结构？" class="headerlink" title="SETR 的模型结构？"></a>SETR 的模型结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Pasted%20image%2020230401145718.png" alt=""></li><li><strong>输入</strong>：图像 (B, C,H, W)，需要对图像进行 Patch 化，然后加入 Position embedding，得到 (B, S, L)</li><li><strong>Encoder</strong>：以 tansformer block 搭建，输入 (B, S, L)，输出也是 (B, S, L)</li><li><strong>Decoder</strong>：将 Encoder 输出 (B, S, L)转为 4D 数据 (B, H/16, W/16, C)，然后使用 3 种分辨率上采样方法验证 Decoder 效果</li></ul><h1 id="SETR-的-Decoder-设计？"><a href="#SETR-的-Decoder-设计？" class="headerlink" title="SETR 的 Decoder 设计？"></a>SETR 的 Decoder 设计？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Pasted%20image%2020230401145650.png" alt=""></li><li>Encoder 输出 (B, H/16, W/16, C)数据， SETR 设计了三种解码器上采样方法还原分辨率，实验证明 PUP 效果更好，最终采样该方法</li><li><strong>最原始上采样 (Naive upsampling)</strong>：通过简单的1x1卷积加上双线性插值来实现图像像素恢复</li><li><strong>渐进式上采样 (Progressive UPsampling, PUP)</strong>：一步到位式的上采样可能会产生大量的噪声，渐进式上采样则可以缓解这种问题。每一次上采样只恢复上一步图像的2倍，这样经过4次操作就可以回复原始图像</li><li><strong>多层次特征加总 (Multi-Level feature Aggregation, MLA)</strong>：这种设计跟特征金字塔网络类似</li></ul><h1 id="SETR-的注意力可视化？"><a href="#SETR-的注意力可视化？" class="headerlink" title="SETR 的注意力可视化？"></a>SETR 的注意力可视化？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/Pasted%20image%2020230401152844.png" alt=""></li><li><strong>第一张图</strong>：可视化 Encoder 的某些 tansformer block 的输出，可以看出在低层的 tansformer block 已经构建全局注意力关系</li><li><strong>第二张图</strong>：可视化一个固定点 (patch)与其他点 (patch)的注意力关系，可以看出，tansformer block 确实构建了目标的注意力关系</li></ul><p>参考：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/348418189">重新思考语义分割范式——SETR - 知乎</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;一直以来，分割都是在 FCN 的基础上搭建 Encoder-Decoder 进行的，基于 CNN 的缺点，虽然有设计方法取增大感受野、引入注意力机制，但还是没有背离这个规则。&lt;strong&gt;SETR 以 transformer 替代 CNN 的 Encoder 部分&lt;/strong&gt;，将 2D 图片问题转为序列注意力构建问题，能在保持分辨率不变的情况下进行特征学习，最后使用 CNN 类似的金字塔结构还原分辨率&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="transformer" scheme="https://shaogui.life/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>Hexo博客速度优化</title>
    <link href="https://shaogui.life/2022/06/15/Hexo%E5%8D%9A%E5%AE%A2%E9%80%9F%E5%BA%A6%E4%BC%98%E5%8C%96/"/>
    <id>https://shaogui.life/2022/06/15/Hexo%E5%8D%9A%E5%AE%A2%E9%80%9F%E5%BA%A6%E4%BC%98%E5%8C%96/</id>
    <published>2022-06-15T13:05:24.609Z</published>
    <updated>2022-06-15T01:53:14.900Z</updated>
    
    <content type="html"><![CDATA[<p>本文对Hexo博客进行访问优化，使得访问速度更快了，主要是安装hexo-neat插件，实现对html、css、js、image等静态资源的高效压缩。通过压缩这些静态资源，可以减少请求的数据量从而达到优化博客访问速度的目的</p><a id="more"></a><h2 id="资源压缩1"><a href="#资源压缩1" class="headerlink" title="资源压缩1"></a>资源压缩<sup><a href="#fn_1" id="reffn_1">1</a></sup></h2><p><strong>安装插件</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-neat --save</span><br></pre></td></tr></table></figure></p><p><strong>配置插件</strong><br>打开博客根目录文件<code>_config.yml</code>，添加以下配置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hexo-neat 压缩</span></span><br><span class="line">neat_enable: <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 压缩html</span></span><br><span class="line">neat_html:</span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">  exclude:</span><br><span class="line"><span class="comment"># 压缩css  </span></span><br><span class="line">neat_css:</span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">  exclude:</span><br><span class="line">    - <span class="string">&#x27;**/*.min.css&#x27;</span></span><br><span class="line"><span class="comment"># 压缩js</span></span><br><span class="line">neat_js:</span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">  mangle: <span class="literal">true</span></span><br><span class="line">  output:</span><br><span class="line">  compress:</span><br><span class="line">  exclude:</span><br><span class="line">    - <span class="string">&#x27;**/*.min.js&#x27;</span></span><br><span class="line">    - <span class="string">&#x27;**/jquery.fancybox.pack.js&#x27;</span></span><br><span class="line">    - <span class="string">&#x27;**/index.js&#x27;</span></span><br></pre></td></tr></table></figure></p><h2 id="图片懒加载2"><a href="#图片懒加载2" class="headerlink" title="图片懒加载2"></a>图片懒加载<sup><a href="#fn_2" id="reffn_2">2</a></sup></h2><p>即文字先出来，图片慢慢出来，显著提高加载速度</p><p><strong>安装插件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-lazyload-image --save</span><br></pre></td></tr></table></figure><p><strong>配置文件</strong></p><p>打开配置文件<code>_config.yml</code>，添加以下配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图片懒加载</span></span><br><span class="line">lazyload:</span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span> </span><br><span class="line">  onlypost: <span class="literal">false</span></span><br><span class="line">  loadingImg: /images/loading.gif <span class="comment">#如果不填写图片则使用默认的图片</span></span><br></pre></td></tr></table></figure><blockquote id="fn_2"><sup>2</sup>. <a href="https://www.cnblogs.com/lfri/p/12221963.html">Hexo-Next提高加载速度</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文对Hexo博客进行访问优化，使得访问速度更快了，主要是安装hexo-neat插件，实现对html、css、js、image等静态资源的高效压缩。通过压缩这些静态资源，可以减少请求的数据量从而达到优化博客访问速度的目的&lt;/p&gt;</summary>
    
    
    
    <category term="软件工具" scheme="https://shaogui.life/categories/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="hexo" scheme="https://shaogui.life/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>YOLACT++：Better Real-time Instance Segmentation</title>
    <link href="https://shaogui.life/2022/04/26/YOLACT++%EF%BC%9ABetter%20Real-time%20Instance%20Segmentation/"/>
    <id>https://shaogui.life/2022/04/26/YOLACT++%EF%BC%9ABetter%20Real-time%20Instance%20Segmentation/</id>
    <published>2022-04-26T10:44:48.000Z</published>
    <updated>2023-05-20T04:23:38.358Z</updated>
    
    <content type="html"><![CDATA[<p> yolactplusplus 通过引入可变形卷积、使用更多的 anchor、重新生成的 Mask scoreing 分支等措施，改进了 yolact 模型</p><a id="more"></a><h1 id="什么是-yolactplusplus-？"><a href="#什么是-yolactplusplus-？" class="headerlink" title="什么是 yolactplusplus ？"></a>什么是 yolactplusplus ？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/yolactplusplus-20230508212450.png" alt=""></li><li>YOLACT 主要是通过两个并行的子网络来实现实例分割的。(1) Prediction Head 分支生成各个 anchor 的类别置信度、位置回归参数以及 mask 的掩码系数；(2) Protonet 分支生成一组原型 mask。然后将原型 mask 和 mask 的掩码系数相乘，从而得到图片中每一个目标物体的 mask</li><li>yolactplusplus 通过引入可变形卷积、使用更多的 anchor、重新生成的 Mask scoreing 分支等措施，改进了 yolact 模型</li></ul><h1 id="yolactplusplus-的网络结构？"><a href="#yolactplusplus-的网络结构？" class="headerlink" title="yolactplusplus 的网络结构？"></a>yolactplusplus 的网络结构？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/yolactplusplus-20230508212450-1.png" alt=""></li><li><strong>Featrue Backbone&amp;Featrue Pyramid</strong>：使用 ResNet101 提取图片特征，并引入可变形卷积，然后使用 FPN 结构进行特征融合</li><li><strong>prototypes</strong>：从 P3 级别的特征生成全局的 prototype mask (138,138,32)，这里固定是 32 个 mask，后续所有实例的 mask 是这 32 个 mask 的线性组合</li><li><strong>Predict Head</strong>：基于 anchor 预测目标的类别、位置和 Mask coefficients，其中 Mask coefficients 是每个 anchor 预测长度为 32 的向量，用于加权 prototypes，得到当前 anchor 的 mask 预测</li><li><strong>corp&amp;Threashold</strong>：根据定位结果和 Mask 预测结果，裁剪目标区域，并使用二值化求得目标的 Mask</li><li><strong>Mask Re-Scoring</strong>：受MS R-CNN的启发，高质量的mask并不一定就对应着高的分类置信度，换句话说，以包围框得分来评价mask好坏并不合理，所以在模型后添加了Mask Re-Scoring分支，该分支使用YOLACT生成的裁剪后的原型mask(未作阈值化)作为输入，输出对应每个类别的GT-mask的IoU</li></ul><h1 id="yolactplusplus-的-Fast-Mask-Re-Scoring-分支？"><a href="#yolactplusplus-的-Fast-Mask-Re-Scoring-分支？" class="headerlink" title="yolactplusplus 的 Fast Mask Re-Scoring 分支？"></a>yolactplusplus 的 Fast Mask Re-Scoring 分支？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/yolactplusplus-20230508212451.png" alt=""></li><li>受MS R-CNN的启发，高质量的mask并不一定就对应着高的分类置信度，换句话说，以包围框得分来评价mask好坏并不合理，所以在模型后添加了Mask Re-Scoring分支，该分支使用YOLACT生成的裁剪后的原型mask(未作阈值化)作为输入，输出对应每个类别的GT-mask的IoU</li></ul><h1 id="yolactplusplus-的-Prediction-Head？"><a href="#yolactplusplus-的-Prediction-Head？" class="headerlink" title="yolactplusplus 的 Prediction Head？"></a>yolactplusplus 的 Prediction Head？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/yolactplusplus-20230508212451-1.png" alt=""></li><li>YOLACT 是 anchor-based 的，yolactplusplus对 anchor 设计进行优化。经过实验，选择在每个 FPN 层上乘3种大小，相当于anchor数量较原来的YOLACT增加了3倍</li></ul><h1 id="yolactplusplus-的可变形卷积？"><a href="#yolactplusplus-的可变形卷积？" class="headerlink" title="yolactplusplus 的可变形卷积？"></a>yolactplusplus 的可变形卷积？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/yolactplusplus-20230508212452.png" alt=""></li><li>参考Deformable ConvNets v2的思路，将ResNet的C3-C5中的各个标准3x3卷积换成3x3可变性卷积，但没有使用堆叠的可变形卷积模块，因为延迟太高</li></ul><h1 id="yolactplusplus-与-yolact-的区别？"><a href="#yolactplusplus-与-yolact-的区别？" class="headerlink" title="yolactplusplus 与 yolact 的区别？"></a>yolactplusplus 与 yolact 的区别？</h1><ul><li><img src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/yolactplusplus-20230508212452-1.png" alt=""></li><li>yolactplusplus 通过引入可变形卷积、使用更多的 anchor、重新生成的 Mask scoreing 分支等措施，改进了 yolact 模型，yolactplusplus 效果越来越好，但是速度变慢了</li></ul><h1 id="yolactplusplus-与-Mask-RCNN-的区别？"><a href="#yolactplusplus-与-Mask-RCNN-的区别？" class="headerlink" title="yolactplusplus 与 Mask RCNN 的区别？"></a>yolactplusplus 与 Mask RCNN 的区别？</h1><ul><li>YOLACT++直接使用全尺寸的 mask 作为 scoring 分支的输入，而 MS R-CNN 使用的是 ROI Align 后的特征再与其经过 mask 预测分支计算后的特征拼接后的组成的特征</li><li>YOLACT++的scoring分支没有使用FC层，这使得分割的速度提高</li></ul><p>参考：</p><ol><li><a href="https://blog.csdn.net/wh8514/article/details/105520870">图像分割之YOLACT &amp; YOLACT++_Vincent8514的博客-CSDN博客</a></li><li><a href="https://hub.baai.ac.cn/view/19447">当前最快的实例分割模型：YOLACT 和 YOLACT++ - 智源社区</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt; yolactplusplus 通过引入可变形卷积、使用更多的 anchor、重新生成的 Mask scoreing 分支等措施，改进了 yolact 模型&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://shaogui.life/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="实例分割" scheme="https://shaogui.life/tags/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
</feed>
