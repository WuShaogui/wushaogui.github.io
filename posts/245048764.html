<!DOCTYPE html><html lang="zh-CN"><head><style type="text/css">.douban-card-block{display:flex;justify-content:center;align-items:center;width:100%;max-height:400px}.douban-card{display:flex;margin:30px 10px;padding:15px;border-radius:15px;position:relative;justify-content:center;align-items:center;overflow:hidden;color:#faebd7;text-decoration:none}.douban-card:hover{text-decoration:none}.douban-card-bgimg{position:absolute;width:115%;height:115%;filter:blur(15px) brightness(.6);background-size:100%;background-position:center;background-repeat:no-repeat}.douban-card-img{position:relative;height:130px;width:80px;background-size:100%;background-position:center;background-repeat:no-repeat}.douban-card-left:hover .douban-card-img{filter:blur(5px) brightness(.6);transform:perspective(800px) rotateX(180deg)}.douban-card-left .douban-card-img{transition:all .5s ease}.douban-card-left{position:relative;display:flex;flex-direction:column;align-items:center}.douban-card-left .douban-card-status{height:130px;width:80px;text-align:center;font-weight:700;position:absolute;left:0;top:30%;transform:rotateX(180deg);backface-visibility:hidden;transition:all .5s ease}.douban-card-left:hover .douban-card-status{transform:perspective(800px) rotateX(0)}.douban-card-right{position:relative;display:flex;flex-direction:column;margin-left:12px;font-size:16px;font-family:"Courier New",Courier,monospace;line-height:1.3;color:#faebd7}.douban-card-item{margin-top:4px}</style><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta name="theme-color" content="#222" media="(prefers-color-scheme: light)"><meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=Roboto+Mono:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=PT+Mono:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script><script class="next-config" data-name="main" type="application/json">{"hostname":"www.shaogui.life","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.22.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"flat","show_result":true},"fold":{"enable":true,"height":200},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script><meta name="description" content="在使用 transformer 类模型预测时，是逐位输出 token，输出完整的回答的过程，是 transformer 的解码策略。"><meta property="og:type" content="article"><meta property="og:title" content="大模型参数设置"><meta property="og:url" content="https://www.shaogui.life/posts/245048764.html"><meta property="og:site_name" content="年轻人起来冲"><meta property="og:description" content="在使用 transformer 类模型预测时，是逐位输出 token，输出完整的回答的过程，是 transformer 的解码策略。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%94%A8%E5%8F%82%E6%95%B0-20241202180829.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%94%A8%E5%8F%82%E6%95%B0-20241202180830.png"><meta property="article:published_time" content="2024-08-26T02:26:09.000Z"><meta property="article:modified_time" content="2025-01-18T10:49:30.312Z"><meta property="article:author" content="Shaogui"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%94%A8%E5%8F%82%E6%95%B0-20241202180829.png"><link rel="canonical" href="https://www.shaogui.life/posts/245048764.html"><script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://www.shaogui.life/posts/245048764.html","path":"posts/245048764.html","title":"大模型参数设置"}</script><script class="next-config" data-name="calendar" type="application/json">""</script><title>大模型参数设置 | 年轻人起来冲</title><noscript><link rel="stylesheet" href="/css/noscript.css"></noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><link rel="alternate" href="/atom.xml" title="年轻人起来冲" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage" class="use-motion"><div class="headband"></div><main class="main"><div class="column"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><p class="site-title">年轻人起来冲</p><i class="logo-line"></i></a></div><div class="site-nav-right"><div class="toggle popup-trigger" aria-label="搜索" role="button"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">77</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">70</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">544</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" maxlength="80" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close" role="button"><i class="fa fa-times-circle"></i></span></div><div class="search-result-container"><div class="search-result-icon"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></header><aside class="sidebar"><div class="sidebar-inner sidebar-overview-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="sidebar-panel-container"><div class="post-toc-wrap sidebar-panel"></div><div class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Shaogui" src="/images/avatar-2023.png"><p class="site-author-name" itemprop="name">Shaogui</p><div class="site-description" itemprop="description">害怕失败是本能，勇敢面对才是本事</div></div><div class="site-state-wrap animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">544</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">70</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">77</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author animated"><span class="links-of-author-item"><a href="https://github.com/WuShaogui" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;WuShaogui" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/mu-zhi-zhi-tian" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;mu-zhi-zhi-tian" rel="noopener me" target="_blank"><i class="fab fa-zhihu fa-fw"></i>知乎</a></span></div><div class="cc-license animated" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a></div></div></div><div class="back-to-top animated" role="button" aria-label="返回顶部"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div><div class="sidebar-inner sidebar-blogroll"><div class="links-of-blogroll animated"><div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i> 链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://manaai.cn/" title="http:&#x2F;&#x2F;manaai.cn&#x2F;" rel="noopener" target="_blank">神力AI</a></li></ul></div></div></aside></div><div class="main-inner post posts-expand"><div class="post-block"><article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.shaogui.life/posts/245048764.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar-2023.png"><meta itemprop="name" content="Shaogui"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="年轻人起来冲"><meta itemprop="description" content="害怕失败是本能，勇敢面对才是本事"></span><span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork"><meta itemprop="name" content="大模型参数设置 | 年轻人起来冲"><meta itemprop="description" content=""></span><header class="post-header"><h1 class="post-title" itemprop="name headline">大模型参数设置</h1><div class="post-meta-container"><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2024-08-26 10:26:09" itemprop="dateCreated datePublished" datetime="2024-08-26T10:26:09+08:00">2024-08-26</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2025-01-18 18:49:30" itemprop="dateModified" datetime="2025-01-18T18:49:30+08:00">2025-01-18</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">2-深度学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLM%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88%E6%8C%87%E5%8D%97/" itemprop="url" rel="index"><span itemprop="name">LLM开发工程师指南</span></a> </span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv"><span class="post-meta-item-icon"><i class="far fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-break"></span> <span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>5.1k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>5 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>在使用 transformer 类模型预测时，是逐位输出 token，输出完整的回答的过程，是 transformer 的解码策略。</p><span id="more"></span><p>目前已有的解码策略如下：假设词库大小是 N，预测 token 数量是 T，假设使用 BeamSearch，束宽为 K</p><table><thead><tr><th>解码策略</th><th>设置</th><th>描述</th><th>复杂度</th><th>优缺点</th></tr></thead><tbody><tr><td>暴力遍历</td><td>-</td><td>每个单步时都做 N 次预测，直到遇到停止 token 或者达到最大预测长度</td><td>N*N*T</td><td>全局最优，但是复杂度极高</td></tr><tr><td>贪婪搜索 greedy_search</td><td>Num_beams=1, do_sample=False</td><td>每个单步时选择最大概率的 token，并作为下次预测的输入，直到遇到停止 token 或者达到最大预测长度</td><td>N*T</td><td>局部最优，搜索空间唯一，每次生成结果一样</td></tr><tr><td>随机贪婪搜索 sample</td><td>Num_beams=1, do_sample=True</td><td>每个单步时会根据模型输出的概率进行采用，而不是选条件概率最高的词，增加多样性</td><td>N*T</td><td>局部最优，结果多样化</td></tr><tr><td>贪婪柱搜索 beam_search</td><td>Num_beams&gt;1, do_sample=False</td><td>每个单步时会会选择 K 个 “概率链” 最大的输出</td><td>N*T*K</td><td>局部最优，比贪婪搜索空间更大</td></tr><tr><td>采样柱搜索 beam_sample</td><td>Num_beams&gt;1, do_sample=True</td><td>每个单步时会会选择 K 个 “概率链” 较大的输出，并且增加多样性</td><td>N*T*K</td><td>局部最优，比贪婪搜索空间更大，结果多样化</td></tr></tbody></table><p>不同的解码方式参数，组成了 LLMs 的 “调用参数”，注意这些参数不是直接修改模型参数，而是通过影响模型预测 “后处理” 实现影响模型输出，以下例子可以说明这个观点</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM, set_seed</span><br><span class="line">set_seed(<span class="number">0</span>)  <span class="comment"># for reproducibility</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">"openai-community/gpt2"</span>,cache_dir=<span class="string">'./models'</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">"openai-community/gpt2"</span>,cache_dir=<span class="string">'./models'</span>)</span><br><span class="line">model.config.pad_token_id = model.config.eos_token_id</span><br><span class="line">inputs = tokenizer([<span class="string">"Hugging Face Company is"</span>], return_tensors=<span class="string">"pt"</span>)</span><br><span class="line"></span><br><span class="line">generate_kwargs = {<span class="string">"temperature"</span>: <span class="number">0.9</span>, <span class="string">"top_k"</span>:<span class="number">30</span>, <span class="string">"num_beams"</span>:<span class="number">2</span>, <span class="string">"do_sample"</span>: <span class="literal">True</span>,<span class="string">"num_return_sequences"</span>:<span class="number">2</span>}</span><br><span class="line">outputs = model.generate(**inputs, **generate_kwargs)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.batch_decode(outputs, skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></tbody></table></figure><blockquote><p>[‘Hugging Face Company is one of the largest and most recognizable brands in the industry. The company has’, ‘Hugging Face Company is one of the largest and most recognizable brands in the industry. The company is’]</p></blockquote><p>深入代码内部，发现设置的参数确实是在函数输出后起作用的<br><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%94%A8%E5%8F%82%E6%95%B0-20241202180829.png" alt="大模型调用参数-20241202180829"><br>继续分析，类似 temperature、top_k 的内部实现如下：<br><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%94%A8%E5%8F%82%E6%95%B0-20241202180830.png" alt="大模型调用参数-20241202180830"></p><p>不同框架调用 LLMs 提供可修改的参数不一样，这是因为不同框架采样不同的后处理方式，但是一般来说，包含以下参数：</p><table><thead><tr><th>参数</th><th>推荐值</th><th>简介</th><th>定义</th></tr></thead><tbody><tr><td><strong>temperature</strong></td><td>0.95</td><td>这个值越大生成内容越随机，多样性更好</td><td>这个参数控制着生成的随机性。较高的温度值会增加文本的多样性和创造性，但可能会牺牲一些准确性或连贯性。具体地，temperature 会调整概率输出的 softmax 概率分布，如果 temperature 的值为 1，则没有任何调整；如果其值比 1 大，则会生成更加随机的文本；如果其值比 1 小，则生成的文本更加保守。</td></tr><tr><td><strong>top_p</strong></td><td>0.95</td><td>单步累计采用阈值，越大越多 token 会被考虑</td><td>如果累计概率已经超过 0.95，剩下的 token 不会被考虑例如有下面的 token 及其概率，a: 0.9, b: 0.03, c: 0.03, d: 0.015,e… 。则只会采用用 abc，因为已经是 0.96 超过了 0.95</td></tr><tr><td><strong>top_k</strong></td><td>50</td><td>单步采用 token 的数量，越大采用 token 会越多</td><td>单步中最多考虑的 token 数量</td></tr><tr><td>max_length</td><td>512</td><td>最大采样长度</td><td>模型生成的文本最大长度，超过的话会做截断，512 是参考值，这个依赖于实际情况自己设置</td></tr><tr><td>num_beams</td><td>1</td><td>beam 搜索数量，越大文本质量越高</td><td>想象一棵树，这个树在每一层的叶子节点数量都是 num_beams 个，正常模型推理时设置成 1 就行啦；num_beams=20 表示在每一步时，模型会保留 20 个最有可能的候选序列，保留方式是累计概率乘积。这有助于生成更加精确和高质量的文本。</td></tr><tr><td>do_sample</td><td>False</td><td>是否概率采样 token 得到结果</td><td>当设置为 False 时，模型在生成文本时不会随机采样，而是选择最可能的下一个词。这使得生成的文本更加确定和一致。</td></tr><tr><td>num_beam_groups</td><td>1</td><td>分成 num_beam_groups 组进行搜索</td><td>这个参数与束搜索相关。它将搜索的束分为不同的组，每个组内部进行搜索。这可以增加文本的多样性。Num_beam_groups 包含 num_beams</td></tr><tr><td>num_return_sequences</td><td>1</td><td>有多少条返回的结果</td><td>推理的话设成 1 就好了</td></tr><tr><td>output_scores</td><td>True</td><td>调试实验时用到</td><td>设为 True 时模型在生成文本的每一步都会输出每个词的分数（或概率），这有助于了解模型是如何在不同选项中做出选择的。</td></tr><tr><td><strong>repetition_penalty</strong></td><td>1</td><td>重复惩罚值，越大越不会生成重复 token</td><td>默认值为 1.0，其中较高的值意味着更强的惩罚，生成的文本中将出现更少的重复。如果取值为 0，则没有惩罚，生成的文本可能包含大量重复的内容。</td></tr><tr><td>max_new_tokens</td><td>256</td><td>模型生成的最大新词数</td><td>在这里设置为 256，意味着每次生成的文本最多包含 256 个新词。</td></tr><tr><td>diversity_penalty</td><td>1.5</td><td>当使用多束搜索时，这个参数惩罚那些在不同束中过于相似的词，以提高生成文本的多样性。</td><td>设置为 1.5 意味着对相似性施加较大的惩罚。如果在同一个 step 中某个 beam 生成的词和其他 beam 有相同的，那么就减去这个值作为惩罚，仅在 num_beam_groups 启用时这个值才有效</td></tr><tr><td>length_penalty</td><td>1</td><td>beam search 分数会受到生成序列长度的惩罚</td><td>length_penalty=0.0：无惩罚、length_penalty&lt;0.0：鼓励模型生成长句子、length_penalty&gt;0.0：鼓励模型生成短句子</td></tr><tr><td>eos_token_id</td><td>-</td><td>指定搜索时的结束 token</td><td>有时可以提升模型性能，例如同时指定<eos>和<user>为结束符可以让模型在<user>出现时也结束，防止模型停不下来</user></user></eos></td></tr><tr><td>bad_words_ids</td><td>-</td><td>禁止生成的 token</td><td>帮助解决伦理安全、种族歧视等问题</td></tr><tr><td>prefix_allowed_tokens_fn</td><td>-</td><td>约束模型只能在给定的 tokens 里生成 token</td><td>帮助特定功能的模型提升性能</td></tr></tbody></table><p>以下通过设置不同参数，观察 LLMs 不同搜索策略的效果</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 贪婪搜索</span></span><br><span class="line">Generate_kwargs = {<span class="string">"num_beams"</span>: <span class="number">1</span>, <span class="string">"do_sample"</span>: <span class="literal">False</span>}</span><br><span class="line">Outputs = model.Generate (**inputs, **generate_kwargs)</span><br><span class="line">Print (tokenizer. Batch_decode (outputs, skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></tbody></table></figure><blockquote><p>[‘Hugging Face Company is a company that has been around for over 20 years. We have been in’]</p></blockquote><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机贪婪搜索</span></span><br><span class="line">Generate_kwargs = {<span class="string">"num_beams"</span>: <span class="number">1</span>, <span class="string">"do_sample"</span>: <span class="literal">True</span>,<span class="string">"num_return_sequences"</span>: <span class="number">2</span>}</span><br><span class="line">Outputs = model.Generate (**inputs, **generate_kwargs)</span><br><span class="line">Print (tokenizer. Batch_decode (outputs, skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></tbody></table></figure><blockquote><p>[‘Hugging Face Company is a full-service independent contractor that specializes in building and operating public access public’, ‘Hugging Face Company is out of business. For many years their headquarters have been called The New Haven’]</p></blockquote><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 贪婪柱搜索</span></span><br><span class="line">Generate_kwargs = {<span class="string">"num_beams"</span>: <span class="number">2</span>, <span class="string">"do_sample"</span>: <span class="literal">False</span>}</span><br><span class="line">Outputs = model.Generate (**inputs, **generate_kwargs)</span><br><span class="line">Print (tokenizer. Batch_decode (outputs, skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></tbody></table></figure><blockquote><p>[‘Hugging Face Company is a company that specializes in the production of high-quality, high-quality’]</p></blockquote><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采样柱搜索</span></span><br><span class="line">Generate_kwargs = {<span class="string">"num_beams"</span>: <span class="number">2</span>, <span class="string">"do_sample"</span>: <span class="literal">True</span>,<span class="string">"num_return_sequences"</span>: <span class="number">2</span>}</span><br><span class="line">Outputs = model.Generate (**inputs, **generate_kwargs)</span><br><span class="line">Print (tokenizer. Batch_decode (outputs, skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></tbody></table></figure><blockquote><p>[‘Hugging Face Company is a small, small business that has been in business for over 50 years.’, ‘Hugging Face Company is a small, small business that has been in business for over a decade.’]</p></blockquote><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多组柱搜索搜索</span></span><br><span class="line">Generate_kwargs = {<span class="string">"num_beams"</span>: <span class="number">2</span>, <span class="string">"num_beam_groups"</span>: <span class="number">2</span>,<span class="string">'diversity_penalty'</span>: <span class="number">0.2</span>,<span class="string">"num_return_sequences"</span>: <span class="number">2</span>}</span><br><span class="line">Outputs = model.Generate (**inputs, **generate_kwargs)</span><br><span class="line">Print (tokenizer. Batch_decode (outputs, skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></tbody></table></figure><blockquote><p>[‘Hugging Face Company is a company that has been around for over 20 years. We have been in’, ‘Hugging Face Company is a small, independent, non-profit organization that provides free, confidential,’]</p></blockquote></div><footer class="post-footer"><div class="reward-container"><div>请我一杯咖啡吧！</div><button>赞赏</button><div class="post-reward"><div><img src="/images/wechatpay.png" alt="Shaogui 微信"> <span>微信</span></div><div><img src="/images/alipay.png" alt="Shaogui 支付宝"> <span>支付宝</span></div></div></div><div class="post-copyright"><ul><li class="post-copyright-author"><strong>本文作者： </strong>Shaogui</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://www.shaogui.life/posts/245048764.html" title="大模型参数设置">https://www.shaogui.life/posts/245048764.html</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><span>欢迎关注我的其它发布渠道</span><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="https://www.zhihu.com/people/mu-zhi-zhi-tian"><span class="icon"><i class="fab fa-zhihu"></i> </span><span class="label">知乎</span></a></div><div class="social-item"><a target="_blank" class="social-link" href="/atom.xml"><span class="icon"><i class="fa fa-rss"></i> </span><span class="label">RSS</span></a></div></div></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/1919787814.html" rel="prev" title="transformer：Attention Is All You Need"><i class="fa fa-angle-left"></i> transformer：Attention Is All You Need</a></div><div class="post-nav-item"><a href="/posts/4013245924.html" rel="next" title="大模型初次使用">大模型初次使用 <i class="fa fa-angle-right"></i></a></div></div></footer></article></div><div class="comments utterances-container"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2016 – <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Shaogui</span></div><div class="wordcount"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-chart-line"></i> </span><span>站点总字数：</span> <span title="站点总字数">1.9m</span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span>站点阅读时长 &asymp;</span> <span title="站点阅读时长">28:48</span></span></div><div class="busuanzi-count"><span class="post-meta-item" id="busuanzi_container_site_uv"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-item" id="busuanzi_container_site_pv"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div><div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动</div></div></footer><div class="toggle sidebar-toggle" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div><div class="sidebar-dimmer"></div><div class="reading-progress-bar"></div><a href="https://github.com/WuShaogui" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><noscript><div class="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script><script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script><script src="/js/third-party/search/local-search.js"></script><script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"neutral","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script><script src="/js/third-party/tags/mermaid.js"></script><script src="/js/third-party/pace.js"></script><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script><script src="/js/third-party/math/mathjax.js"></script><script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"WuShaogui/wushaogui.github.io","issue_term":"pathname","theme":"github-light"}</script><script src="/js/third-party/comments/utterances.js"></script></body></html>