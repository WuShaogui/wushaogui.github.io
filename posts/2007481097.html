<!DOCTYPE html><html lang="zh-CN"><head><style type="text/css">.douban-card-block{display:flex;justify-content:center;align-items:center;width:100%;max-height:400px}.douban-card{display:flex;margin:30px 10px;padding:15px;border-radius:15px;position:relative;justify-content:center;align-items:center;overflow:hidden;color:#faebd7;text-decoration:none}.douban-card:hover{text-decoration:none}.douban-card-bgimg{position:absolute;width:115%;height:115%;filter:blur(15px) brightness(.6);background-size:100%;background-position:center;background-repeat:no-repeat}.douban-card-img{position:relative;height:130px;width:80px;background-size:100%;background-position:center;background-repeat:no-repeat}.douban-card-left:hover .douban-card-img{filter:blur(5px) brightness(.6);transform:perspective(800px) rotateX(180deg)}.douban-card-left .douban-card-img{transition:all .5s ease}.douban-card-left{position:relative;display:flex;flex-direction:column;align-items:center}.douban-card-left .douban-card-status{height:130px;width:80px;text-align:center;font-weight:700;position:absolute;left:0;top:30%;transform:rotateX(180deg);backface-visibility:hidden;transition:all .5s ease}.douban-card-left:hover .douban-card-status{transform:perspective(800px) rotateX(0)}.douban-card-right{position:relative;display:flex;flex-direction:column;margin-left:12px;font-size:16px;font-family:"Courier New",Courier,monospace;line-height:1.3;color:#faebd7}.douban-card-item{margin-top:4px}</style><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta name="theme-color" content="#222" media="(prefers-color-scheme: light)"><meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=Roboto+Mono:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=PT+Mono:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script><script class="next-config" data-name="main" type="application/json">{"hostname":"www.shaogui.life","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.22.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"flat","show_result":true},"fold":{"enable":true,"height":200},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script><meta name="description" content="文本类 prompt 的编写指南"><meta property="og:type" content="article"><meta property="og:title" content="文本类 prompt 编写指南"><meta property="og:url" content="https://www.shaogui.life/posts/2007481097.html"><meta property="og:site_name" content="年轻人起来冲"><meta property="og:description" content="文本类 prompt 的编写指南"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205950.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205951.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205952.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205953.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205954.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205956.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205957.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205958.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205954.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%8C%87%E4%BB%A4%E5%B7%A5%E7%A8%8BPromptEngineering-20241130115725.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E5%8F%8D%E5%B0%84Reflexion-20241214191453.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E5%8F%8D%E5%B0%84Reflexion-20241214191831.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E4%B8%BB%E5%8A%A8%E6%8F%90%E7%A4%BAActivePrompt-20241215083631.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%80%9D%E7%BB%B4%E7%BA%BF%E7%B4%A2ThoT-20241216085152.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E8%87%AA%E6%88%91%E8%AF%A2%E9%97%AE%E6%8F%90%E7%A4%BASelf-Ask-20241216090421.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%80%9D%E7%BB%B4%E9%93%BECoT-20241214165005.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%80%9D%E7%BB%B4%E9%93%BECoT-20241214165224.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E8%87%AA%E5%8A%A8%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%B8%88APE-20241214181852.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E8%87%AA%E5%8A%A8%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%B8%88APE-20241214181838.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E7%9F%A5%E8%AF%86%E6%8F%90%E7%A4%BA-20241214171911.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E8%87%AA%E5%8A%A8%E6%8E%A8%E7%90%86%E5%B9%B6%E4%BD%BF%E7%94%A8%E5%B7%A5%E5%85%B7ART-20241214180607.png"><meta property="og:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E5%85%83%E6%8F%90%E7%A4%BAMetaPrompt-20241214170444.png"><meta property="article:published_time" content="2024-09-07T14:43:40.000Z"><meta property="article:modified_time" content="2025-01-18T10:51:21.023Z"><meta property="article:author" content="Shaogui"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205950.png"><link rel="canonical" href="https://www.shaogui.life/posts/2007481097.html"><script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://www.shaogui.life/posts/2007481097.html","path":"posts/2007481097.html","title":"文本类 prompt 编写指南"}</script><script class="next-config" data-name="calendar" type="application/json">""</script><title>文本类 prompt 编写指南 | 年轻人起来冲</title><noscript><link rel="stylesheet" href="/css/noscript.css"></noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><link rel="alternate" href="/atom.xml" title="年轻人起来冲" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage" class="use-motion"><div class="headband"></div><main class="main"><div class="column"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><p class="site-title">年轻人起来冲</p><i class="logo-line"></i></a></div><div class="site-nav-right"><div class="toggle popup-trigger" aria-label="搜索" role="button"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">77</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">70</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">544</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" maxlength="80" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close" role="button"><i class="fa fa-times-circle"></i></span></div><div class="search-result-container"><div class="search-result-icon"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></header><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="sidebar-panel-container"><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%B4%E6%8E%A5%E6%8F%90%E7%A4%BA"><span class="nav-number">1.</span> <span class="nav-text">直接提示</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9B%B6%E6%A0%B7%E6%9C%AC%E6%8F%90%E7%A4%BA-zeroshot"><span class="nav-number">1.1.</span> <span class="nav-text">零样本提示 ZeroShot</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%91%E6%A0%B7%E6%9C%AC%E6%8F%90%E7%A4%BA-fewshot"><span class="nav-number">1.2.</span> <span class="nav-text">少样本提示 FewShot</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#react"><span class="nav-number">1.3.</span> <span class="nav-text">ReAct</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E5%90%91%E6%80%A7%E5%88%BA%E6%BF%80"><span class="nav-number">1.4.</span> <span class="nav-text">方向性刺激</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%93%BE%E5%BC%8F%E6%8F%90%E7%A4%BA-chainofthought"><span class="nav-number">2.</span> <span class="nav-text">链式提示 ChainofThought</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E5%B0%91%E5%88%B0%E6%9C%80%E5%A4%9A%E6%8F%90%E7%A4%BA-ltm"><span class="nav-number">2.1.</span> <span class="nav-text">最少到最多提示 LtM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9B%B6%E6%A0%B7%E6%9C%AC%E6%80%9D%E7%BB%B4%E9%93%BE-zeroshot-cot"><span class="nav-number">2.2.</span> <span class="nav-text">零样本思维链 ZeroShot-CoT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%91%E6%A0%B7%E6%9C%AC%E6%80%9D%E7%BB%B4%E9%93%BE-fewshot-cot"><span class="nav-number">2.3.</span> <span class="nav-text">少样本思维链 FewShot-CoT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%9D%E7%BB%B4%E6%A0%91-tot"><span class="nav-number">2.4.</span> <span class="nav-text">思维树 ToT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E5%B0%91%E5%88%B0%E6%9C%80%E5%A4%9A%E6%8F%90%E7%A4%BA-ltm-2"><span class="nav-number">2.5.</span> <span class="nav-text">最少到最多提示 LtM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E6%B4%BD%E6%80%A7"><span class="nav-number">2.6.</span> <span class="nav-text">自洽性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%B0%84-reflexion"><span class="nav-number">2.7.</span> <span class="nav-text">反射 Reflexion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E5%8A%A8%E6%8F%90%E7%A4%BA-activeprompt"><span class="nav-number">2.8.</span> <span class="nav-text">主动提示 ActivePrompt</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%9D%E7%BB%B4%E7%BA%BF%E7%B4%A2-thot"><span class="nav-number">2.9.</span> <span class="nav-text">思维线索 ThoT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E6%88%91%E8%AF%A2%E9%97%AE%E6%8F%90%E7%A4%BA-self-ask"><span class="nav-number">2.10.</span> <span class="nav-text">自我询问提示 Self-Ask</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E6%80%9D%E7%BB%B4%E9%93%BE-auto-cot"><span class="nav-number">2.11.</span> <span class="nav-text">自动思维链 (Auto-CoT)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94%E6%80%9D%E7%BB%B4%E9%93%BE-ccot"><span class="nav-number">2.12.</span> <span class="nav-text">对比思维链 CCoT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%A8%E6%A0%BC%E6%80%9D%E7%BB%B4%E9%93%BE-tabularcot"><span class="nav-number">2.13.</span> <span class="nav-text">表格思维链 TabularCot</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E8%B0%B1%E6%8F%90%E7%A4%BA"><span class="nav-number">3.</span> <span class="nav-text">图谱提示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E7%B1%BB%E6%8F%90%E7%A4%BA-generate"><span class="nav-number">4.</span> <span class="nav-text">生成类提示 (Generate)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%B8%88-ape"><span class="nav-number">4.1.</span> <span class="nav-text">自动提示工程师 APE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E7%9F%A5%E8%AF%86%E6%8F%90%E7%A4%BA"><span class="nav-number">4.2.</span> <span class="nav-text">生成知识提示</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%86%E6%88%90%E5%BC%8F%E6%8F%90%E7%A4%BA"><span class="nav-number">5.</span> <span class="nav-text">集成式提示</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E6%8E%A8%E7%90%86%E5%B9%B6%E4%BD%BF%E7%94%A8%E5%B7%A5%E5%85%B7-art"><span class="nav-number">5.1.</span> <span class="nav-text">自动推理并使用工具 ART</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96"><span class="nav-number">6.</span> <span class="nav-text">其他</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%83%E6%8F%90%E7%A4%BA-metaprompt"><span class="nav-number">6.1.</span> <span class="nav-text">元提示 MetaPrompt</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Shaogui" src="/images/avatar-2023.png"><p class="site-author-name" itemprop="name">Shaogui</p><div class="site-description" itemprop="description">害怕失败是本能，勇敢面对才是本事</div></div><div class="site-state-wrap animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">544</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">70</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">77</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author animated"><span class="links-of-author-item"><a href="https://github.com/WuShaogui" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;WuShaogui" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/mu-zhi-zhi-tian" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;mu-zhi-zhi-tian" rel="noopener me" target="_blank"><i class="fab fa-zhihu fa-fw"></i>知乎</a></span></div><div class="cc-license animated" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a></div></div></div><div class="back-to-top animated" role="button" aria-label="返回顶部"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div><div class="sidebar-inner sidebar-blogroll"><div class="links-of-blogroll animated"><div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i> 链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://manaai.cn/" title="http:&#x2F;&#x2F;manaai.cn&#x2F;" rel="noopener" target="_blank">神力AI</a></li></ul></div></div></aside></div><div class="main-inner post posts-expand"><div class="post-block"><article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.shaogui.life/posts/2007481097.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar-2023.png"><meta itemprop="name" content="Shaogui"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="年轻人起来冲"><meta itemprop="description" content="害怕失败是本能，勇敢面对才是本事"></span><span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork"><meta itemprop="name" content="文本类 prompt 编写指南 | 年轻人起来冲"><meta itemprop="description" content=""></span><header class="post-header"><h1 class="post-title" itemprop="name headline">文本类 prompt 编写指南</h1><div class="post-meta-container"><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2024-09-07 22:43:40" itemprop="dateCreated datePublished" datetime="2024-09-07T22:43:40+08:00">2024-09-07</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2025-01-18 18:51:21" itemprop="dateModified" datetime="2025-01-18T18:51:21+08:00">2025-01-18</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">2-深度学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLM%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88%E6%8C%87%E5%8D%97/" itemprop="url" rel="index"><span itemprop="name">LLM开发工程师指南</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LLM%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88%E6%8C%87%E5%8D%97/PromptEngineering/" itemprop="url" rel="index"><span itemprop="name">PromptEngineering</span></a> </span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv"><span class="post-meta-item-icon"><i class="far fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-break"></span> <span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>13k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>12 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><p>文本类 prompt 的编写指南</p><span id="more"></span><p>目前 prompt 技巧包含以下种类：</p><table><thead><tr><th>种类</th><th>描述</th></tr></thead><tbody><tr><td>直接提示</td><td>侧重点在于通过特定的指令设计，来提示模型产生更好的输出</td></tr><tr><td>链式提示</td><td>关注 LLM 的内部逻辑，在这个过程中，一般会将复杂问题进行拆解，分成多个步骤进行推理</td></tr><tr><td>图谱提示</td><td>和链条提示方式非常相似，都是关注大模型内部结构化的推理过程。但这个过程并不是线性的，而是假定训练过程中隐式地构建了网状或者是图形结构的知识图谱</td></tr><tr><td>生成类提示</td><td>利用 LLM 内部的知识来迭代补充知识的生成</td></tr><tr><td>集成式提示</td><td>大语言模型与外部资源进行交互，从而实现复杂任务，提升答案质量</td></tr></tbody></table><h2 id="直接提示"><a class="markdownIt-Anchor" href="#直接提示"></a> 直接提示</h2><h3 id="零样本提示-zeroshot"><a class="markdownIt-Anchor" href="#零样本提示-zeroshot"></a> 零样本提示 ZeroShot</h3><p>可以理解为不给 llm 任何上下文，直接向其发出指令，如以下命令</p><table><thead><tr><th>提示</th><th>llm 输出</th></tr></thead><tbody><tr><td>将文本分类为中性、负面或正面。<br><br>文本：我认为这次假期还可以。<br>情感：</td><td>中性</td></tr></tbody></table><p>模型的零样本能力是模型经过预训练后的基础能力，这个和后面 “零样本思维链” 作区分，可以认为：“零样本思维链”= 零样本 +“一步一步地思考”，但是 “零样本思维链” 一般用于解决复杂的，需要多步骤去解决的任务</p><h3 id="少样本提示-fewshot"><a class="markdownIt-Anchor" href="#少样本提示-fewshot"></a> 少样本提示 FewShot</h3><p>当零样本提示没有作用时，可以向模型提供少量样本，引导模型有更准确的输出</p><table><thead><tr><th>提示</th><th>llm 输出</th></tr></thead><tbody><tr><td>A “whatpu” is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:<br>We were traveling in Africa and we saw these very cute whatpus.<br><br>To do a “farduddle” means to jump up and down really fast. An example of a sentence that uses the word farduddle is:</td><td>When we won the game, we all started to farduddle in celebration.</td></tr><tr><td>This is awesome! // Negative<br>This is bad! // Positive<br>Wow that movie was rad! // Positive<br>What a horrible show! //</td><td>Negative</td></tr></tbody></table><p>少样本提示可以解决一些上下文明确的提问，对于哪些复杂的、有多步推理的问题，即使提示样本，llm 也可能也无法得到正确答案</p><table><thead><tr><th>提示</th><th>llm 输出</th></tr></thead><tbody><tr><td>The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.<br>A: The answer is False.<br><br>The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.<br>A: The answer is True.<br><br>The odd numbers in this group add up to an even number: 16, 11, 14, 4, 8, 13, 24.<br>A: The answer is True.<br><br>The odd numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.<br>A: The answer is False.<br><br>The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.<br>A:</td><td>The answer is True.</td></tr></tbody></table><p>此时 llm 输出是错误的，对于这种类型的推理问题，似乎 few-shot prompting 不足以获得可靠的响应。此时需要使用更高级的提示词技术少样本思维链 FewShot-CoT 去解决类似问题</p><h3 id="react"><a class="markdownIt-Anchor" href="#react"></a> ReAct</h3><p>无论是样本提示，还是生成更多、更准确的上下文，都涉及人工设计内容或者规则，既然 llm 如此有效，能不能让 llm 代替完成这些工作呢？答案是 ReAct</p><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205950.png" alt="文本类prompt的编写指南-20250115205950"></p><p>ReAct 通过重复 “thought-&gt;act-&gt;Obs”，不断重复这个过程，直到问题能被回答，也就是 llm 不断通过自己上一步的结果，规划自己的下一步。在 “Act” 过程中，llm 通过外部工具，获取最新的、明确的知识，一定程度缓解幻觉问题</p><h3 id="方向性刺激"><a class="markdownIt-Anchor" href="#方向性刺激"></a> 方向性刺激</h3><p>通过一个可调节的 LM 生成 Prompt 的方向性刺激或提示，使得 LLM 回答更加准确，这个可调节的 LM 通过增强训练优化</p><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205951.png" alt="文本类prompt的编写指南-20250115205951"></p><p>其实就是通过 llm 为 prompt 生成更加准确的上下文</p><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205952.png" alt="文本类prompt的编写指南-20250115205952"></p><h2 id="链式提示-chainofthought"><a class="markdownIt-Anchor" href="#链式提示-chainofthought"></a> 链式提示 ChainofThought</h2><ol><li><strong>什么是 Chain of Mind 提示</strong></li></ol><p>思维链提示是一种提示工程方法，它通过鼓励大型语言模型 （LLM） 将其推理分解为一系列中间步骤来增强其推理能力</p><p>从本质上讲，Chain of Thought prompting 鼓励模型以循序渐进的方式思考问题，这应该模仿人类如何分解复杂问题</p><p>左边是标准的 prompt，右边提供例子的同时，给出得到答案的步骤，最终模型解决了新的问题</p><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205953.png" alt="文本类prompt的编写指南-20250115205953"></p><ol start="2"><li><p><strong>为什么 Chain of Thought 提示有帮助</strong></p></li><li><p><strong>分解复杂的问题：</strong> Chain of Thought 提示使 LLM 能够将复杂问题分解为一系列中间步骤。从理论上讲，这种循序渐进的方法允许模型将更多注意力分配给问题的每个部分，从而实现更准确的推理。</p></li><li><p>模型思维<strong>过程一瞥：</strong> 通过查看模型执行的推理步骤，用户可以更好地了解模型并调试推理路径是否 / 何时出错。</p></li><li><p><strong>适用范围广：</strong> Chain of Thought 提示已经在大量不同的任务中成功进行了测试。它的用途广泛，可以应用于需要任何类型推理的各种任务。</p></li><li><p><strong>易于实施：</strong> 虽然实施 Chain of Mind 提示的方法有很多种，但也有很多非常简单的方法。</p></li><li><p><strong>Chain of Thought 提示示例</strong></p></li></ol><table><thead><tr><th>prompt</th><th>Type</th></tr></thead><tbody><tr><td>Q:<br>A; Let’s think step by step</td><td>零样本思维链 ZeroShot-CoT</td></tr><tr><td>Let’s work this out in a step-by-step way to be sure we have the right answer."</td><td>零样本思维链 ZeroShot-CoT</td></tr><tr><td>First, let’s think about this logically.</td><td>零样本思维链 ZeroShot-CoT</td></tr><tr><td>Given the following problem, generate multiple answers using diverse reasoning paths and aggregate the final answers to come to a final conclusion.<br><br>Question:</td><td>自洽性</td></tr><tr><td>Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there<br>Will be 21 trees. How many trees did the grove workers plant today?<br><br>A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have<br>Been 21 - 15 = 6. The answer is 6.<br><br>Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?<br>A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.<br><br>Q:<br>A:</td><td>少样本思维链 FewShot-CoT</td></tr><tr><td>Here is a question or task:<br><br>Let’s think step-by-step to answer this:<br><br>Step 1) Abstract the key concepts and principles relevant to this question:<br><br>Step 2) Use the abstractions to reason through the question:<br><br>Final Answer:</td><td>Step-Back</td></tr><tr><td># Problem:<br># Instructions<br>## Tutorial: Identify core concepts or algorithms used to solve the problem<br>## Relevant problems: Recall three relevant and distinct problems. For each problem, describe it and explain the solution.<br>## Solve the initial problem:</td><td>自动思维链 (Auto-CoT)</td></tr><tr><td><br>“Walk me through this context in manageable parts step by step, summarizing and analyzing as we go.”</td><td>思维线索 ThoT</td></tr><tr><td>Question : James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a year?<br>Explanation: He writes each friend 3<em>2=6 pages a week. So he writes 6</em>2=12 pages every week. That means he writes 12<em>52=624 pages a year.<br>Wrong Explanation: He writes each friend 12</em>52=624 pages a week. So he writes 3<em>2=6 pages every week. That means he writes 6</em>2=12 pages a year.<br>Question: James has 30 teeth. His dentist drills 4 of them and caps 7 more teeth than he drills. What percentage of James’ teeth does the dentist fix?</td><td>对比思维链 CCoT</td></tr><tr><td>Jackson is planting tulips. He can fit 6 red tulips in a row and 8 blue<br>Tulips in a row. If Jackson buys 36 red tulips and 24 blue tulips, how<br>Many rows of flowers will he plant?<br><br><strong>|step|event|result|</strong></td><td>表格思维链 TabularCot</td></tr></tbody></table><h3 id="最少到最多提示-ltm"><a class="markdownIt-Anchor" href="#最少到最多提示-ltm"></a> 最少到最多提示 LtM</h3><p>将复杂的问题分解为多个简单的问题地解决，与思维链提示过程类似，需要解决的问题被分解成一组建立在彼此之上的子问题。在第二步中，这些子问题被逐个解决。与思维链不同的是，先前子问题的解决方案被输入到提示中，以尝试解决下一个问题</p><p>比如在回答大型文档的问答中，先通过第一个 prompt 提取相关引文，然后使用第二个 prompt 回答问题</p><table><thead><tr><th>提示</th><th>llm 输出</th></tr></thead><tbody><tr><td>你是一个很有帮助的助手。你的任务是根据文档回答问题。第一步是从文档中提取与问题相关的引文，由 #### 分隔。请使用<quotes></quotes>输出引文列表。如果没有找到相关引文，请回应 “未找到相关引文！”。<br><br>####<br><br>####</td><td><quotes><br>- Chain-of-thought (CoT) prompting[27]<br>- Generated knowledge prompting[37]<br>- Least-to-most prompting[38]<br>- Self-consistency decoding[39]<br>- Complexity-based prompting[41]<br>- Self-refine[42]<br>- Tree-of-thought prompting[43]<br>- Maieutic prompting[45]<br>- Directional-stimulus prompting[46]<br>- Textual inversion and embeddings[59]<br>- Using gradient descent to search for prompts[61][62][63][64]<br>- Prompt injection[65][66][67]<br></quotes></td></tr><tr><td>根据从文档中提取的相关引文（由<quotes></quotes>分隔）和原始文档（由 #### 分隔），请构建对问题的回答。请确保答案准确、语气友好且有帮助。<br><br>####<br><br>####<br><br><quotes><br>- Chain-of-thought (CoT) prompting[27]<br>- Generated knowledge prompting[37]<br>- Least-to-most prompting[38]<br>- Self-consistency decoding[39]<br>- Complexity-based prompting[41]<br>- Self-refine[42]<br>- Tree-of-thought prompting[43]<br>- Maieutic prompting[45]<br>- Directional-stimulus prompting[46]<br>- Textual inversion and embeddings[59]<br>- Using gradient descent to search for prompts[61][62][63][64]<br>- Prompt injection[65][66][67]<br></quotes></td><td>文档中提到的提示技术包括：<br><br>1. Chain-of-thought (CoT) prompting[27]<br>2. Generated knowledge prompting[37]<br>3. Least-to-most prompting[38]<br>4. Self-consistency decoding[39]<br>5. Complexity-based prompting[41]<br>6. Self-refine[42]<br>7. Tree-of-thought prompting[43]<br>8. Maieutic prompting[45]<br>9. Directional-stimulus prompting[46]<br>10. Textual inversion and embeddings[59]<br>11. Using gradient descent to search for prompts[61][62][63][64]<br>12. Prompt injection[65][66][67]<br><br>每一种技术都采用独特的策略来增强或明确与大型语言模型的互动，以产生期望的结果。</td></tr></tbody></table><p>不仅仅通过手动设计 “多步骤的 prompt”，还可以使用 llm 自己提出解决问题的 prmot，这有点类似 Agent 的 Plan-and-Execute 技术，首先通过 llm 设计解决问题的子任务，通过一个个回答子任务，回答最终问题</p><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205954.png" alt="文本类prompt的编写指南-20250115205954|LtM过程"></p><h3 id="零样本思维链-zeroshot-cot"><a class="markdownIt-Anchor" href="#零样本思维链-zeroshot-cot"></a> 零样本思维链 ZeroShot-CoT</h3><p>少样本思维链 FewShot-CoT 提供一个类似的问题及解题思路，应该模型去思考，那么不提供例子，而是在 prompt 中添加类似 <code>Let's think step by step</code>，实际上，这也是有效果的</p><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205956.png" alt="文本类prompt的编写指南-20250115205956|完整零样本思维链过程"></p><table><thead><tr><th>提示</th><th>llm 输出</th></tr></thead><tbody><tr><td>I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?<br><br>Let’s think step by step.</td><td>First, you started with 10 apples.<br>You gave away 2 apples to the neighbor and 2 to the repairman, so you had 6 apples left.<br>Then you bought 5 more apples, so now you had 11 apples.<br>Finally, you ate 1 apple, so you would remain with 10 apples.</td></tr></tbody></table><h3 id="少样本思维链-fewshot-cot"><a class="markdownIt-Anchor" href="#少样本思维链-fewshot-cot"></a> 少样本思维链 FewShot-CoT</h3><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205957.png" alt="文本类prompt的编写指南-20250115205957|思维链Cot"></p><p>通过在 prompt 引入推理步骤，结合少样本提示，引导模型解决复杂的推理问题</p><table><thead><tr><th>提示</th><th>llm 输出</th></tr></thead><tbody><tr><td>The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.<br>A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.<br><br>The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.<br>A: Adding all the odd numbers (17, 19) gives 36. The answer is True.<br><br>The odd numbers in this group add up to an even number: 16, 11, 14, 4, 8, 13, 24.<br>A: Adding all the odd numbers (11, 13) gives 24. The answer is True.<br><br>The odd numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.<br>A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.<br><br>The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.<br>A:</td><td>Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.</td></tr></tbody></table><h3 id="思维树-tot"><a class="markdownIt-Anchor" href="#思维树-tot"></a> 思维树 ToT</h3><p>对于需要探索及动态决策的复杂任务来说，传统的提示技巧是不够，类比以下：在 Agent 的 Plan-and-Execute 技术中，会提前规划子任务，然后按顺序执行子任务，但是如果某个子任务的答案不理想，会影响后续子任务的回答。</p><p>基于此提出了思维树（Tree of Thoughts，ToT）框架，ToT 使 LM 能够做出深思熟虑的决策、考虑多种推理路径并自我评估选择。它还允许 LM 在做出全局决策时进行必要的前瞻性或回溯</p><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205958.png" alt="文本类prompt的编写指南-20250115205958"></p><p>ToT 需要针对不同的任务定义思维 / 步骤的数量以及每步的候选项数量。例如，论文中的 “算 24 游戏” 是一种数学推理任务，需要分成 3 个思维步骤，每一步都需要一个中间方程。而每个步骤保留最优的（best） 5 个候选项</p><p>将 ToT 框架的主要概念概括成了一段简短的提示词，指导 LLM 在一次提示中对中间思维做出评估。ToT 提示词的例子如下：</p><figure class="highlight text"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">假设三位不同的专家来回答这个问题。</span><br><span class="line">所有专家都写下他们思考这个问题的第一个步骤，然后与大家分享。</span><br><span class="line">然后，所有专家都写下他们思考的下一个步骤并分享。</span><br><span class="line">以此类推，直到所有专家写完他们思考的所有步骤。</span><br><span class="line">只要大家发现有专家的步骤出错了，就让这位专家离开。</span><br><span class="line">请问...</span><br></pre></td></tr></tbody></table></figure><h3 id="最少到最多提示-ltm-2"><a class="markdownIt-Anchor" href="#最少到最多提示-ltm-2"></a> 最少到最多提示 LtM</h3><p>将复杂的问题分解为多个简单的问题地解决，与思维链提示过程类似，需要解决的问题被分解成一组建立在彼此之上的子问题。在第二步中，这些子问题被逐个解决。与思维链不同的是，先前子问题的解决方案被输入到提示中，以尝试解决下一个问题</p><p>比如在回答大型文档的问答中，先通过第一个 prompt 提取相关引文，然后使用第二个 prompt 回答问题</p><table><thead><tr><th>提示</th><th>llm 输出</th></tr></thead><tbody><tr><td>你是一个很有帮助的助手。你的任务是根据文档回答问题。第一步是从文档中提取与问题相关的引文，由 #### 分隔。请使用<quotes></quotes>输出引文列表。如果没有找到相关引文，请回应 “未找到相关引文！”。<br><br>####<br><br>####</td><td><quotes><br>- Chain-of-thought (CoT) prompting[27]<br>- Generated knowledge prompting[37]<br>- Least-to-most prompting[38]<br>- Self-consistency decoding[39]<br>- Complexity-based prompting[41]<br>- Self-refine[42]<br>- Tree-of-thought prompting[43]<br>- Maieutic prompting[45]<br>- Directional-stimulus prompting[46]<br>- Textual inversion and embeddings[59]<br>- Using gradient descent to search for prompts[61][62][63][64]<br>- Prompt injection[65][66][67]<br></quotes></td></tr><tr><td>根据从文档中提取的相关引文（由<quotes></quotes>分隔）和原始文档（由 #### 分隔），请构建对问题的回答。请确保答案准确、语气友好且有帮助。<br><br>####<br><br>####<br><br><quotes><br>- Chain-of-thought (CoT) prompting[27]<br>- Generated knowledge prompting[37]<br>- Least-to-most prompting[38]<br>- Self-consistency decoding[39]<br>- Complexity-based prompting[41]<br>- Self-refine[42]<br>- Tree-of-thought prompting[43]<br>- Maieutic prompting[45]<br>- Directional-stimulus prompting[46]<br>- Textual inversion and embeddings[59]<br>- Using gradient descent to search for prompts[61][62][63][64]<br>- Prompt injection[65][66][67]<br></quotes></td><td>文档中提到的提示技术包括：<br><br>1. Chain-of-thought (CoT) prompting[27]<br>2. Generated knowledge prompting[37]<br>3. Least-to-most prompting[38]<br>4. Self-consistency decoding[39]<br>5. Complexity-based prompting[41]<br>6. Self-refine[42]<br>7. Tree-of-thought prompting[43]<br>8. Maieutic prompting[45]<br>9. Directional-stimulus prompting[46]<br>10. Textual inversion and embeddings[59]<br>11. Using gradient descent to search for prompts[61][62][63][64]<br>12. Prompt injection[65][66][67]<br><br>每一种技术都采用独特的策略来增强或明确与大型语言模型的互动，以产生期望的结果。</td></tr></tbody></table><p>不仅仅通过手动设计 “多步骤的 prompt”，还可以使用 llm 自己提出解决问题的 prmot，这有点类似 Agent 的 Plan-and-Execute 技术，首先通过 llm 设计解决问题的子任务，通过一个个回答子任务，回答最终问题</p><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%96%87%E6%9C%AC%E7%B1%BBprompt%E7%9A%84%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97-20250115205954.png" alt="文本类prompt的编写指南-20250115205954|LtM过程"></p><h3 id="自洽性"><a class="markdownIt-Anchor" href="#自洽性"></a> 自洽性</h3><p>用思维链提示中的自洽解码替代简单的贪婪解码，即通过少样本思维链（CoT）提示采样多个、多样化的推理路径，并使用生成的结果来选择最一致的答案</p><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%8C%87%E4%BB%A4%E5%B7%A5%E7%A8%8BPromptEngineering-20241130115725.png" alt="指令工程PromptEngineering-20241130115725|自洽性"></p><h3 id="反射-reflexion"><a class="markdownIt-Anchor" href="#反射-reflexion"></a> 反射 Reflexion</h3><p>比 ReAct 更进一步的框架，通过更加复杂的 “自我反省” 规划模型下一步 prompt，如下图所示，如果将 “Evalustor-&gt;Self-reflection-&gt;Experience” 替换为普通 llm，也就是 ReAct 了</p><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E5%8F%8D%E5%B0%84Reflexion-20241214191453.png" alt="反射Reflexion-20241214191453"></p><p>Reflexion 过程的关键步骤是：a） 定义任务，b） 生成轨迹，c） 评估，d） 执行反射， e） 生成下一个轨迹</p><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E5%8F%8D%E5%B0%84Reflexion-20241214191831.png" alt="反射Reflexion-20241214191831"></p><p>Reflexion 最适合以下情况：</p><ol><li><strong>代理需要从反复试验中学习</strong>：Reflexion 旨在通过反思过去的错误并将这些知识纳入未来的决策来帮助代理提高绩效。这使得它非常适合代理需要通过反复试验来学习的任务，例如决策、推理和编程。</li><li><strong>传统的强化学习方法不切实际</strong>：传统的强化学习 （RL） 方法通常需要大量的训练数据和昂贵的模型微调。Reflexion 提供了一种轻量级的替代方案，不需要微调底层语言模型，使其在数据和计算资源方面更加高效。</li><li><strong>需要细致入微的反馈</strong>：Reflexion 利用口头反馈，这可能比传统 RL 中使用的标量奖励更细致、更具体。这使代理能够更好地了解其错误，并在后续试验中做出更有针对性的改进。</li><li><strong>可解释性和显式记忆很重要</strong>：与传统的 RL 方法相比，Reflexion 提供了一种更具可解释性和明确的情景记忆形式。代理的自我反思存储在其内存中，从而可以更轻松地分析和理解其学习过程。</li></ol><p>Reflexion 的一些限制：</p><ul><li><strong>依赖自我评估能力</strong>：Reflexion 依赖于代理准确评估其性能并生成有用的自我反思的能力。这可能具有挑战性，尤其是对于复杂的任务，但随着模型功能的不断改进，预计 Reflexion 会随着时间的推移而变得更好。</li><li><strong>长期内存约束</strong>：Reflexion 使用具有最大容量的滑动窗口，但对于更复杂的任务，使用高级结构（如向量嵌入或 SQL 数据库）可能是有利的。</li><li><strong>代码生成限制</strong>：测试驱动开发在指定准确的输入 - 输出映射（例如，非确定性生成器函数和受硬件影响的函数输出）方面存在限制。</li></ul><h3 id="主动提示-activeprompt"><a class="markdownIt-Anchor" href="#主动提示-activeprompt"></a> 主动提示 ActivePrompt</h3><p><strong>Active Prompting</strong>（或&nbsp;<strong>Active-Prompt</strong>）) 是一种通过选择性地对模型显示不确定性最大的示例进行人工注释来改进少样本思维链 FewShot-CoT 提示性能的技术。这种方法通过只关注模型最具挑战性的问题，帮助最大限度地提高人工注释工作的效率。</p><p>其实就是使用这个技术筛选 CoT 数据集中，推理步骤效果低下的数据，然后将这些样本挑选出来，通过人工重新修改，提示 CoT 数据集的数据质量，减少对所有训练数据进行注释的需要来节省大量人力资源</p><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E4%B8%BB%E5%8A%A8%E6%8F%90%E7%A4%BAActivePrompt-20241215083631.png" alt="主动提示ActivePrompt-20241215083631"></p><p>第一步是使用或不使用少量 CoT 示例查询 LLM。对一组训练问题生成&nbsp;<em>k</em>&nbsp;个可能的答案。基于&nbsp;<em>k</em>&nbsp;个答案计算不确定度度量。选择最不确定的问题由人类进行注释。然后使用新的注释范例来推断每个问题</p><h3 id="思维线索-thot"><a class="markdownIt-Anchor" href="#思维线索-thot"></a> 思维线索 ThoT</h3><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%80%9D%E7%BB%B4%E7%BA%BF%E7%B4%A2ThoT-20241216085152.png" alt="思维线索ThoT-20241216085152"></p><p>这个方法主要是通过特定 promp 多次使用 llm 提炼，解决混乱上下文导致输出鲜果不佳的问题</p><h3 id="自我询问提示-self-ask"><a class="markdownIt-Anchor" href="#自我询问提示-self-ask"></a> 自我询问提示 Self-Ask</h3><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E8%87%AA%E6%88%91%E8%AF%A2%E9%97%AE%E6%8F%90%E7%A4%BASelf-Ask-20241216090421.png" alt="自我询问提示Self-Ask-20241216090421"></p><p><strong>Self-Ask</strong>&nbsp;通过将复杂的问题分解为子问题并逐步回答它们来改进 LLM 推理，与思维链 （CoT） 提示类似，Self-Ask 将问题分解为一个循序渐进的过程。但是，与 CoT 不同的是，Self-Ask 会提示模型在回答提示中的主要问题之前明确说明下一个后续问题</p><p>使用的 prompt</p><figure class="highlight text"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Question: {A complex question} </span><br><span class="line">Are follow up questions needed here: Yes. </span><br><span class="line">Follow up: {Sub-question 1} </span><br><span class="line">Intermediate answer: {Correct answer to sub-question 1} </span><br><span class="line">Follow up: {Sub-question 2} </span><br><span class="line">Intermediate answer: {Correct answer to sub-question 2} </span><br><span class="line">So the final answer is: {Correct answer to the complex question} </span><br><span class="line">Question: {Your prompt with a complex question} Are follow up </span><br><span class="line">questions needed here:</span><br></pre></td></tr></tbody></table></figure><h3 id="自动思维链-auto-cot"><a class="markdownIt-Anchor" href="#自动思维链-auto-cot"></a> 自动思维链 (Auto-CoT)</h3><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%80%9D%E7%BB%B4%E9%93%BECoT-20241214165005.png" alt="思维链CoT-20241214165005"></p><p>上图是使用零样本思维链、少样本思维链的过程，这两个过程中，零样本思维链不一定能提出正确的推理步骤，少样本思维链依赖手工设计水平，于是提出 Auto-CoT 摆脱这两种方法的缺陷</p><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E6%80%9D%E7%BB%B4%E9%93%BECoT-20241214165224.png" alt="思维链CoT-20241214165224"></p><p>Auto-Cot 分为 3 步：</p><ol><li>对问题聚类：对数据集中的问题聚类，聚类的目的是类似的问题，有类似的推理步骤</li><li>使用 “零样本思维链” 回答每个聚类问题：将回答获取的推理步骤，作为这类问题的标准推理步骤</li><li>使用 “少样本思维链” 回答用户问题：回答问题时，将类似问题的推理步骤到 prompt 中</li></ol><p>总结：Auto-Cot 主要是优化推理步骤，是的步骤更符合用户的问题</p><h3 id="对比思维链-ccot"><a class="markdownIt-Anchor" href="#对比思维链-ccot"></a> 对比思维链 CCoT</h3><p>在少样本思维链 FewShot-CoT 的基础上新增负面样例，增强大模型回答能力</p><figure class="highlight text"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Question : James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a year?</span><br><span class="line">Explanation: He writes each friend 3*2=6 pages a week. So he writes 6*2=12 pages every week. That means he writes 12*52=624 pages a year.</span><br><span class="line">Wrong Explanation: He writes each friend 12*52=624 pages a week. So he writes 3*2=6 pages every week. That means he writes 6*2=12 pages a year.</span><br><span class="line">Question: James has 30 teeth. His dentist drills 4 of them and caps 7 more teeth than he drills. What percentage of James' teeth does the dentist fix?</span><br></pre></td></tr></tbody></table></figure><h3 id="表格思维链-tabularcot"><a class="markdownIt-Anchor" href="#表格思维链-tabularcot"></a> 表格思维链 TabularCot</h3><p>表格思维链提示类似于 Thread-of-Thought 提示，因为它使用不同的格式进行推理步骤。具体来说，它采用零镜头 Chain of Thought 提示，指示模型以结构化格式提供其推理，通常使用 Markdown 表格</p><figure class="highlight text"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Jackson is planting tulips. He can fit 6 red tulips in a row and 8 blue</span><br><span class="line">tulips in a row. If Jackson buys 36 red tulips and 24 blue tulips, how</span><br><span class="line">many rows of flowers will he plant?</span><br><span class="line"></span><br><span class="line">**|step|event|result|**</span><br></pre></td></tr></tbody></table></figure><h2 id="图谱提示"><a class="markdownIt-Anchor" href="#图谱提示"></a> 图谱提示</h2><h2 id="生成类提示-generate"><a class="markdownIt-Anchor" href="#生成类提示-generate"></a> 生成类提示 (Generate)</h2><h3 id="自动提示工程师-ape"><a class="markdownIt-Anchor" href="#自动提示工程师-ape"></a> 自动提示工程师 APE</h3><p>自动提示工程师 （APE），这是一个用于自动指令生成和选择的框架。通过 3 个 llm 扮演答案生成的：产出、打分、裁判</p><p>设计不同的问题，然后将不同的类似 <code>Let's think step by step</code> 添加到 prompt 中，然后使用 llm 评估添加不同这些 prompt 的回答问题的打分，选择打分高的添加内容</p><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E8%87%AA%E5%8A%A8%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%B8%88APE-20241214181852.png" alt="自动提示工程师APE-20241214181852"></p><p>比如 APE 发现 <code>Let's work this out in a step by step way to be sure we have the right answer.</code> 比 <code>Let's think step by step</code> 有更好的答案</p><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E8%87%AA%E5%8A%A8%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%B8%88APE-20241214181838.png" alt="自动提示工程师APE-20241214181838"></p><h3 id="生成知识提示"><a class="markdownIt-Anchor" href="#生成知识提示"></a> 生成知识提示</h3><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E7%9F%A5%E8%AF%86%E6%8F%90%E7%A4%BA-20241214171911.png" alt="知识提示-20241214171911"></p><p>生成知识通过 llm 补充 prompt 上下文不足的问题，让 llm 有更充足的信息回答问题，涉及两个过程</p><ul><li>知识生成 (Knowledge Generation)：基于问题，通过 llm 生成不同相关的知识</li><li>知识集成 (Knowledge Integration)：将问题和知识进行组合，生成新的 prompt, 给 llm 回答</li></ul><table><thead><tr><th>prompt</th><th>llm 输出</th></tr></thead><tbody><tr><td>问题：高尔夫球的一部分是试图获得比其他人更高的得分。是或否？<br><br>知识：高尔夫球的目标是以最少的杆数打完一组洞。一轮高尔夫球比赛通常包括 18 个洞。每个洞在标准高尔夫球场上一轮只打一次。每个杆计为一分，总杆数用于确定比赛的获胜者。<br><br>解释和答案：</td><td>不是，高尔夫球的目标不是获得比其他人更高的得分。相反，目标是以最少的杆数打完一组洞。总杆数用于确定比赛的获胜者，而不是总得分。</td></tr><tr><td>问题：高尔夫球的一部分是试图获得比其他人更高的得分。是或否？<br><br>知识：高尔夫球是一项精密的球杆运动，竞争选手（或高尔夫球手）使用许多类型的球杆，用最少的杆数将球打入球场上一系列洞中。目标是以最低的分数完成比赛，该分数是通过将每个洞上所用的杆数相加计算出来的。得分最低的选手赢得比赛。<br><br>解释和答案：</td><td>是的，高尔夫球的一部分是试图获得比其他人更高的得分。每个选手都试图以最少的杆数打完一组洞。总杆数用于确定比赛的获胜者，而不是总得分。得分最低的选手赢得比赛。</td></tr><tr><td>以上第一个例子，回答准确，第二个例子回答错误，所以说这个技术依赖于结合的知识</td><td></td></tr></tbody></table><h2 id="集成式提示"><a class="markdownIt-Anchor" href="#集成式提示"></a> 集成式提示</h2><h3 id="自动推理并使用工具-art"><a class="markdownIt-Anchor" href="#自动推理并使用工具-art"></a> 自动推理并使用工具 ART</h3><p>ART（Automatic Reasoning and Tool-use）在回答问题时，交替使用 Cot 提示和工具，也就是说回答问题时，不仅 llm 参与，工具也会进行计算，这很接近 Agent 的原理</p><p>ART 的工作原理</p><ul><li>接到一个新任务的时候，从任务库中选择多步推理和使用工具的示范。</li><li>在测试中，调用外部工具时，先暂停生成，将工具输出整合后继续接着生成</li></ul><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E8%87%AA%E5%8A%A8%E6%8E%A8%E7%90%86%E5%B9%B6%E4%BD%BF%E7%94%A8%E5%B7%A5%E5%85%B7ART-20241214180607.png" alt="自动推理并使用工具ART-20241214180607"></p><h2 id="其他"><a class="markdownIt-Anchor" href="#其他"></a> 其他</h2><h3 id="元提示-metaprompt"><a class="markdownIt-Anchor" href="#元提示-metaprompt"></a> 元提示 MetaPrompt</h3><p>无论是样本，还是思维链，都是在内容上引导模型回答更加准确，而元提示 (MetaPrompt) 则侧重于任务和问题的结构和语法方面，怎么说呢？以下是例子</p><p><img data-src="https://picgo-1304919305.cos.ap-guangzhou.myqcloud.com/picGo/%E5%85%83%E6%8F%90%E7%A4%BAMetaPrompt-20241214170444.png" alt="元提示MetaPrompt-20241214170444"></p><p>在 prompt 中，并没有直接提供样本，而是强调解题的框架，这是一种比样本更加抽象的提示。与样本提示相比，它有以下优势</p><ul><li>Token 优势：更抽象的概括，意味使用更少的 token，花费更低</li><li>自适应不同问题：减少设计样本的影响，可以自适应不同的提问</li></ul><p>MetaPrompt 的设计规则：</p><p><strong>1. 以结构为导向</strong>：优先考虑问题和解决方案的格式和模式，而不是具体的内容<br><strong>2. 以语法为中心</strong>：使用语法作为预期响应或解决方案的指导模板<br><strong>3. 抽象示例</strong>：使用抽象示例作为框架，说明问题和解决方案的结构，而不关注具体细节<br><strong>4. 多才多艺</strong>：适用于各个领域，能够为各种问题提供结构化的响应<br><strong>5. 分类方法</strong>：借鉴类型论，强调提示中组件的分类和逻辑排列</p></div><footer class="post-footer"><div class="reward-container"><div>请我一杯咖啡吧！</div><button>赞赏</button><div class="post-reward"><div><img src="/images/wechatpay.png" alt="Shaogui 微信"> <span>微信</span></div><div><img src="/images/alipay.png" alt="Shaogui 支付宝"> <span>支付宝</span></div></div></div><div class="post-copyright"><ul><li class="post-copyright-author"><strong>本文作者： </strong>Shaogui</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://www.shaogui.life/posts/2007481097.html" title="文本类 prompt 编写指南">https://www.shaogui.life/posts/2007481097.html</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><span>欢迎关注我的其它发布渠道</span><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="https://www.zhihu.com/people/mu-zhi-zhi-tian"><span class="icon"><i class="fab fa-zhihu"></i> </span><span class="label">知乎</span></a></div><div class="social-item"><a target="_blank" class="social-link" href="/atom.xml"><span class="icon"><i class="fa fa-rss"></i> </span><span class="label">RSS</span></a></div></div></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/376669125.html" rel="prev" title="认识大模型"><i class="fa fa-angle-left"></i> 认识大模型</a></div><div class="post-nav-item"><a href="/posts/2856492008.html" rel="next" title="贪婪柱搜索 beam_search">贪婪柱搜索 beam_search <i class="fa fa-angle-right"></i></a></div></div></footer></article></div><div class="comments utterances-container"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2016 – <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Shaogui</span></div><div class="wordcount"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-chart-line"></i> </span><span>站点总字数：</span> <span title="站点总字数">1.9m</span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span>站点阅读时长 &asymp;</span> <span title="站点阅读时长">28:48</span></span></div><div class="busuanzi-count"><span class="post-meta-item" id="busuanzi_container_site_uv"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-item" id="busuanzi_container_site_pv"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div><div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动</div></div></footer><div class="toggle sidebar-toggle" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div><div class="sidebar-dimmer"></div><div class="reading-progress-bar"></div><a href="https://github.com/WuShaogui" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><noscript><div class="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script><script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script><script src="/js/third-party/search/local-search.js"></script><script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"neutral","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script><script src="/js/third-party/tags/mermaid.js"></script><script src="/js/third-party/pace.js"></script><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script><script src="/js/third-party/math/mathjax.js"></script><script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"WuShaogui/wushaogui.github.io","issue_term":"pathname","theme":"github-light"}</script><script src="/js/third-party/comments/utterances.js"></script></body></html>