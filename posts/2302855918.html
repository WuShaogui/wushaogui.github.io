<!DOCTYPE html><html lang="zh-CN"><head><style type="text/css">.douban-card-block{display:flex;justify-content:center;align-items:center;width:100%;max-height:400px}.douban-card{display:flex;margin:30px 10px;padding:15px;border-radius:15px;position:relative;justify-content:center;align-items:center;overflow:hidden;color:#faebd7;text-decoration:none}.douban-card:hover{text-decoration:none}.douban-card-bgimg{position:absolute;width:115%;height:115%;filter:blur(15px) brightness(.6);background-size:100%;background-position:center;background-repeat:no-repeat}.douban-card-img{position:relative;height:130px;width:80px;background-size:100%;background-position:center;background-repeat:no-repeat}.douban-card-left:hover .douban-card-img{filter:blur(5px) brightness(.6);transform:perspective(800px) rotateX(180deg)}.douban-card-left .douban-card-img{transition:all .5s ease}.douban-card-left{position:relative;display:flex;flex-direction:column;align-items:center}.douban-card-left .douban-card-status{height:130px;width:80px;text-align:center;font-weight:700;position:absolute;left:0;top:30%;transform:rotateX(180deg);backface-visibility:hidden;transition:all .5s ease}.douban-card-left:hover .douban-card-status{transform:perspective(800px) rotateX(0)}.douban-card-right{position:relative;display:flex;flex-direction:column;margin-left:12px;font-size:16px;font-family:"Courier New",Courier,monospace;line-height:1.3;color:#faebd7}.douban-card-item{margin-top:4px}</style><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta name="theme-color" content="#222" media="(prefers-color-scheme: light)"><meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=Roboto+Mono:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=PT+Mono:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script><script class="next-config" data-name="main" type="application/json">{"hostname":"www.shaogui.life","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.22.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"flat","show_result":true},"fold":{"enable":true,"height":200},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script><meta property="og:type" content="article"><meta property="og:title" content="02-TensorRT 开发者指南"><meta property="og:url" content="https://www.shaogui.life/posts/2302855918.html"><meta property="og:site_name" content="年轻人起来冲"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2023-03-17T10:46:48.000Z"><meta property="article:modified_time" content="2025-01-19T06:22:14.089Z"><meta property="article:author" content="Shaogui"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://www.shaogui.life/posts/2302855918.html"><script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://www.shaogui.life/posts/2302855918.html","path":"posts/2302855918.html","title":"02-TensorRT 开发者指南"}</script><script class="next-config" data-name="calendar" type="application/json">""</script><title>02-TensorRT 开发者指南 | 年轻人起来冲</title><noscript><link rel="stylesheet" href="/css/noscript.css"></noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><link rel="alternate" href="/atom.xml" title="年轻人起来冲" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage" class="use-motion"><div class="headband"></div><main class="main"><div class="column"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><p class="site-title">年轻人起来冲</p><i class="logo-line"></i></a></div><div class="site-nav-right"><div class="toggle popup-trigger" aria-label="搜索" role="button"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">77</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">70</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">544</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" maxlength="80" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close" role="button"><i class="fa fa-times-circle"></i></span></div><div class="search-result-container"><div class="search-result-icon"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></header><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="sidebar-panel-container"><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8tensort%E5%B8%A6%E6%9D%A5%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="nav-number">1.</span> <span class="nav-text">使用 TensoRT 带来的优势？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9B%BF%E4%BB%A3tensorrt%E7%9A%84%E6%96%B9%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B"><span class="nav-number">2.</span> <span class="nav-text">替代 TensorRT 的方法有哪些？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensorrt%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84"><span class="nav-number">3.</span> <span class="nav-text">TensorRT 是如何工作的？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensorrt%E7%9A%84%E6%A0%B8%E5%BF%83%E5%8A%9F%E8%83%BD"><span class="nav-number">4.</span> <span class="nav-text">TensorRT 的核心功能？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensorrt%E7%9A%84-%E7%89%88%E6%9C%AC%E5%8F%B7majorminorpatch%E8%AF%B4%E6%98%8E"><span class="nav-number">5.</span> <span class="nav-text">TensorRT 的 版本号 (MAJOR.MINOR.PATCH) 说明？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensorrt%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E8%A7%A3%E6%9E%90%E5%99%A8ibuilder%E5%88%9B%E5%BB%BA%E7%BD%91%E7%BB%9C%E5%AE%9A%E4%B9%89"><span class="nav-number">6.</span> <span class="nav-text">TensorRT 如何使用解析器 (IBuilder) 创建网络定义？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensorrt%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E6%9E%84%E5%BB%BA%E5%99%A8%E5%88%9B%E5%BB%BA%E5%BC%95%E6%93%8E"><span class="nav-number">7.</span> <span class="nav-text">TensorRT 如何使用构建器创建引擎？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%8A%A0%E5%BF%ABtensorrt%E6%9E%84%E5%BB%BA%E5%99%A8%E7%9A%84%E6%9E%84%E5%BB%BA%E6%97%B6%E9%97%B4"><span class="nav-number">8.</span> <span class="nav-text">如何加快 TensorRT 构建器的构建时间？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensorrt%E5%A6%82%E4%BD%95%E5%BA%8F%E5%88%97%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="nav-number">9.</span> <span class="nav-text">TensorRT 如何序列化模型？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96tensorrt%E5%BC%95%E6%93%8E%E5%B9%B6%E6%89%A7%E8%A1%8C%E6%8E%A8%E7%90%86"><span class="nav-number">10.</span> <span class="nav-text">如何反序列化 TensorRT 引擎并执行推理？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%A8tensorrt%E4%B8%AD%E5%A6%82%E4%BD%95%E5%AF%B9%E6%98%BE%E5%AD%98%E8%BF%9B%E8%A1%8C%E7%AE%A1%E7%90%86"><span class="nav-number">11.</span> <span class="nav-text">在 TensorRT 中如何对显存进行管理？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%A8tensorrt%E4%B8%AD%E5%A6%82%E4%BD%95%E5%AE%9A%E5%88%B6%E5%8C%96%E6%9D%83%E9%87%8D"><span class="nav-number">12.</span> <span class="nav-text">在 TensorRT 中如何定制化权重？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8c%E8%AE%BE%E7%BD%AEtensorrt%E7%9A%84tf32%E6%8E%A8%E7%90%86%E6%A8%A1%E5%BC%8F"><span class="nav-number">13.</span> <span class="nav-text">如何使用 C++ 设置 TensorRT 的 TF32 推理模式 ？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8c%E8%AE%BE%E7%BD%AEtensorrt%E7%9A%84fp16%E6%8E%A8%E7%90%86%E6%A8%A1%E5%BC%8F"><span class="nav-number">14.</span> <span class="nav-text">如何使用 C++ 设置 TensorRT 的 FP16 推理模式 ？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8c%E8%AE%BE%E7%BD%AEtensorrt%E7%9A%84int8%E6%8E%A8%E7%90%86%E6%A8%A1%E5%BC%8F"><span class="nav-number">15.</span> <span class="nav-text">如何使用 C++ 设置 TensorRT 的 INT8 推理模式 ？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensort%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%8A%A8%E6%80%81shape"><span class="nav-number">16.</span> <span class="nav-text">TensoRT 如何使用动态 shape？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E6%9E%9C%E5%9C%A8%E4%B8%80%E4%B8%AAgpu%E4%B8%8A%E6%9E%84%E5%BB%BA%E5%BC%95%E6%93%8E%E5%B9%B6%E5%9C%A8%E5%8F%A6%E4%B8%80%E4%B8%AAgpu%E4%B8%8A%E8%BF%90%E8%A1%8C%E5%BC%95%E6%93%8E%E8%BF%99%E5%8F%AF%E8%A1%8C%E5%90%97"><span class="nav-number">17.</span> <span class="nav-text">如果在一个 GPU 上构建引擎，并在另一个 GPU 上运行引擎，这可行吗？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorrt%E5%9C%A8%E5%A4%9A%E4%B8%AAgpu%E4%B8%8Amutiple-gpus"><span class="nav-number">18.</span> <span class="nav-text">如何使用 TensorRT 在多个 GPU 上 (mutiple GPUs)？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3ibuilderconfig%E7%9A%84setmaxworkspacesize"><span class="nav-number">19.</span> <span class="nav-text">如何理解 IBuilderConfig 的 setMaxWorkspaceSize？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E6%9C%80%E4%BD%B3%E7%9A%84%E5%B7%A5%E4%BD%9C%E7%A9%BA%E9%97%B4%E5%A4%A7%E5%B0%8F-workspace-size"><span class="nav-number">20.</span> <span class="nav-text">如何选择最佳的工作空间大小 (workspace size)？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensorrt%E7%9A%84engine%E5%92%8C%E6%A0%A1%E5%87%86%E8%A1%A8%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E8%B7%A8tensorrt%E7%89%88%E6%9C%AC"><span class="nav-number">21.</span> <span class="nav-text">TensorRT 的 engine 和校准表是否可以跨 TensorRT 版本？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E9%92%88%E5%AF%B9%E5%87%A0%E7%A7%8D%E4%B8%8D%E5%90%8C%E6%89%B9%E9%87%8F%E4%BC%98%E5%8C%96%E7%9A%84%E5%BC%95%E6%93%8E"><span class="nav-number">22.</span> <span class="nav-text">如何创建一个针对几种不同批量优化的引擎？</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Shaogui" src="/images/avatar-2023.png"><p class="site-author-name" itemprop="name">Shaogui</p><div class="site-description" itemprop="description">害怕失败是本能，勇敢面对才是本事</div></div><div class="site-state-wrap animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">544</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">70</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">77</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author animated"><span class="links-of-author-item"><a href="https://github.com/WuShaogui" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;WuShaogui" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/mu-zhi-zhi-tian" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;mu-zhi-zhi-tian" rel="noopener me" target="_blank"><i class="fab fa-zhihu fa-fw"></i>知乎</a></span></div><div class="cc-license animated" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a></div></div></div><div class="back-to-top animated" role="button" aria-label="返回顶部"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div><div class="sidebar-inner sidebar-blogroll"><div class="links-of-blogroll animated"><div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i> 链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://manaai.cn/" title="http:&#x2F;&#x2F;manaai.cn&#x2F;" rel="noopener" target="_blank">神力AI</a></li></ul></div></div></aside></div><div class="main-inner post posts-expand"><div class="post-block"><article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.shaogui.life/posts/2302855918.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar-2023.png"><meta itemprop="name" content="Shaogui"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="年轻人起来冲"><meta itemprop="description" content="害怕失败是本能，勇敢面对才是本事"></span><span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork"><meta itemprop="name" content="02-TensorRT 开发者指南 | 年轻人起来冲"><meta itemprop="description" content=""></span><header class="post-header"><h1 class="post-title" itemprop="name headline">02-TensorRT 开发者指南</h1><div class="post-meta-container"><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2023-03-17 18:46:48" itemprop="dateCreated datePublished" datetime="2023-03-17T18:46:48+08:00">2023-03-17</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2025-01-19 14:22:14" itemprop="dateModified" datetime="2025-01-19T14:22:14+08:00">2025-01-19</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">2-深度学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/D-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%83%A8%E7%BD%B2/" itemprop="url" rel="index"><span itemprop="name">D-深度学习部署</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/D-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%83%A8%E7%BD%B2/TensorRT/" itemprop="url" rel="index"><span itemprop="name">TensorRT</span></a> </span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv"><span class="post-meta-item-icon"><i class="far fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-break"></span> <span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>8k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>7 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><ul><li></li></ul><span id="more"></span><h3 id="使用tensort带来的优势"><a class="markdownIt-Anchor" href="#使用tensort带来的优势"></a> 使用 TensoRT 带来的优势？</h3><ul><li><strong>吞吐量：</strong> 单位时间内的推理数量更多</li><li><strong>功率：</strong> 单位功率的吞吐率更高</li><li><strong>推理时间：</strong> 毫秒级的推理时间</li><li><strong>准确性：</strong> 训练过的神经网络可以提供正确的结果</li></ul><h3 id="替代tensorrt的方法有哪些"><a class="markdownIt-Anchor" href="#替代tensorrt的方法有哪些"></a> 替代 TensorRT 的方法有哪些？</h3><ul><li>使用训练框架本身进行推理：需占用更多的 GPU，训练框架倾向于高效的训练，而不是推断</li><li>专门设计用于执行自定义应用程序网络使用的低级库和数学运算：工作量大，兼容性不强</li></ul><h3 id="tensorrt是如何工作的"><a class="markdownIt-Anchor" href="#tensorrt是如何工作的"></a> TensorRT 是如何工作的？</h3><ul><li><strong>构建阶段：</strong> 根据已经训练好的网络，执行特定的网络、特定平台的优化，生成推理引擎的阶段</li><li><strong>非跨平台：</strong> 生成的推理引擎不能跨平台或 TensorRT 版本移植。计划特定于构建它们的确切 GPU 模型（除了平台和 TensorRT 版本），并且必须重新针对特定 GPU，以防您想在不同的 GPU 上运行它们</li><li><strong>优化概述：</strong> 通过消除死计算、折叠来优化网络图常量，以及重新排序和组合操作以更有效地运行图形处理器</li><li><strong>选择精度：</strong> 自动将 32 位浮点计算减少到 16 位并支持浮点值的量化 ，甚至是 8 位整数</li></ul><h3 id="tensorrt的核心功能"><a class="markdownIt-Anchor" href="#tensorrt的核心功能"></a> TensorRT 的核心功能？</h3><ul><li><strong>网络定义：</strong> 网络定义接口为应用程序提供定义网络的方法。可以指定输入和输出张量，并可以添加和配置层</li><li><strong>优化配置文件：</strong> 优化配置文件指定动态维度上的约束</li><li><strong>构建器配置：</strong> 构建器配置接口指定创建引擎的详细信息。它允许应用程序指定优化配置文件，最大工作空间大小，最低可接受的精度水平，定时迭代计数的自动调整，和一个接口量化网络运行在 8 位精度</li><li><strong>构建器：</strong> 接口允许从网络定义和构建器配置<strong>创建</strong>优化的引擎</li><li><strong>引擎：</strong> Engine 接口允许应用程序执行推理。它支持同步和异步执行、分析和枚举，以及查询引擎输入和输出的绑定。一个单引擎可以有多个执行上下文，允许使用一组训练过的参数同时执行多个推理</li><li><strong>ONNX 解析器：</strong> 此解析器可用于解析 ONNX</li></ul><h3 id="tensorrt的-版本号majorminorpatch说明"><a class="markdownIt-Anchor" href="#tensorrt的-版本号majorminorpatch说明"></a> TensorRT 的 版本号 (MAJOR.MINOR.PATCH) 说明？</h3><ul><li>MAJOR：在进行不兼容的 API 或 ABI 时，主要版本会发生更改</li><li>MINOR：以向后兼容的方式添加功能时的 MINOR 版本</li><li>PATCH：补丁版本时进行向后兼容的错误修复</li></ul><h3 id="tensorrt如何使用解析器ibuilder创建网络定义"><a class="markdownIt-Anchor" href="#tensorrt如何使用解析器ibuilder创建网络定义"></a> TensorRT 如何使用解析器 (IBuilder) 创建网络定义？</h3><ul><li>由于 TensorRT 7.0，ONNX 解析器只支持全维模式，这意味着必须使用 explicitBatch 拆分批处理</li><li>创建构建器和网络定义<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">IBuilder* builder = createInferBuilder(gLogger);</span><br><span class="line"><span class="type">const</span> <span class="keyword">auto</span> explicitBatch = <span class="number">1U</span> &lt; &lt; static_cast(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH);  </span><br><span class="line">INetworkDefinition* network = builder-&gt;createNetworkV2(explicitBatch);</span><br></pre></td></tr></tbody></table></figure></li><li>创建 ONNX 解析器并解析模型<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">nvonnxparser::IParser* parser = </span><br><span class="line">nvonnxparser::createParser(*network, gLogger);</span><br><span class="line">parser-&gt;parseFromFile(onnx_filename, ILogger::Severity::kWARNING);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; parser.getNbErrors(); ++i)</span><br><span class="line">{</span><br><span class="line">	<span class="built_in">std</span>::<span class="built_in">cout</span> &lt; &lt; parser-&gt;getError(i)-&gt;desc() &lt; &lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure></li></ul><h3 id="tensorrt如何使用构建器创建引擎"><a class="markdownIt-Anchor" href="#tensorrt如何使用构建器创建引擎"></a> TensorRT 如何使用构建器创建引擎？</h3><ul><li>生成器 IBuilderConfig 有许多属性，一个特别重要的属性是最大工作空间大小</li><li>使用 builder 对象构建引擎<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">IBuilderConfig* config = builder-&gt;createBuilderConfig();</span><br><span class="line"><span class="comment">// // 构建器（builder）的工作空间，越大表示占用GPU越多，构建速度越快</span></span><br><span class="line">config-&gt;setMaxWorkspaceSize(<span class="number">16</span>_MiB);</span><br><span class="line"><span class="comment">// 当引擎构造完成时，TensorRT会复制重量</span></span><br><span class="line">ICudaEngine* engine = builder-&gt;buildEngineWithConfig(*network, *config);</span><br></pre></td></tr></tbody></table></figure></li><li>销毁解析器、网络定义、配置、构建器<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">parser-&gt;destroy();</span><br><span class="line">network-&gt;destroy();</span><br><span class="line">config-&gt;destroy();</span><br><span class="line">builder-&gt;destroy();</span><br></pre></td></tr></tbody></table></figure></li></ul><h3 id="如何加快tensorrt构建器的构建时间"><a class="markdownIt-Anchor" href="#如何加快tensorrt构建器的构建时间"></a> 如何加快 TensorRT 构建器的构建时间？</h3><ul><li>onnx-&gt;trt 这一过程是构建器（builder）为每一层计算获选内核的过程，试想 onnx 是一个层级的算子序列<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mn>1</mn><mo>=</mo><mo stretchy="false">{</mo><mi>A</mi><mo separator="true">,</mo><mi>B</mi><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mo>…</mo><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">L1=\{A,B,C ,…\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal">L</span><span class="mord">1</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">{</span><span class="mord mathnormal">A</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.05017em">B</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner">…</span><span class="mclose">}</span></span></span></span>，每层算子对应 1 个或多个 TensorRT 算子<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mn>2</mn><mo>=</mo><mo stretchy="false">{</mo><msub><mi>A</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>A</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>A</mi><mn>3</mn></msub><mo separator="true">,</mo><mo>…</mo><mo stretchy="false">}</mo><mo separator="true">,</mo><mo stretchy="false">{</mo><msub><mi>B</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>B</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>B</mi><mn>3</mn></msub><mo separator="true">,</mo><mo>…</mo><mo stretchy="false">}</mo><mo separator="true">,</mo><mo>…</mo></mrow><annotation encoding="application/x-tex">L2=\{A_1,A_2,A_3,…\},\{B_1,B_2,B_3,…\},…</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal">L</span><span class="mord">2</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner">…</span><span class="mclose">}</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:.05017em">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.05017em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05017em">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.05017em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05017em">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.05017em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner">…</span><span class="mclose">}</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner">…</span></span></span></span>，这些算子的速度与硬件参数相关，为了构建最快的引擎，需要遍历<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mn>1</mn><mo separator="true">,</mo><mi>L</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">L1,L2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8777699999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal">L</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">L</span><span class="mord">2</span></span></span></span> 列表组合这些算子，统计其运行时间，最终获得最快的算子组合，所以这个过程尤其耗时</li><li>从 TensorRT 8.0 开始，运行在拥有相同 CUDA 设备属性和 CUDA/TensorRT 版本的设备上的构建器实例可以序列化和加载时间缓存</li><li>程序<ul><li>创建一个新的空计时缓存<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">IBuilderConfig* config = builder-&gt;createBuilderConfig();</span><br><span class="line">ITimingCache* cache = config-&gt;createTimingCache(cacheFile.data(), cacheFile.size());</span><br></pre></td></tr></tbody></table></figure></li><li>将全局计时缓存附加到构建器<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 在拥有不同 CUDA 设备属性的设备之间共享缓存可能会导致功能/性能问题。建议是只有当设备型号相同但 UUID 不同时才这样做。但是，加载的缓存必须由与当前生成器实例相同的 TensorRT 版本生成</span></span><br><span class="line">config-&gt;setTimingCache(*cache, <span class="literal">false</span>);</span><br></pre></td></tr></tbody></table></figure></li><li>序列化全局计时缓存<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IHostMemory* serializedCache = cache-&gt;serialize();</span><br></pre></td></tr></tbody></table></figure></li><li>设置生成器标志来关闭全局 / 本地缓存<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config-&gt;setFlag(BuilderFlag::kDISABLE_TIMING_CACHE);</span><br></pre></td></tr></tbody></table></figure></li></ul></li></ul><h3 id="tensorrt如何序列化模型"><a class="markdownIt-Anchor" href="#tensorrt如何序列化模型"></a> TensorRT 如何序列化模型？</h3><ul><li>序列化引擎不能跨平台移植 TensorRT 版本，如，tensorrt8 序列化的引擎不能在 tensorrt7 上使用</li><li>不仅 tensorrt 的版本问题，高版本的 TensorRT 依赖于高版本的 CUDA 版本，而高版本的 CUDA 版本依赖于高版本的驱动，如果想要使用新版本的 TensorRT，更换环境是不可避免的；</li><li>程序<ul><li>运行离线状态的构建器，然后序列化模型<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">IHostMemory *serializedModel = engine-&gt;serialize();</span><br><span class="line"><span class="comment">// store model to disk</span></span><br><span class="line"><span class="comment">// &lt;…&gt;</span></span><br><span class="line">serializedModel-&gt;destroy();</span><br></pre></td></tr></tbody></table></figure></li><li>创建运行时用于反序列化<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">IRuntime* runtime = createInferRuntime(gLogger);</span><br><span class="line">ICudaEngine* engine = runtime-&gt;deserializeCudaEngine(modelData, modelSize, nullptr);</span><br></pre></td></tr></tbody></table></figure></li></ul></li></ul><h3 id="如何反序列化tensorrt引擎并执行推理"><a class="markdownIt-Anchor" href="#如何反序列化tensorrt引擎并执行推理"></a> 如何反序列化 TensorRT 引擎并执行推理？</h3><ul><li>创建执行上下文来存储中间激活值<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 一个引擎可以有多个执行上下文，允许为多个重叠推理任务使用一组权重。例如，您可以在并行 CUDA 流中使用一个引擎和一个上下文来处理图像。每个上下文都是在与引擎相同的 GPU 上创建的</span></span><br><span class="line">IExecutionContext *context = engine-&gt;createExecutionContext();</span><br></pre></td></tr></tbody></table></figure></li><li>使用输入和输出 blob 名称来获得相应的输入和输出索引<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> inputIndex = engine-&gt;getBindingIndex(INPUT_BLOB_NAME);</span><br><span class="line"><span class="type">int</span> outputIndex = engine-&gt;getBindingIndex(OUTPUT_BLOB_NAME);</span><br></pre></td></tr></tbody></table></figure></li><li>设置一个指向 GPU 上输入和输出缓冲区的缓冲区数组<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span>* buffers[<span class="number">2</span>];</span><br><span class="line">buffers[inputIndex] = inputBuffer;</span><br><span class="line">buffers[outputIndex] = outputBuffer;</span><br></pre></td></tr></tbody></table></figure></li><li>执行推理<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 异步推理</span></span><br><span class="line">context-&gt;enqueueV2(buffers, stream, nullptr);</span><br><span class="line"><span class="comment">// 同步推理</span></span><br><span class="line"><span class="type">bool</span> status = context-&gt;executeV2(buffers.getDeviceBindings().data())</span><br><span class="line"><span class="comment">// 从同一个 IExecutionContext 对象调用 enqueue 或 enqueueV2，并同时调用不同 CUDA 数据流，将导致未定义行为/值。为了在多个 CUDA 流中并发执行推理，每个 CUDA 流使用一个 IExecutionContext</span></span><br></pre></td></tr></tbody></table></figure></li></ul><h3 id="在tensorrt中如何对显存进行管理"><a class="markdownIt-Anchor" href="#在tensorrt中如何对显存进行管理"></a> 在 TensorRT 中如何对显存进行管理？</h3><ul><li>创建运行上下文时设置―<ul><li>默认情况下，在创建 IExecutionContext 时，将分配持久设备内存来保存激活数据</li><li>要避免这种分配，请调用 createexecutioncontextwithoutdevicemory。然后，应用程序负责调用 IExecutionContext: : setdevicemory () ，以提供运行网络所需的内存。内存块的大小由 ICudaEngine: : getdevicemorysize () 返回</li></ul></li><li>通过实现 IGpuAllocator 接口―<ul><li>应用程序可以提供定制的分配器，以便在构建和运行时使用。如果您的应用程序希望控制所有的 GPU 内存并将子分配给 TensorRT，而不是直接从 CUDA 分配 TensorRT</li><li>接口实现后，在 IBuilder 或 IRuntime 接口上调用 setGpuAllocator (&amp; allocator)。然后通过这个接口分配和释放所有设备内存</li></ul></li></ul><h3 id="在tensorrt中如何定制化权重"><a class="markdownIt-Anchor" href="#在tensorrt中如何定制化权重"></a> 在 TensorRT 中如何定制化权重？</h3><ul><li>可以为发动机重新装配新的重量，而不需要重新装配</li><li>程序<ul><li>在建造之前请求一个可定制权重的引擎<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">config-&gt;setFlag(BuilderFlag::kREFIT) </span><br><span class="line">builder-&gt;buildEngineWithConfig(network, config);</span><br></pre></td></tr></tbody></table></figure></li><li>创建一个 reftter 对象<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ICudaEngine* engine = ...;</span><br><span class="line">IRefitter* refitter = createInferRefitter(*engine,gLogger)</span><br></pre></td></tr></tbody></table></figure></li><li>通过层名更新权重<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Weights newWeights = ...;</span><br><span class="line">refitter-&gt;setWeights(<span class="string">"MyLayer"</span>,WeightsRole::kKERNEL,</span><br><span class="line">					newWeights);</span><br></pre></td></tr></tbody></table></figure></li><li>通过权重名更新权重<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Weights newWeights = ...;</span><br><span class="line">refitter-&gt;setNamedWeights(<span class="string">"MyWeights"</span>, newWeights);</span><br></pre></td></tr></tbody></table></figure></li><li>获取必须提供的权重<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取层名</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> n = refitter-&gt;getMissing(<span class="number">0</span>, nullptr, nullptr);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span> <span class="title function_">layerNames</span><span class="params">(n)</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span> <span class="title function_">weightsRoles</span><span class="params">(n)</span>;</span><br><span class="line">refitter-&gt;getMissing(n, layerNames.data(), </span><br><span class="line">						weightsRoles.data());</span><br><span class="line"><span class="comment">// 获取权重名              </span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> n = refitter-&gt;getMissingWeights(<span class="number">0</span>, nullptr);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span> <span class="title function_">weightsNames</span><span class="params">(n)</span>;</span><br><span class="line">refitter-&gt;getMissingWeights(n, weightsNames.data());         </span><br></pre></td></tr></tbody></table></figure></li><li>按照任意顺序提供权重<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; ++i)</span><br><span class="line">	refitter-&gt;setWeights(layerNames[i], weightsRoles[i],</span><br><span class="line">						 Weights{...});</span><br></pre></td></tr></tbody></table></figure></li><li>使用提供的所有权重更新引擎<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">bool</span> success = refitter-&gt;refitCudaEngine();</span><br><span class="line">assert(success);</span><br></pre></td></tr></tbody></table></figure></li><li>销毁定制器<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">refitter-&gt;destroy();</span><br></pre></td></tr></tbody></table></figure></li></ul></li></ul><h3 id="如何使用c设置tensorrt的tf32推理模式"><a class="markdownIt-Anchor" href="#如何使用c设置tensorrt的tf32推理模式"></a> 如何使用 C++ 设置 TensorRT 的 TF32 推理模式 ？</h3><ul><li>默认情况下，TensorRT 允许使用 TF32 TensoRT 内核</li><li>使用 TF32 TensoRT 内核通常不会丢失精度，比 FP16 更稳定，但需要更精细的权重，其引擎序列化后更大，推理时间更长<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 检查平台是否支持TF32</span></span><br><span class="line"><span class="type">bool</span> hasTf32 = builder-&gt;platformHasTf32()</span><br><span class="line"><span class="comment">// 设置TensoRT使用TF32</span></span><br><span class="line">config-&gt;setFlag(BuilderFlag::kTF32);</span><br><span class="line"><span class="comment">// 清除</span></span><br><span class="line">config-&gt;clearFlag(BuilderFlag::kTF32);</span><br></pre></td></tr></tbody></table></figure></li></ul><h3 id="如何使用c设置tensorrt的fp16推理模式"><a class="markdownIt-Anchor" href="#如何使用c设置tensorrt的fp16推理模式"></a> 如何使用 C++ 设置 TensorRT 的 FP16 推理模式 ？</h3><ul><li><figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  设置TensoRT使用FP16</span></span><br><span class="line">config-&gt;setFlag(BuilderFlag::kFP16);</span><br><span class="line"><span class="comment">// 不保证在构建引擎时使用16位内核，使用以下标志强制16位精度</span></span><br><span class="line">config-&gt; setFlag（BuilderFlag :: kSTRICT_TYPES）</span><br></pre></td></tr></tbody></table></figure></li><li><strong>注意：</strong> 唯一具有<strong>全速率 FP16 性能的 GPU 是 Tesla P100、Quadro GP100 和 Jetson TX1/TX2， 不支持</strong>全速率 **FP16，所以，在这些型号中使用 fp16 精度反而比 fp32 慢</li></ul><h3 id="如何使用c设置tensorrt的int8推理模式"><a class="markdownIt-Anchor" href="#如何使用c设置tensorrt的int8推理模式"></a> <strong>如何使用 C++ 设置 TensorRT 的 INT8 推理模式 ？</strong></h3><ul><li>为了执行 INT8 推断，需要对 FP32 激活张量和权重进行量化，为了表示 32 位浮点值和 INT 8 位量化值，TensorRT 需要了解每个激活张量的动态范围。动态范围用于确定适当的量化比例</li><li>设置构建器标志可启用 INT8 精度推断<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config-&gt; setFlag（BuilderFlag :: kINT8）;</span><br></pre></td></tr></tbody></table></figure></li><li>两种方式向网络提供动态范围<ul><li>使用 setDynamicRange API 手动设置每个网络张量的动态范围<figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ITensor *tensor = network-&gt; getLayer（layer_index）-&gt; getOutput（output_index）;</span><br><span class="line">tensor -&gt; setDynamicRange（min_float，max_float）;</span><br></pre></td></tr></tbody></table></figure></li><li>使用 INT8 校准使用校准数据集生成每个张量动态范围</li></ul></li></ul><h3 id="tensort如何使用动态shape"><a class="markdownIt-Anchor" href="#tensort如何使用动态shape"></a> TensoRT 如何使用动态 shape？</h3><ul><li>动态形状是将指定某些或所有张量维度推迟到运行时的能力</li><li>构建动态 shape 的引擎的步骤<ul><li>网络必须被定义为隐式批处理大小</li><li>使用 - 1 表示不确定的维度</li><li>指定一个或多个优化配置文件在构建时，为带有运行时维度的输入指定允许的维度范围，以及自动调谐器应该优化的维度</li></ul></li><li>使用带动态 shape 的引擎？<ul><li>从引擎创建执行上下文，与没有动态形状相同</li><li>在运行时，您需要在设置输入尺寸之前设置优化配置文件，配置文件按照添加的顺序编号，从开始 0// 使用 executeV2 推理时 context-&gt;setOptimizationProfile (0); // 设置推断输入尺寸</li><li>指定执行上下文的输入维度。设置输入尺寸后，您可以获得 TensorRT 计算给定的输入尺寸 // 0 表示输入节点的 bindingIndex context.setBindingDimensions (0, Dims3 (3, 150, 250))</li><li>开始推理</li></ul></li></ul><h3 id="如果在一个gpu上构建引擎并在另一个gpu上运行引擎这可行吗"><a class="markdownIt-Anchor" href="#如果在一个gpu上构建引擎并在另一个gpu上运行引擎这可行吗"></a> 如果在一个 GPU 上构建引擎，并在另一个 GPU 上运行引擎，这可行吗？</h3><ul><li>建议不要；但是，如果这样做了，需要遵循以下准则</li><li>TensoRT 的 major, minor, patch 3 个层次的版本号必须和系统匹配，以确保 TensoRT core 存在</li><li>CUDA 的 major, minor 2 个层次的版本号必须和系统匹配，以确保 相同的硬件特性存在，因此内核不会无法执行</li><li>软件设备参数需要匹配， 如果任何属性不匹配，将收到警告―<ul><li>最大 GPU 时钟速度 (Maximum GPU graphics clock speed)</li><li>最大 GPU 内存时钟速度 (Maximum GPU memory clock speed)</li><li>GPU 内存总线宽度 (GPU memory bus width)</li><li>总 GPU 内存 (Total GPU memory)</li><li>GPUL2 缓存大小 (GPU L2 cache size)</li><li>SM 处理器计数 (SM processor count)</li><li>异步引擎计数 (Asynchronous engine count)</li></ul></li></ul><h3 id="如何使用tensorrt在多个gpu上mutiple-gpus"><a class="markdownIt-Anchor" href="#如何使用tensorrt在多个gpu上mutiple-gpus"></a> 如何使用 TensorRT 在多个 GPU 上 (mutiple GPUs)？</h3><ul><li>无论是由构建器还是反序列化生成的引擎，<strong>每个 ICudaEngine 对象在实例化时被绑定到特定的 GPU</strong></li><li>如果要选择使用特定 GPU，请<strong>在调用生成器或反序列化引擎之前使用 cudaSetDevice ()</strong></li><li>注意，<strong>每个 IExecutionContext 与引擎绑定到同一个 GPU</strong></li></ul><h3 id="如何理解ibuilderconfig的setmaxworkspacesize"><a class="markdownIt-Anchor" href="#如何理解ibuilderconfig的setmaxworkspacesize"></a> 如何理解 IBuilderConfig 的 setMaxWorkspaceSize？</h3><ul><li>最大工作空间大小，寻找层实现通常需要一个临时工作区，这个参数限制了网络中任何层可以使用的最大大小。如果提供的工作空间不够，则 TensorRT 可能无法找到一个层的实现</li><li>给出模型中任一层能使用的内存上限。运行时，每一层需要多少内存系统分配多少，并不是每次都分 1 GB，但不会超过 1 GB</li><li>设置内存上限过小会报以下警告<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[I] [TRT] Some tactics <span class="keyword">do</span> not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output</span><br></pre></td></tr></tbody></table></figure></li><li>经过实践，设置内存上限很大，引擎构建时间也不一定增加或减少，推理速度有下降趋势，但是不是很明显 (可能某些操作提升明显)</li></ul><h3 id="如何选择最佳的工作空间大小-workspace-size"><a class="markdownIt-Anchor" href="#如何选择最佳的工作空间大小-workspace-size"></a> 如何选择最佳的工作空间大小 (workspace size)？</h3><ul><li>方法 IBuilderConfig: :setMaxWorkspaceSize () 控制可分配的最大工作空间，并防止构建器考虑需要更多工作空间的算法</li><li>运行时，在创建 IExecutionContext。即使在中设置了额度，分配的额度也不会超过 IBuilderConfig: :setMaxWorkspaceSize () 的要求</li><li>因此，应用程序应该允许 TensorRT 构建器尽可能多的工作空间；在运行时，TensorRT 分配的不超过这个数量，通常更少</li></ul><h3 id="tensorrt的engine和校准表是否可以跨tensorrt版本"><a class="markdownIt-Anchor" href="#tensorrt的engine和校准表是否可以跨tensorrt版本"></a> TensorRT 的 <strong>engine 和校准表是否可以跨 TensorRT 版本？</strong></h3><ul><li>不可以。内部实现和格式会不断优化，并且可以在不同版本之间进行更改</li><li>不能保证发动机和校准表与不同版本的二进制兼容 TensorRT。当使用新版本的时，应用程序应该构建新的引擎和 INT8 校准表 TensorRT</li></ul><h3 id="如何创建一个针对几种不同批量优化的引擎"><a class="markdownIt-Anchor" href="#如何创建一个针对几种不同批量优化的引擎"></a> 如何创建一个针对几种不同批量优化的引擎？</h3><ul><li>TensorRT 允许针对给定批处理大小优化的引擎以任何较小的大小运行，但这些较小大小的引擎的性能却不能得到很好的优化</li><li>要针对多个不同的批次大小进行优化，请在分配给的维度上创建优化配置文件 optfilerselector: :KOpt</li></ul></div><footer class="post-footer"><div class="reward-container"><div>请我一杯咖啡吧！</div><button>赞赏</button><div class="post-reward"><div><img src="/images/wechatpay.png" alt="Shaogui 微信"> <span>微信</span></div><div><img src="/images/alipay.png" alt="Shaogui 支付宝"> <span>支付宝</span></div></div></div><div class="post-copyright"><ul><li class="post-copyright-author"><strong>本文作者： </strong>Shaogui</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://www.shaogui.life/posts/2302855918.html" title="02-TensorRT 开发者指南">https://www.shaogui.life/posts/2302855918.html</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><span>欢迎关注我的其它发布渠道</span><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="https://www.zhihu.com/people/mu-zhi-zhi-tian"><span class="icon"><i class="fab fa-zhihu"></i> </span><span class="label">知乎</span></a></div><div class="social-item"><a target="_blank" class="social-link" href="/atom.xml"><span class="icon"><i class="fa fa-rss"></i> </span><span class="label">RSS</span></a></div></div></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/602210745.html" rel="prev" title="KD"><i class="fa fa-angle-left"></i> KD</a></div><div class="post-nav-item"><a href="/posts/1373687827.html" rel="next" title="03-TensorRT 的资料收集">03-TensorRT 的资料收集 <i class="fa fa-angle-right"></i></a></div></div></footer></article></div><div class="comments utterances-container"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2016 – <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Shaogui</span></div><div class="wordcount"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-chart-line"></i> </span><span>站点总字数：</span> <span title="站点总字数">1.9m</span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span>站点阅读时长 &asymp;</span> <span title="站点阅读时长">28:48</span></span></div><div class="busuanzi-count"><span class="post-meta-item" id="busuanzi_container_site_uv"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-item" id="busuanzi_container_site_pv"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div><div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动</div></div></footer><div class="toggle sidebar-toggle" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div><div class="sidebar-dimmer"></div><div class="reading-progress-bar"></div><a href="https://github.com/WuShaogui" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><noscript><div class="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script><script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script><script src="/js/third-party/search/local-search.js"></script><script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"neutral","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script><script src="/js/third-party/tags/mermaid.js"></script><script src="/js/third-party/pace.js"></script><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script><script src="/js/third-party/math/mathjax.js"></script><script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"WuShaogui/wushaogui.github.io","issue_term":"pathname","theme":"github-light"}</script><script src="/js/third-party/comments/utterances.js"></script></body></html>